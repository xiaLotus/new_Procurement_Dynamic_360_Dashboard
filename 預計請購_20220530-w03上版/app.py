import datetime
import json
import os
import re
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
from ldap3 import Server, Connection, ALL, NTLM # type: ignore
from ldap3.core.exceptions import LDAPException, LDAPBindError # type: ignore
import uuid
from filelock import FileLock, Timeout
from werkzeug.utils import secure_filename
import logging
import shutil
import numpy as np
import traceback

BACKEND_DATA = r"D:\Data\Backend_Access_Management\Backend_data.json"
VENDER_FILE_PATH = f'static/data/vender.ini'

app = Flask(__name__)
CORS(app)
CSV_FILE = "static/data/Planned_Purchase_Request_List.csv"
JSON_FILE = f"static/data/money.json"
BUYER_FILE = f"static/data/Buyer_detail.csv"

def read_json_file():
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå­—å…¸


def clean_value(val):
    try:
        if isinstance(val, str) and val.endswith(".0"):
            return str(int(float(val)))
        return str(val) if val is not None else ""
    except Exception:
        return str(val)


def authenticate_user(username, password):
    try:
        server = Server('ldap://KHADDC02.kh.asegroup.com', get_info = ALL)
        # ä½¿ç”¨ NTLM
        user = f'kh\\{username}'
        password = f'{password}'
        return True

        # print("å¸³è™Ÿ: ", username, " å¯†ç¢¼: ", password)
        # # å»ºç«‹é€£æ¥
        # conn = Connection(server, user = user, password = password, authentication = NTLM)

        # # å˜—è©¦ç¶å®š
        # if conn.bind():
        #     app.logger.info(f"User {username} login successful.")
        
        # else:
        #     # app.logger.warning(f"Login failed for user {username}: {conn.last_error}")
        #     return False
    except Exception as e:
        # app.logger.error(f"Error during authentication for user {username}: {e}")
        return False


@app.route("/api/getAllLoginer", methods=["POST"])
def getAllLoginer():
    try:
        data = request.get_json()
        user_id = data.get("username")
        print(user_id)

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            items = json.load(f)

        for item in items:
            if str(item.get("å·¥è™Ÿ", "")).strip() == user_id:
                return jsonify({"name": "Username Find"})
            
        return jsonify({"error": "Item not found"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/login', methods=['POST'])
def login():
    data = request.json
    if not data:
        return jsonify({'message': 'æœªæä¾›ç™»å…¥è³‡è¨Š'}), 400
    
    username = data.get('username', '').strip()
    password = data.get('password', '').strip()
    
    if authenticate_user(username, password):
        return jsonify({'message': 'ç™»å…¥æˆåŠŸ'})
    return jsonify({'message': 'å¸³è™Ÿæˆ–å¯†ç¢¼éŒ¯èª¤'}), 401


# è¨­å®šå„²å­˜ç¯©é¸ç‹€æ…‹çš„è³‡æ–™å¤¾
FILTERS_DIR = 'user_filters'
if not os.path.exists(FILTERS_DIR):
    os.makedirs(FILTERS_DIR)

@app.route('/api/save-filters-json', methods=['POST'])
def save_filters_json():
    try:
        data = request.json
        username = data.get('username') # type: ignore
        
        if not username:
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘ä½¿ç”¨è€…åç¨±'}), 400
        
        # å„²å­˜åˆ° JSON æª”æ¡ˆ
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # å¯«å…¥æª”æ¡ˆ
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return jsonify({'status': 'success', 'message': 'ç¯©é¸ç‹€æ…‹å·²å„²å­˜'})
        
    except Exception as e:
        print(f"å„²å­˜ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/get-filters-json/<username>', methods=['GET'])
def get_filters_json(username):
    try:
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                filters = json.load(f)
            return jsonify(filters)
        else:
            return jsonify({'message': 'æ‰¾ä¸åˆ°ç¯©é¸è¨­å®š'}), 404
            
    except Exception as e:
        print(f"è¼‰å…¥ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


@app.route('/api/clear-filters-json/<username>', methods=['DELETE'])
def clear_filters_json(username):
    try:
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        if os.path.exists(filename):
            os.remove(filename)
            return jsonify({'status': 'success', 'message': 'ç¯©é¸ç‹€æ…‹å·²æ¸…é™¤'})
        else:
            return jsonify({'status': 'success', 'message': 'æ²’æœ‰éœ€è¦æ¸…é™¤çš„ç¯©é¸ç‹€æ…‹'})
            
    except Exception as e:
        print(f"æ¸…é™¤ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


def update_verification_status_and_po_numbers():
    """
    æ›´æ–°é©—æ”¶ç‹€æ…‹å’ŒPO No.æ¬„ä½
    1. æ ¹æ“šBuyer_detail.csvä¸­åŒIDçš„é©—æ”¶ç‹€æ…‹ä¾†æ›´æ–°Planned_Purchase_Request_List.csvçš„é©—æ”¶ç‹€æ…‹
    2. å°‡åŒIDä¸‹çš„PO No.çµ„åˆæˆå­—ä¸²
    3. æª¢æŸ¥Buyer_detail.csvä¸­çš„ePR No.ï¼Œå¦‚æœæœ‰å€¼å‰‡è¨­é–‹å–®ç‹€æ…‹ç‚º'V'
    """
    # backup_files()
    try:
        # è®€å–å…©å€‹CSVæª”æ¡ˆ
        main_df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)  # Planned_Purchase_Request_List.csv
        detail_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)  # Buyer_detail.csv
        
        # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
        if 'é©—æ”¶ç‹€æ…‹' not in main_df.columns:
            main_df['é©—æ”¶ç‹€æ…‹'] = 'X'
        if 'PO No.' not in main_df.columns:
            main_df['PO No.'] = ''
        if 'é–‹å–®ç‹€æ…‹' not in detail_df.columns:
            detail_df['é–‹å–®ç‹€æ…‹'] = 'X'
        
        # ===== æ­¥é©Ÿ1ï¼šæ›´æ–° BUYER_FILE ä¸­çš„é–‹å–®ç‹€æ…‹ =====
        for idx, detail_row in detail_df.iterrows():
            epr_no = str(detail_row.get('ePR No.', '')).strip()
            if epr_no and epr_no.isdigit() and len(epr_no) == 10:
                detail_df.at[idx, 'é–‹å–®ç‹€æ…‹'] = 'V' # type: ignore
            else:
                detail_df.at[idx, 'é–‹å–®ç‹€æ…‹'] = 'X' # type: ignore
        
        detail_df.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
        
        # ===== æ­¥é©Ÿ2ï¼šæ›´æ–°ä¸»è¡¨ é©—æ”¶ç‹€æ…‹ å’Œ PO No. =====
        for idx, main_row in main_df.iterrows():
            main_id = str(main_row.get('Id', '')).strip()
            if not main_id:
                continue
                
            detail_records = detail_df[detail_df['Id'].astype(str).str.strip() == main_id]
            
            if detail_records.empty:
                main_df.at[idx, 'é©—æ”¶ç‹€æ…‹'] = 'X' # type: ignore
                main_df.at[idx, 'PO No.'] = '' # type: ignore
                continue
            
            # é©—æ”¶ç‹€æ…‹ï¼šåªæœ‰ç•¶æ‰€æœ‰éƒ½æ˜¯ V æ‰è¨­ V
            statuses = detail_records['é©—æ”¶ç‹€æ…‹'].fillna('X').astype(str).str.strip()
            main_df.at[idx, 'é©—æ”¶ç‹€æ…‹'] = 'V' if all(s == 'V' for s in statuses if s) else 'X' # type: ignore
            
            # PO No. çµ„åˆ
            po_numbers = detail_records['PO No.'].fillna('').astype(str).str.strip()
            unique_po_numbers = []
            for po in po_numbers:
                if po and po not in unique_po_numbers:
                    unique_po_numbers.append(po)
            main_df.at[idx, 'PO No.'] = '<br />'.join(unique_po_numbers) if unique_po_numbers else '' # type: ignore
        
        main_df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        return True
    
    except Exception as e:
        print(f"âŒ æ›´æ–°é©—æ”¶ç‹€æ…‹å’ŒPO No.æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback; traceback.print_exc()
        return False


# ===============================
@app.route("/data")
def get_data():
    update_verification_status_and_po_numbers()
    df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)

    expected_columns = [
        "Id", "é–‹å–®ç‹€æ…‹", "WBS", "è«‹è³¼é †åº", "éœ€æ±‚è€…", "è«‹è³¼é …ç›®", "éœ€æ±‚åŸå› ",
        "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No.", "é€²åº¦è¿½è¹¤è¶…é€£çµ", "å‚™è¨»",
        "Status", "ç°½æ ¸ä¸­é—œå¡", "å ±å‘Šè·¯å¾‘", "é©—æ”¶è·¯å¾‘", "åˆä½œé¡åˆ¥", "åˆä½œå» å•†", 
        "å‰è³¼å–®å–®è™Ÿ", "é©—æ”¶ç‹€æ…‹", "PO No."
    ]
    for col in expected_columns:
        if col not in df.columns:
            df[col] = ""

    if "Id" not in df.columns:
        df.insert(0, "Id", [str(uuid.uuid4()) for _ in range(len(df))])
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
    else:
        df["Id"] = df["Id"].fillna("").astype(str).str.strip()
        missing_ids = df["Id"] == ""
        df.loc[missing_ids, "Id"] = [str(uuid.uuid4()) for _ in range(missing_ids.sum())]
        if missing_ids.any():
            df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

    df["ePR No."] = df["ePR No."].apply(lambda x: "" if pd.isna(x) or x == 0 else str(int(float(x))))
    df["ç¸½é‡‘é¡"] = pd.to_numeric(df["ç¸½é‡‘é¡"], errors="coerce").fillna(0)
    df["ç¸½é‡‘é¡"] = df["ç¸½é‡‘é¡"].apply(lambda x: "" if x == 0 else int(x))

    numeric_columns = ["è«‹è³¼é †åº", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "å‰è³¼å–®å–®è™Ÿ", "é©—æ”¶ç‹€æ…‹"]
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(
                lambda x: "" if pd.isna(x) or x == 0 else str(int(float(x)))
                if str(x).replace('.','').isdigit() else str(x)
            )
    
    df = df.fillna("")
    return jsonify(df.to_dict(orient="records"))


@app.route('/api/unordered-count')
def get_unordered_count():
    
    if not os.path.exists(CSV_FILE) or os.path.getsize(CSV_FILE) == 0:
        return jsonify({
            "count_X": 0,
            "count_V": 0,
            "monthly_expenses": {}
        })

    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna("")

        count_X = df[df["é–‹å–®ç‹€æ…‹"] != "V"].shape[0]
        count_V = df[df["é–‹å–®ç‹€æ…‹"] == "V"].shape[0]

        df_unordered = df[df["é–‹å–®ç‹€æ…‹"] != "V"].copy()

        # âœ… åŠ é€™ä¸€è¡Œï¼Œéæ¿¾æ‰éæ³•çš„éœ€æ±‚æ—¥
        df_unordered["éœ€æ±‚æ—¥"] = df_unordered["éœ€æ±‚æ—¥"].astype(str).str.replace("/", "", regex=False)
        df_unordered = df_unordered[df_unordered["éœ€æ±‚æ—¥"].str.match(r'^\d{8}$')].copy()

        df_unordered["ç¸½é‡‘é¡"] = pd.to_numeric(df_unordered["ç¸½é‡‘é¡"], errors="coerce").fillna(0)
        df_unordered["éœ€æ±‚æ—¥"] = df_unordered["éœ€æ±‚æ—¥"].astype(str)
        df_unordered["æœˆä»½"] = df_unordered["éœ€æ±‚æ—¥"].str.slice(0, 6)

        monthly_sums = df_unordered.groupby("æœˆä»½")["ç¸½é‡‘é¡"].sum().sort_index()
        monthly_expenses = {month: int(round(amount, 2)) for month, amount in monthly_sums.items()}

        print(monthly_expenses)

        return jsonify({
            "count_X": count_X,
            "count_V": count_V,
            "monthly_expenses": monthly_expenses
        })

    except Exception as e:
        traceback.print_exc()  
        return jsonify({
            "count_X": 0,
            "count_V": 0,
            "monthly_expenses": {},
            "error": str(e)
        }), 500


# 12/1
# ========================================================================
# ğŸ†• æ–°å¢APIï¼šæœˆåº¦èŠ±è²»åˆ†æï¼ˆç”¨æ–¼æ–°åœ–è¡¨é é¢ï¼‰
# ========================================================================
@app.route('/api/monthly_expense_analysis', methods=['POST'])
def monthly_expense_analysis():
    """æœˆåº¦è²»ç”¨åˆ†æAPI - æ”¯æŒç¯„åœæŸ¥è©¢"""
    try:
        data = request.json
        start_month = data.get('start_month', '2025-02')
        end_month = data.get('end_month', '2025-11')
        
        # è®€å–CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        
        # éæ¿¾æœ‰æ—¥æœŸçš„è¨˜éŒ„
        df_with_date = df[df['å·²é–‹å–®æ—¥æœŸ'].notna()].copy()
        
        if len(df_with_date) == 0:
            return jsonify({'success': False, 'message': 'CSVä¸­æ²’æœ‰æœ‰æ•ˆæ•¸æ“š'})
        
        # âœ… æ­£ç¢ºè½‰æ›æ—¥æœŸæ ¼å¼ (20250506 -> 202505)
        df_with_date['å·²é–‹å–®æ—¥æœŸ_str'] = df_with_date['å·²é–‹å–®æ—¥æœŸ'].astype(int).astype(str)
        df_with_date['å¹´æœˆ'] = df_with_date['å·²é–‹å–®æ—¥æœŸ_str'].str[:6]
        
        # è½‰æ›æŸ¥è©¢æœˆä»½ (2025-02 -> 202502)
        start_month_num = start_month.replace('-', '')
        end_month_num = end_month.replace('-', '')
        
        # æ•¸å­—ç¯„åœç¯©é¸
        df_filtered = df_with_date[
            (df_with_date['å¹´æœˆ'] >= start_month_num) & 
            (df_with_date['å¹´æœˆ'] <= end_month_num)
        ]
        
        if len(df_filtered) == 0:
            return jsonify({'success': False, 'message': f'{start_month} åˆ° {end_month} æ²’æœ‰æ•¸æ“š'})
        
        # åˆ†é›¢æ­£å¸¸å’ŒWBS
        df_normal = df_filtered[df_filtered['WBS'].isna()].copy()
        df_wbs = df_filtered[df_filtered['WBS'].notna()].copy()
        
        # ç”Ÿæˆæ‰€æœ‰æœˆä»½åˆ—è¡¨
        all_months = []
        current = start_month_num
        while current <= end_month_num:
            all_months.append(f"{current[:4]}-{current[4:]}")
            year, month = int(current[:4]), int(current[4:])
            month = month + 1 if month < 12 else 1
            year = year + 1 if month == 1 else year
            current = f"{year}{month:02d}"
        
        # æ­£å¸¸èŠ±è²»è¶¨å‹¢
        normal_trend = []
        if len(df_normal) > 0:
            normal_monthly = df_normal.groupby('å¹´æœˆ')['ç¸½é‡‘é¡'].apply(
                lambda x: int(x.astype(float).sum())
            ).to_dict()
            for month in all_months:
                month_key = month.replace('-', '')
                normal_trend.append({'month': month, 'amount': normal_monthly.get(month_key, 0)})
        else:
            normal_trend = [{'month': m, 'amount': 0} for m in all_months]
        
        # WBSèŠ±è²»è¶¨å‹¢
        wbs_trend = []
        if len(df_wbs) > 0:
            wbs_monthly = df_wbs.groupby('å¹´æœˆ')['ç¸½é‡‘é¡'].apply(
                lambda x: int(x.astype(float).sum())
            ).to_dict()
            for month in all_months:
                month_key = month.replace('-', '')
                wbs_trend.append({'month': month, 'amount': wbs_monthly.get(month_key, 0)})
        else:
            wbs_trend = [{'month': m, 'amount': 0} for m in all_months]
        
        return jsonify({
            'success': True,
            'data': {
                'normal': {
                    'total': int(df_normal['ç¸½é‡‘é¡'].astype(float).sum()) if len(df_normal) > 0 else 0,
                    'average': int(df_normal['ç¸½é‡‘é¡'].astype(float).mean()) if len(df_normal) > 0 else 0,
                    'count': len(df_normal),
                    'trend': normal_trend
                },
                'wbs': {
                    'total': int(df_wbs['ç¸½é‡‘é¡'].astype(float).sum()) if len(df_wbs) > 0 else 0,
                    'average': int(df_wbs['ç¸½é‡‘é¡'].astype(float).mean()) if len(df_wbs) > 0 else 0,
                    'count': len(df_wbs),
                    'trend': wbs_trend
                }
            }
        })
        
    except Exception as e:
        print(f"æœˆåº¦è²»ç”¨åˆ†æéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        return jsonify({'success': False, 'message': f'éŒ¯èª¤: {str(e)}'}), 500
    

@app.route('/api/getrestofmoney', methods = ['GET'])
def getrestofmoney():
    import datetime, re, math

    budget = read_json_file()
    current_date = datetime.datetime.now()
    current_year = str(current_date.year)
    month_no_pad = str(current_date.month)       # '8'
    month_pad = str(current_date.month).zfill(2) # '08'
    yyyymm = current_year + month_pad

    # å˜—è©¦å…©ç¨® key
    data = budget.get("é ç®—", {}).get(current_year, {}).get(month_no_pad, {})
    if not data:
        data = budget.get("é ç®—", {}).get(current_year, {}).get(month_pad, {})

    current_budget = float(data.get('ç•¶æœˆè«‹è³¼é ç®—', 0))
    additional_budget = float(data.get('ç•¶æœˆè¿½åŠ é ç®—', 0))

    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
    except:
        df = pd.DataFrame()

    total_money = 0.0
    if not df.empty:
        for _, row in df.iterrows():
            status = str(row.get('é–‹å–®ç‹€æ…‹', '')).strip()
            raw_date = str(row.get('å·²é–‹å–®æ—¥æœŸ', '')).strip()
            cleaned_date = ''.join([c for c in raw_date if c.isdigit()])[:8]
            wbs = str(row.get('WBS', '')).strip()

            raw_amount = str(row.get('ç¸½é‡‘é¡', '')).replace(',', '').strip()
            try:
                amount = float(raw_amount) if raw_amount and raw_amount.lower() != 'nan' else 0.0
            except:
                amount = 0.0
            if math.isnan(amount):
                amount = 0.0

            monthKey = cleaned_date[:6]
            matchMonth = len(cleaned_date) == 8 and monthKey == yyyymm

            if status == 'V' and matchMonth and not re.match(r'^\d{2}FT0A\d{4}$', wbs):
                total_money += amount
                # print(f"[DEBUG] åŠ ç¸½ -> ç‹€æ…‹={status}, æ—¥æœŸ={cleaned_date}, æœˆä»½={monthKey}, é‡‘é¡={amount}, WBS={wbs}")

    rest_money = (current_budget + additional_budget) - total_money
    # print(f"[DEBUG] ç•¶æœˆé ç®—={current_budget}, è¿½åŠ ={additional_budget}, å·²é–‹å–®={total_money}, å‰©é¤˜={rest_money}")
    return jsonify({
        'ç•¶æœˆè«‹è³¼é ç®—': int(current_budget),
        'ç•¶æœˆè¿½åŠ é ç®—': int(additional_budget),
        'å·²é–‹å–®ç¸½é¡': int(total_money),
        'å‰©é¤˜é‡‘é¡': int(rest_money)
    })




@app.route('/api/budget_months', methods=['GET'])
def get_budget_months():
    """
    ç²å–æ‰€æœ‰å¯ç”¨çš„é ç®—æœˆä»½é¸é …å’Œå°æ‡‰çš„é ç®—é‡‘é¡
    """
    try:
        budget = read_json_file()
        # print(f"ğŸ“Š è®€å–é ç®—è³‡æ–™: {budget}")
        
        budget_list = []
        
        # å¾é ç®—è³‡æ–™ä¸­æå–æ‰€æœ‰å¹´ä»½å’Œæœˆä»½ï¼Œä¸¦è¨ˆç®—é ç®—
        budget_data = budget.get("é ç®—", {})
        
        for year in budget_data.keys():
            year_data = budget_data[year]
            for month in year_data.keys():
                # æ ¼å¼åŒ–ç‚º YYYYMM æ ¼å¼
                month_padded = str(month).zfill(2)
                year_month = f"{year}{month_padded}"
                
                # ç²å–è©²æœˆä»½çš„é ç®—è³‡æ–™
                month_budget_data = year_data[month]
                current_budget = month_budget_data.get('ç•¶æœˆè«‹è³¼é ç®—', 0)
                additional_budget = month_budget_data.get('ç•¶æœˆè¿½åŠ é ç®—', 0)
                total_budget = current_budget + additional_budget
                
                # å»ºç«‹æœˆä»½å’Œé ç®—çš„å°æ‡‰è³‡æ–™
                budget_info = {
                    'month': year_month,
                    'money': total_budget,
                    'ç•¶æœˆè«‹è³¼é ç®—': current_budget,
                    'ç•¶æœˆè¿½åŠ é ç®—': additional_budget
                }
                
                budget_list.append(budget_info)
                # print(f"ğŸ’° {year_month}: è«‹è³¼={current_budget:,}, è¿½åŠ ={additional_budget:,}, ç¸½è¨ˆ={total_budget:,}")
        
        # æŒ‰æœˆä»½æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        budget_list.sort(key=lambda x: x['month'], reverse=True)
        
        # print(f"ğŸ“… å®Œæ•´é ç®—æ¸…å–®: {budget_list}")
        
        return jsonify({
            'success': True,
            'budget_list': budget_list,
            'count': len(budget_list)
        })
        
    except Exception as e:
        print(f"âŒ ç²å–é ç®—æœˆä»½éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
    

@app.route('/api/uploadMoney', methods=['POST'])
def uploadMoney():
    try:
        data = request.get_json()
        year = data.get('currentYear')
        month = data.get('currentMonth')
        current_budget = data.get('currentBudget')
        additional_budget = data.get('additionalBudget')

        budget_data = read_json_file()  # è®€åŸå§‹ JSON è³‡æ–™
        print("budget_data: ", budget_data)

        # æª¢æŸ¥å¹´åº¦
        if "é ç®—" not in budget_data:
            budget_data["é ç®—"] = {}

        if str(year) not in budget_data["é ç®—"]:
            budget_data["é ç®—"][str(year)] = {}

        # æ›´æ–°è©²æœˆä»½è³‡æ–™
        budget_data["é ç®—"][str(year)][str(month)] = {
            "ç•¶æœˆè«‹è³¼é ç®—": current_budget,
            "ç•¶æœˆè¿½åŠ é ç®—": additional_budget
        }

        # âœ… æ­£ç¢ºå¯«å…¥æ›´æ–°å¾Œçš„è³‡æ–™
        with open(JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(budget_data, f, ensure_ascii=False, indent=4)

        return jsonify({'message': 'è³‡æ–™æäº¤æˆåŠŸ'}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 400


@app.route('/api/requesters', methods=['GET'])
def get_requesters():
    try:
        with open("config.cfg", "r", encoding="utf-8-sig") as f:
            lines = f.readlines()
            names = [line.strip() for line in lines if line.strip()]  
            # print("requesters: ", names)
            return jsonify(names)
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@app.route('/api/admins', methods=['GET'])
def get_admins():
    try:
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            data = json.load(f)
            seen = set()
            names = []
            for entry in data:
                if entry["è«‹è³¼ç¶²é å¾Œå°"] == 'O':
                    name = entry["å·¥è™Ÿ"]
                    if name not in seen:
                        seen.add(name)
                        names.append(name)
            # print("admins: ", names)
            return jsonify(names)
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@app.route("/api/add", methods = ['POST'])
def add_new_item():
    try:
        # å‚™ä»½
        # backup_files()
        data = request.get_json()
        main_data = {k: v for k, v in data.items() if k != 'tableRows'}
        table_rows = data.get('tableRows', [])

        print("ä¸»è³‡æ–™ï¼š", main_data)
        
        if not data:
            return jsonify({'status': 'error', 'message': 'ç„¡æ•ˆçš„ JSON è«‹æ±‚'}), 400

        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)

        # ç¢ºä¿æ‰€æœ‰æ¬„ä½ä¸€è‡´
        # new_row = {col: data.get(col, "") for col in df.columns}

        new_row = {col: clean_value(main_data.get(col, "")) for col in df.columns}

        if isinstance(new_row["éœ€æ±‚æ—¥"], str) and "/" in new_row["éœ€æ±‚æ—¥"]:
            new_row['éœ€æ±‚æ—¥'] = new_row["éœ€æ±‚æ—¥"].replace("/", "")

        if isinstance(new_row["å·²é–‹å–®æ—¥æœŸ"], str) and "/" in new_row["å·²é–‹å–®æ—¥æœŸ"]:
            new_row['å·²é–‹å–®æ—¥æœŸ'] = new_row["å·²é–‹å–®æ—¥æœŸ"].replace("/", "")

        def safe_float_to_int64(value):
            try:
                return str(int(float(value)))
            except (ValueError, TypeError, OverflowError):
                return ""

        # âœ¨ è¦è½‰æ›çš„æ¬„ä½ï¼ˆé¿å… int32 æº¢ä½ï¼‰
        numeric_fields = ["è«‹è³¼é †åº", "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No."]
        for field in numeric_fields:
            if field in new_row:
                new_row[field] = safe_float_to_int64(new_row[field])

        new_row["Id"] = str(uuid.uuid4())  # è‡ªå‹•ç”¢ç”Ÿå”¯ä¸€ ID

        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        print(new_row)
        # å„²å­˜å› CSV
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

        # å¯«å…¥å¦ä¸€å¼µè¡¨
        print("è¡¨æ ¼åˆ—ï¼š", table_rows)
        DETAIL_CSV_FILE = "static/data/Buyer_detail.csv"  # å»ºè­°å­˜å¦ä¸€ä»½ CSV æª”

        detail_columns = [
            "Id", "é–‹å–®ç‹€æ…‹", "äº¤è²¨é©—è­‰", "User", "ePR No.", "PO No.",
            "Item", "å“é …", "è¦æ ¼", "æ•¸é‡", "ç¸½æ•¸", "å–®åƒ¹", "ç¸½åƒ¹", "å‚™è¨»", "å­—æ•¸",
            "isEditing", "backup", "_alertedItemLimit", "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ",
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "ç™¼ç¥¨æœˆä»½", "WBS", "éœ€æ±‚æ—¥", "RTé‡‘é¡", "RTç¸½é‡‘é¡","é©—æ”¶ç‹€æ…‹"
        ]

        # è™•ç†å¾Œçš„è³‡æ–™åˆ—ï¼ˆæ¯ç­† row è£œä¸Šä¸»è³‡æ–™ IDï¼Œæ¬„ä½é †åºçµ±ä¸€ï¼‰
        cleaned_rows = []
        for row in table_rows:
            cleaned_row = {"Id": new_row["Id"]}  # ä¸»è¡¨ Id
            for col in detail_columns[1:-2]:  # ä¸å« isEditing, backupï¼Œé€™å…©å€‹æ‰‹å‹•è£œ
                cleaned_row[col] = row.get(col, "")
            cleaned_row["isEditing"] = "False"
            cleaned_row["backup"] = "{}"
            cleaned_rows.append(cleaned_row)
            
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨å°± appendï¼Œå¦å‰‡å»ºç«‹æ–°æª”
        if os.path.exists(DETAIL_CSV_FILE):
            df_detail = pd.read_csv(DETAIL_CSV_FILE, encoding="utf-8-sig", dtype=str)
            df_detail = pd.concat([df_detail, pd.DataFrame(cleaned_rows)], ignore_index=True)
        else:
            df_detail = pd.DataFrame(cleaned_rows, columns=detail_columns)

        # å¯«å…¥ CSV
        df_detail.to_csv(DETAIL_CSV_FILE, index=False, columns=detail_columns, encoding="utf-8-sig")

        return jsonify({'status': 'success', 'message': 'è³‡æ–™æ–°å¢æˆåŠŸ'}), 200

    except Exception as e:
        print("æ–°å¢éŒ¯èª¤ï¼š", e)
        return jsonify({'status': 'error', 'message': str(e)}), 500
    



@app.route('/update', methods=['POST'])
def update_data():
    # å‚™ä»½
    # backup_files()
    new_data = request.json
    if not isinstance(new_data, dict):
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    print(new_data)
    main_data = {k: v for k, v in new_data.items() if k != "tableRows"} # type: ignore
    detail_data = new_data.get("tableRows", []) # type: ignore
    
    if main_data is None:
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    
    target_id = str(main_data.get("Id", "")).strip()
    if not target_id:
        return jsonify({"message": "ç¼ºå°‘ Id"}), 400
    
    try:
        df = pd.read_csv(CSV_FILE, dtype=str)
        df.fillna("", inplace=True)

        # æ‰¾åˆ°å°æ‡‰åˆ—
        df["Id"] = df["Id"].astype(str).str.strip()
        match_idx = df.index[df["Id"] == target_id].tolist()
        if not match_idx:
            return jsonify({"message": "æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™"}), 404
        idx = match_idx[0]

        # è™•ç†æ—¥æœŸæ¬„ä½æ ¼å¼
        if isinstance(new_data.get("éœ€æ±‚æ—¥"), str):
            main_data["éœ€æ±‚æ—¥"] = main_data["éœ€æ±‚æ—¥"].replace("/", "")
        if isinstance(new_data.get("å·²é–‹å–®æ—¥æœŸ"), str):
            main_data["å·²é–‹å–®æ—¥æœŸ"] = main_data["å·²é–‹å–®æ—¥æœŸ"].replace("/", "")

        # æ›´æ–°æ¬„ä½
        for col in df.columns:
            df.at[idx, col] = str(main_data.get(col, ""))

            # æ¸…æ´—è³‡æ–™ï¼ˆåŒ…å«å»é™¤ .0 èˆ‡æ–œç·šï¼‰
        for col in df.columns:
            val = main_data.get(col, "")
            val = clean_value(val)
            if col in ["éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ"] and isinstance(val, str):
                val = val.replace("/", "")
            df.at[idx, col] = val

        # è‹¥ ePR No. å­˜åœ¨ â†’ è£œé€£çµæ¬„ä½
        if main_data.get('ePR No.'):
            df.at[idx, "é€²åº¦è¿½è¹¤è¶…é€£çµ"] = f"https://khwfap.kh.asegroup.com/ePR/PRQuery/QueryPR?id={main_data['ePR No.']}"


        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
       
    
        detail_file = "static/data/Buyer_detail.csv"
        if os.path.exists(detail_file):
            df_detail = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
            df_detail = df_detail[df_detail["Id"] != target_id]
        else:
            df_detail = pd.DataFrame()

        # åŠ å…¥æ–°çš„ç´°é …è³‡æ–™
        if detail_data:
            new_rows = pd.DataFrame(detail_data)
            new_rows["Id"] = target_id  # åŠ å…¥å°æ‡‰ä¸»è¡¨ Id
            df_detail = pd.concat([df_detail, new_rows], ignore_index=True)

        # å„²å­˜
        df_detail.to_csv(detail_file, index=False, encoding="utf-8-sig")
        
        return jsonify({"message": "æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        print("éŒ¯èª¤ï¼š", e)
        return jsonify({"message": "è³‡æ–™æ›´æ–°å¤±æ•—"}), 500



@app.route('/api/get-username-info', methods=['POST'])
def get_username_info():
    try:
        data = request.get_json()
        emp_id = data.get("emp_id", "").strip()  # å·¥è™Ÿ

        if not emp_id:
            return jsonify({"error": "ç¼ºå°‘å·¥è™Ÿ"}), 400

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            users = json.load(f)

        for entry in users:
            if entry.get("å·¥è™Ÿ", "").strip() == emp_id:
                return jsonify({
                    "name": entry.get("å§“å", "").strip(),
                    "ç­åˆ¥": entry.get("ç­åˆ¥", "").strip()
                })

        return jsonify({"error": "æŸ¥ç„¡æ­¤å·¥è™Ÿ"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
    
@app.route('/api/checkeEPRno', methods=['POST'])
def check_epr_no():
    try:
        epr_no = request.get_json()
        if not isinstance(epr_no, str):
            return jsonify({'error': 'Invalid ePR No. format'}), 400

        df = pd.read_csv(CSV_FILE, dtype=str)  # è®€å–æ‰€æœ‰æ¬„ä½ç‚ºå­—ä¸²é¿å…è½‰å‹å•é¡Œ
        exists = epr_no in df['ePR No.'].dropna().tolist()

        return jsonify({'exists': exists})
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return jsonify({'error': 'æª¢æŸ¥å¤±æ•—'}), 500


@app.route('/api/check-edit-permission', methods=['POST'])
def check_edit_permission():
    try:
        data = request.get_json()
        current_user = data.get("currentUser", "").strip()
        target_id = data.get("id", "").strip()

        if not current_user or not target_id:
            return jsonify({"allowed": False, "error": "ç¼ºå°‘å¿…è¦åƒæ•¸"}), 400
        
        # è®€å– admin å·¥è™Ÿåˆ—è¡¨
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)
            admin_ids = [
                entry.get("å·¥è™Ÿ", "").strip()
                for entry in backend_data
                if entry.get("è«‹è³¼ç¶²é å¾Œå°") == "O"
            ]

        is_admin = current_user in admin_ids

        # æŸ¥è©¢è©² ID å°æ‡‰çš„éœ€æ±‚è€…
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df.fillna("", inplace=True)
        df["Id"] = df["Id"].astype(str).str.strip()

        matched_row = df[df["Id"] == target_id]
        if matched_row.empty:
            return jsonify({"allowed": False, "error": "æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™"}), 404

        row = matched_row.iloc[0]
        requester = row.get("éœ€æ±‚è€…", "").strip()

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)
        
        requester_id = ""
        for entry in backend_data:
            if entry.get("å§“å", "").strip() == requester:
                requester_id = entry.get("å·¥è™Ÿ", "").strip()
                break

        print(f"ä½¿ç”¨è€…: {current_user}, å·¥è™Ÿ: {requester_id}, æ˜¯å¦ç‚º admin: ", is_admin)

        if requester_id == current_user:
            return jsonify({"allowed": True})

        return jsonify({"allowed": is_admin})

    except Exception as e:
        print("æ¬Šé™æª¢æŸ¥å¤±æ•—ï¼š", e)
        return jsonify({"allowed": False, "error": str(e)}), 500




@app.route("/api/get_detail/<id>", methods=["GET"])
def get_detail_by_id(id):
    try:
        detail_file = "static/data/Buyer_detail.csv"
        if not os.path.exists(detail_file):
            return jsonify([])  # æ²’æœ‰è³‡æ–™å›å‚³ç©ºé™£åˆ—

        df = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
        df.fillna("", inplace=True)

        # ç¯©é¸å‡ºæŒ‡å®š Id çš„ç´°é …è³‡æ–™
        detail_rows = df[df["Id"] == id].to_dict(orient="records")
        return jsonify(detail_rows)
    
    except Exception as e:
        print("è®€å–ç´°é …è³‡æ–™éŒ¯èª¤ï¼š", e)
        return jsonify({"error": str(e)}), 500



@app.route("/getItemName", methods=["POST"])
def get_item_name():
    try:
        data = request.get_json()
        user_id = data.get("username")
        user_name = data.get("NeedPerson")
        print(user_id, user_name)

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            items = json.load(f)

        for item in items:
            if str(item.get("å·¥è™Ÿ", "")).strip() == user_id:
                return jsonify({"name": item.get("å§“å", "")})
            
        return jsonify({"error": "Item not found"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/delete', methods=['POST'])
def delete_entry():
    # å‚™ä»½
    # backup_files()
    data = request.json
    if data is None:
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    
    target_id = str(data.get("Id", "")).strip()
    if not target_id:
        return jsonify({"status": "error", "message": "ç¼ºå°‘ Id æ¬„ä½"}), 400

    df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
    # å–æ¶ˆ eprnoçš„æ¬„ä½

    new_df = df[df['Id'] != target_id]

    if len(new_df) == len(df):
        return jsonify({"status": "error", "message": "æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„è³‡æ–™"})

    new_df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

    detail_file = "static/data/Buyer_detail.csv" 
    if os.path.exists(detail_file):
        df_detail = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
        df_detail = df_detail[df_detail["Id"] != target_id]
        df_detail.to_csv(detail_file, index=False, encoding="utf-8-sig")

    return jsonify({"status": "success", "message": "å·²æˆåŠŸåˆªé™¤"})

# é è¨ˆè«‹è³¼ æ›´æ–°ç›®å‰ç‹€æ…‹
@app.route("/api/Status-upload", methods=["POST"])
def upload():
    # å‚™ä»½
    # backup_files()
    if 'file' not in request.files:
        return jsonify({'status': 'fail', 'msg': 'No file uploaded'}), 400

    file = request.files['file']
    filename = file.filename
    if filename != "download.csv":
        return jsonify({'status': 'Fail', 'filename': filename}), 400
    filename = f"status_{uuid.uuid4().hex}.csv"
    filepath = os.path.join('uploads', str(filename))
    file.save(filepath)

    lock = FileLock(f"{CSV_FILE}.lock")

    def clean_nan_value(val):
        if pd.isna(val) or str(val).strip().lower() in ['nan', 'none']:
            return ""
        return str(val).strip()
    
    with lock:
        try:

            df = pd.read_csv(filepath)
            print(df)
            updates = []
            for _, row in df.iterrows():
                epr_no = str(row.get('E-PRè™Ÿç¢¼           ', '')).strip()
                status = clean_nan_value(row.get('ç‹€æ…‹', ''))
                stage = clean_nan_value(row.get('ç°½æ ¸ä¸­é—œå¡', ''))

                if epr_no and epr_no != 'nan':
                    updates.append({
                        'epr_no': epr_no,
                        'ç‹€æ…‹': status,
                        'ç°½æ ¸ä¸­é—œå¡': stage
                    })
                    
            Planned_Purchase_Request_List_df = pd.read_csv(CSV_FILE)
            updated_count = 0
            no_change_count = 0

            # non_null_count = Planned_Purchase_Request_List_df['ePR No.'].notna().sum()


            for update in updates:
                epr = update['epr_no']
                new_status = update['ç‹€æ…‹']
                new_stage = update['ç°½æ ¸ä¸­é—œå¡']
            
                match = Planned_Purchase_Request_List_df['ePR No.'] == int(epr)
                if match.any():
                    idx = Planned_Purchase_Request_List_df[match].index[0]
                    
                    # å–å¾—ç›®å‰å€¼
                    old_status = clean_nan_value(Planned_Purchase_Request_List_df.at[idx, 'Status'])
                    old_stage = clean_nan_value(Planned_Purchase_Request_List_df.at[idx, 'ç°½æ ¸ä¸­é—œå¡'])
                    
                    # åªæœ‰åœ¨çœŸçš„æœ‰è®Šæ›´æ™‚æ‰æ›´æ–°
                    if old_status != new_status or old_stage != new_stage:
                        Planned_Purchase_Request_List_df.at[idx, 'Status'] = new_status
                        Planned_Purchase_Request_List_df.at[idx, 'ç°½æ ¸ä¸­é—œå¡'] = new_stage
                        updated_count += 1
                    else:
                        no_change_count += 1

                    
            def safe_float_to_int64(val):
                try:
                    if pd.isna(val):
                        return ""
                    return str(int(float(val)))
                except:
                    return str(val).strip()
                
            numeric_fields = ["è«‹è³¼é †åº", "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No."]
            for field in numeric_fields:
                if field in Planned_Purchase_Request_List_df.columns:
                    Planned_Purchase_Request_List_df[field] = Planned_Purchase_Request_List_df[field].apply(safe_float_to_int64)

            # æ¸…ç†æ‰€æœ‰æ¬„ä½
            for col in Planned_Purchase_Request_List_df.columns:
                Planned_Purchase_Request_List_df[col] = Planned_Purchase_Request_List_df[col].apply(clean_nan_value)

            Planned_Purchase_Request_List_df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig', na_rep="")
        except Exception as e:
            print("Exception: ", e)


    with lock:
        try:
            df_update = pd.read_csv(filepath, dtype=str)

            # æå–æ›´æ–°è³‡æ–™
            po_updates = []
            for _, row in df_update.iterrows():
                epr_no = str(row.get('E-PRè™Ÿç¢¼           ', '')).strip()
                sap_po  = str(row.get('SAP PO', '')).strip() 
                if epr_no and sap_po:
                    po_updates.append({
                        'epr_no': epr_no,
                        'po_no': sap_po
                    })
            df_detail = pd.read_csv(BUYER_FILE, dtype=str)

            if "PO No." not in df_detail.columns:
                df_detail["PO No."] = ""


            updated = 0
            nochange = 0

            for upd in po_updates:
                epr = upd['epr_no']
                new_po = upd['po_no']

                # ePR No. è½‰æˆ int â†’ str é¿å…æ ¼å¼ä¸åŒ
                try:
                    epr_str = str(int(float(epr)))
                except:
                    epr_str = str(epr).strip()

                match_idx = df_detail.index[df_detail['ePR No.'] == epr_str].tolist()

                if match_idx:
                    for idx in match_idx:
                        old_po = str(df_detail.at[idx, 'PO No.']).strip()

                        if old_po != new_po:
                            df_detail.at[idx, 'PO No.'] = new_po
                            updated += 1
                        else:
                            nochange += 1

            # å¯«å› Buyer_detail.csv
            df_detail.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
            # os.remove(filepath)
            return jsonify({'status': 'success', 'updated_count': updated_count})

        except Exception as e:
            return jsonify({'status': 'Fail', 'reason': e})




# è·¯å¾‘(å ±å‘Šè·¯å¾‘)
@app.route('/upload_report', methods=['POST'])
def upload_report():
    file = request.files.get('file')
    folder = request.form.get('folder')
    username = request.form.get('work_username')

    if not file:
        return jsonify(success=False, message='ç„¡æª”æ¡ˆ')
    if not folder:
        return jsonify(success=False, message='è³‡æ–™å¤¾åç¨±ç‚ºç©º')
    if not username:
        return jsonify(success=False, message='ç¼ºå°‘ username')

    filename = file.filename or ''
    if filename == '':
        return jsonify(success=False, message='æª”æ¡ˆåç¨±ç‚ºç©º')

    try:
        save_path = os.path.join(
            r'\\cim300\FT01_CIM\FT01_4000\11.RRç­äººå“¡-ePRè«‹è³¼ç®¡ç†',
            username,
            folder
        )
        os.makedirs(save_path, exist_ok=True)
        file.save(os.path.join(save_path, filename))
        return jsonify(success=True, message='å·²ä¸Šå‚³')
    except Exception as e:
        print('âŒ å„²å­˜éŒ¯èª¤ï¼š', e)
        return jsonify(success=False, message=str(e))



# è·¯å¾‘(å ±å‘Šè·¯å¾‘)
@app.route('/upload_acceptancereport', methods=['POST'])
def upload_acceptancereport():
    file = request.files.get('file')
    folder = request.form.get('folder')
    username = request.form.get('work_username')

    if not file:
        return jsonify(success=False, message='ç„¡æª”æ¡ˆ')
    if not folder:
        return jsonify(success=False, message='è³‡æ–™å¤¾åç¨±ç‚ºç©º')
    if not username:
        return jsonify(success=False, message='ç¼ºå°‘ username')

    filename = file.filename or ''
    if filename == '':
        return jsonify(success=False, message='æª”æ¡ˆåç¨±ç‚ºç©º')

    try:
        save_path = os.path.join(
            r'\\cim300\FT01_CIM\FT01_4000\11.RRç­äººå“¡-ePRè«‹è³¼ç®¡ç†',
            username,
            r"=å·²çµå–®=",
            folder
        )
        os.makedirs(save_path, exist_ok=True)
        file.save(os.path.join(save_path, filename))
        return jsonify(success=True, message='å·²ä¸Šå‚³')
    except Exception as e:
        print('âŒ å„²å­˜éŒ¯èª¤ï¼š', e)
        return jsonify(success=False, message=str(e))

# å–å¾—è‹±æ–‡åå­—
@app.route('/api/getUsername', methods=['POST'])
def get_username():
    try:
        data = request.get_json()
        username = data.get("username")
        print("æ”¶åˆ°éœ€æ±‚è€…åå­—ï¼š", username)


        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)

        matched = next((entry for entry in backend_data if entry["å§“å"] == username), None)
        
        if matched and "Notes_ID" in matched:
            try:
                name = matched["Notes_ID"].split("_")[0]
            except:
                name = matched["Notes_ID"].split("@")[0]

            return jsonify({"name": name})
        else:
            return jsonify({"name": "æŸ¥ç„¡ä½¿ç”¨è€…"}), 404
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/api/sendmail', methods=['POST']) 
def sendmail():
    try:
        data = request.get_json()
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)

        mail_data = data.get("data", {})
        recipient_clean = mail_data.get("éœ€æ±‚è€…", "").strip()  
        mail_name = ''
        matched = next(
            (entry for entry in backend_data if entry.get("å§“å", "").strip() == recipient_clean),
            None
        )

        if matched and "Notes_ID" in matched:
            mail_name = matched["Notes_ID"]

        raw_cc = data.get("cc", "")  

        cc_names = [name.strip() for name in raw_cc.split(",") if name.strip()]

        cc_list = [name.replace(" ", "_") + "@aseglobal.com" for name in cc_names]

        cc_string = ",".join(cc_list)

        if data["data"]["è«‹è³¼é †åº"] == "1":
            print("æ­£åœ¨å°å…¥è¶…æ€¥ä»¶æ¨¡çµ„...")
            from MailFunction.urgent import send_mail
            print("é–‹å§‹åŸ·è¡Œè¶…æ€¥ä»¶éƒµä»¶ç™¼é€...")
            try:
                print(data["data"], '\n', data["recipient"], '\n', mail_name, '\n', cc_string)
                send_mail(data["data"], data["recipient"], mail_name, cc_string)
                print("è¶…æ€¥ä»¶éƒµä»¶ç™¼é€å®Œæˆ")
            except Exception as urgent_error:
                print(f"âŒ è¶…æ€¥ä»¶éƒµä»¶ç™¼é€éŒ¯èª¤: {str(urgent_error)}")
              
                print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                traceback.print_exc()
                raise urgent_error
        else:
            print("æ­£åœ¨å°å…¥ä¸€èˆ¬éƒµä»¶æ¨¡çµ„...")
            from MailFunction.normail import send_mail
            print("é–‹å§‹åŸ·è¡Œä¸€èˆ¬éƒµä»¶ç™¼é€...")
            try:
                print(data["data"], '\n', data["recipient"], '\n', mail_name, '\n', cc_string)
                send_mail(data["data"], data["recipient"], mail_name, cc_string)
                print("ä¸€èˆ¬éƒµä»¶ç™¼é€å®Œæˆ")
            except Exception as normal_error:
                print(f"âŒ ä¸€èˆ¬éƒµä»¶ç™¼é€éŒ¯èª¤: {str(normal_error)}")
        
                print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                traceback.print_exc()
                raise normal_error

        print("æº–å‚™è¿”å›æˆåŠŸéŸ¿æ‡‰")
        return jsonify({"success": True, "message": "éƒµä»¶ç™¼é€æˆåŠŸ"}), 200
        
    except Exception as e:
        print(f"âŒ ä¸»å‡½æ•¸éŒ¯èª¤: {str(e)}")

        print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500
    
@app.route('/update_for_mail', methods=['POST'])
def update_for_mail():
    # å‚™ä»½
    # backup_files()
    new_data = request.json
    print("æ”¶åˆ°çš„è³‡æ–™:", new_data)
    
    if new_data is None:
        return jsonify({"message": "æ²’æœ‰æ”¶åˆ°è³‡æ–™"}), 400
    
    try:
        df = pd.read_csv(CSV_FILE, dtype=str)
        df.fillna("", inplace=True)

        # æ‰¾åˆ°å°æ‡‰åˆ—
        df["Id"] = df["Id"].astype(str).str.strip()
        target_id = str(new_data['Id']).strip()
        
        print(f"å°‹æ‰¾ Id: {target_id}")
        
        # æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åˆ—
        mask = df["Id"] == target_id
        
        if not mask.any():
            print(f"æ‰¾ä¸åˆ° Id ç‚º {target_id} çš„è³‡æ–™")
            return jsonify({"message": "æ‰¾ä¸åˆ°å°æ‡‰çš„è³‡æ–™"}), 404
        
        print(f"æ‰¾åˆ°å°æ‡‰è³‡æ–™ï¼Œé–‹å§‹æ›´æ–°...")
        
        # æ›´æ–°è³‡æ–™
        for key, value in new_data.items():
            if key in df.columns:
                df.loc[mask, key] = str(value) if value is not None else ""
                print(f"æ›´æ–° {key}: {value}")
        
        # å„²å­˜åˆ° CSV
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        print("è³‡æ–™å·²å„²å­˜")
        
        return jsonify({"message": "æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        print("éŒ¯èª¤ï¼š", e)
        
        traceback.print_exc()
        return jsonify({"message": "è³‡æ–™æ›´æ–°å¤±æ•—"}), 500


# ç½é ­è¨Šæ¯   - ã€‚(å‰è³¼å–®ï¼šï¼Œè©³å¦‚é™„ä»¶)éœ€æ±‚å·¥ç¨‹å¸«ï¼š(CT4:)ï¼Œåˆä½œé–‹ç™¼ï¼š
@app.route('/api/get_phone', methods=['POST'])
def get_phone():
    data = request.get_json()
    name = data.get("name", "")
    with open('static/data/phone.json', 'r', encoding='utf-8-sig') as f:
        phone_data = json.load(f)

    phone_number = phone_data.get(name, "æœªçŸ¥")

    return jsonify({"phone": phone_number})


# å» å•†
@app.route('/api/venders', methods=['GET'])
def get_venders():
    """è®€å– vender.ini ä¸¦è¿”å›ä¾›æ‡‰å•†åˆ—è¡¨"""
    try:
        if os.path.exists(VENDER_FILE_PATH):
            with open(VENDER_FILE_PATH, 'r', encoding='utf-8') as f:
                venders = [line.strip() for line in f.readlines() if line.strip()]
            return jsonify(venders)
        else:
            return jsonify([])
    except Exception as e:
        print(e)
        return jsonify({'error': str(e)}), 500


@app.route('/api/venders', methods=['POST'])
def add_vender():
    """æ–°å¢ä¾›æ‡‰å•†åˆ° vender.ini"""
    try:
        data = request.get_json()
        new_vender = data.get('vender', '').strip()

        if not new_vender:
            return jsonify({'error': 'ä¾›æ‡‰å•†åç¨±ä¸èƒ½æ˜¯ç©ºçš„'}), 400

        # è®€å–ç¾æœ‰æ¸…å–®ï¼Œé¿å…é‡è¤‡
        existing = []
        if os.path.exists(VENDER_FILE_PATH):
            with open(VENDER_FILE_PATH, 'r', encoding='utf-8') as f:
                existing = [line.strip() for line in f.readlines() if line.strip()]

        if new_vender in existing:
            return jsonify({'message': 'ä¾›æ‡‰å•†å·²å­˜åœ¨'}), 200

        # è¿½åŠ å¯«å…¥
        with open(VENDER_FILE_PATH, 'a', encoding='utf-8') as f:
            f.write(new_vender + '\n')

        return jsonify({'message': 'æ–°å¢æˆåŠŸ', 'vender': new_vender}), 201

    except Exception as e:
        print(e)
        return jsonify({'error': str(e)}), 500


# eHub è™•ç†
BUYER_FILE_LOCK = f"static/data/Buyer_detail.csv.lock"  # ğŸ”’ é–æª”æ¡ˆè·¯å¾‘

from difflib import SequenceMatcher
buyer_file_lock = FileLock(BUYER_FILE_LOCK, timeout=10)

def is_po_in_record(row_po_str, target_po):
    """æª¢æŸ¥ PO æ˜¯å¦åœ¨è¨˜éŒ„ä¸­ï¼ˆæ”¯æ´ <br /> åˆ†éš”çš„å¤šå€‹ POï¼‰"""
    po_list = re.split(r"<br\s*/?>", str(row_po_str))
    po_list = [po.strip() for po in po_list if po.strip()]
    return target_po.strip() in po_list


def fuzzy_in(text, keyword):
    """æ¨¡ç³Šæ¯”å°é—œéµå­—æ˜¯å¦åœ¨æ–‡å­—ä¸­"""
    return keyword.strip() in str(text).strip()

def cleanup_temp_csv_files(po_no=None):
    """æ¸…ç†æš«å­˜çš„ CSV æª”æ¡ˆ"""
    try:
        uploads_dir = "uploads"
        if not os.path.exists(uploads_dir):
            return
        
        if po_no:
            # åªåˆªé™¤ç‰¹å®š PO çš„æª”æ¡ˆ
            temp_file = os.path.join(uploads_dir, f"{po_no}.csv")
            if os.path.exists(temp_file):
                os.remove(temp_file)
                logger.info(f"âœ… å·²åˆªé™¤: {temp_file}")
        else:
            # åˆªé™¤æ‰€æœ‰æš«å­˜ CSV æª”æ¡ˆ
            for filename in os.listdir(uploads_dir):
                if filename.endswith('.csv'):
                    file_path = os.path.join(uploads_dir, filename)
                    os.remove(file_path)
                    logger.info(f"âœ… å·²åˆªé™¤: {file_path}")
    except Exception as e:
        logger.error(f"æ¸…ç†æš«å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


@app.route("/api/save_csv", methods=["POST", "OPTIONS"])
def save_csv():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200

    data = request.get_json(silent=True) or {}
    content = data.get("content", "")

    from io import StringIO
    df_all = pd.read_csv(StringIO(content), dtype=str).fillna("")

    # å…ˆè™•ç† Item â†’ ä¸€å¾‹è£œæˆ4ä½æ•¸
    if "PO Item æ¡è³¼å–®é …æ¬¡" in df_all.columns:
        df_all["PO Item æ¡è³¼å–®é …æ¬¡"] = (
            df_all["PO Item æ¡è³¼å–®é …æ¬¡"]
            .str.strip()
            .apply(lambda x: str(int(float(x))) if x.replace('.', '', 1).isdigit() else x)
            .apply(lambda x: x.zfill(4) if x.isdigit() else x)
        )

    # ç¢ºèªæœ‰å¿…è¦æ¬„ä½
    if "PO NO æ¡è³¼å–®è™Ÿç¢¼" not in df_all.columns:
        return jsonify({"status": "error", "msg": "âŒ ç¼ºå°‘ PO NO æ¡è³¼å–®è™Ÿç¢¼ æ¬„ä½!"}), 400

    # æŠ“å‡ºæ‰€æœ‰ä¸åŒçš„ PO NO
    unique_po_nos = df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"].dropna().unique()

    # âœ… ç¬¬ä¸€æ­¥:åˆ†ç¾¤å„²å­˜ uploads/{po_no}.csv
    saved_files = []
    for po_no in unique_po_nos:
        po_no_clean = str(po_no).strip()
        group_df = df_all[df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"] == po_no_clean]

        upload_path = f"uploads/{po_no_clean}.csv"
        group_df.to_csv(upload_path, index=False, encoding="utf-8-sig")

        saved_files.append({
            "po_no": po_no_clean,
            "rows": len(group_df),
            "file": upload_path
        })
        logger.info(f"âœ… å·²å„²å­˜ {upload_path} ({len(group_df)} ç­†è³‡æ–™)")

    try:
        with buyer_file_lock:
            # âœ… ç¬¬äºŒæ­¥:è¼‰å…¥ Buyer_detail.csv (æ¯”å°ç”¨)
            df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
            df_buyer["PO No."] = df_buyer["PO No."].str.strip()
            df_buyer["Item"] = (
                df_buyer["Item"]
                .str.replace(r"\.0$", "", regex=True)
                .str.strip()
                .apply(lambda x: x.zfill(4) if x.isdigit() else x)
            )

            # ğŸ”´ å»ºç«‹ä¸€å€‹åªåŒ…å«ç‹€æ…‹ç‚º V çš„è³‡æ–™ç´¢å¼•
            df_active = df_buyer[df_buyer["é–‹å–®ç‹€æ…‹"] == "V"].copy()
            logger.info(f"ç¸½è³‡æ–™ç­†æ•¸: {len(df_buyer)}, æœ‰æ•ˆè³‡æ–™(ç‹€æ…‹=V): {len(df_active)}")

    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ è®€å– Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"è®€å–æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500

    # ğŸ†• åˆå§‹åŒ–
    all_group_results = []
    quantity_mismatch_items = []
    merged_items = []  # ğŸ†• è¨˜éŒ„åˆä½µçš„é …ç›®
    
    # âœ…âœ…âœ… æ–°å¢ï¼šè¨˜éŒ„å“ªäº› PO æœ‰å•é¡Œï¼ˆåˆ†æ‰¹æˆ–åˆä½µï¼‰
    mismatch_po_set = set()  # æœ‰åˆ†æ‰¹å•é¡Œçš„ PO
    merge_po_set = set()      # æœ‰åˆä½µå•é¡Œçš„ PO

    def clean_name(x: str) -> str:
        return str(x).replace("\n", "").replace("\r", "").replace("<br>", "").strip()

    df_buyer["å“é …_clean"] = df_buyer["å“é …"].apply(clean_name)
    buyer_id_lookup = {
        (row["PO No."], row["Item"]): row.get("Id", "")
        for _, row in df_buyer.iterrows()
    }

    # ğŸ”´ ç¬¬ä¸€è¼ª:æª¢æ¸¬æ‰€æœ‰ PO æ˜¯å¦æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œ
    logger.info("\n" + "="*80)
    logger.info("ç¬¬ä¸€è¼ªæƒæï¼šæª¢æ¸¬åˆ†æ‰¹å’Œåˆä½µå•é¡Œ")
    logger.info("="*80)
    
    for file_info in saved_files:
        po_no = file_info["po_no"]
        csv_path = file_info["file"]
        
        df_po = pd.read_csv(csv_path, encoding="utf-8-sig", dtype=str).fillna("")
        df_po["PO Item æ¡è³¼å–®é …æ¬¡"] = df_po["PO Item æ¡è³¼å–®é …æ¬¡"].str.zfill(4)
        
        buyer_related = df_active[df_active["PO No."].str.contains(po_no, regex=False, na=False)].copy()
        
        xls_item_groups = df_po.groupby("PO Item æ¡è³¼å–®é …æ¬¡")
        
        for item, xls_group in xls_item_groups:
            xls_count = len(xls_group)
            csv_items = buyer_related[buyer_related["Item"] == item]
            csv_count = len(csv_items)
            
            # âœ… æ–°å¢:å¦‚æœ XLS ç­†æ•¸ < CSV ç­†æ•¸,ä»£è¡¨ã€Œåˆ†æ‰¹è®Šå›åˆä½µã€
            if csv_count > 0 and xls_count < csv_count:
                logger.info(f"ğŸ”„ åµæ¸¬åˆ°åˆä½µ:PO={po_no}, Item={item}, XLS={xls_count}ç­† < CSV={csv_count}ç­†")
                merge_po_set.add(po_no)  # âœ… æ¨™è¨˜æ­¤ PO æœ‰åˆä½µå•é¡Œ
                
                xls_rows = []
                for _, row in xls_group.iterrows():
                    xls_rows.append({
                        "description": str(row.get("Description å“å", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0"))
                    })
                
                csv_rows = []
                batch_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["å‚™è¨»"].str.contains("åˆ†æ‰¹", na=False)) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                csv_items = df_buyer[batch_mask]
                
                for _, row in csv_items.iterrows():
                    csv_rows.append({
                        "id": str(row.get("Id", "")),
                        "description": str(row.get("å“é …", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0")),
                        "note": str(row.get("å‚™è¨»", ""))
                    })
                
                # ğŸ†• è¨˜éŒ„éœ€è¦åˆä½µçš„é …ç›®
                merged_items.append({
                    "po_no": po_no,
                    "item": item,
                    "xls_count": int(xls_count),
                    "csv_count": int(csv_count),
                    "xls_data": xls_rows,
                    "csv_data": csv_rows,
                    "action_type": "merge"
                })
                
                logger.info(f"   ğŸ“ å·²è¨˜éŒ„éœ€è¦åˆä½µçš„é …ç›®")

            # ğŸš¨ ç­†æ•¸ä¸ç¬¦ï¼ˆåˆ†æ‰¹ï¼‰
            elif csv_count > 0 and xls_count > csv_count:
                mismatch_po_set.add(po_no)  # âœ… æ¨™è¨˜æ­¤ PO æœ‰åˆ†æ‰¹å•é¡Œ
                
                try:
                    total_sod = csv_items["SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"].astype(float).sum()
                except:
                    total_sod = 0.0
                
                xls_rows = []
                for _, row in xls_group.iterrows():
                    xls_rows.append({
                        "description": str(row.get("Description å“å", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0"))
                    })
                
                csv_rows = []
                for _, row in csv_items.iterrows():
                    csv_rows.append({
                        "id": str(row.get("Id", "")),
                        "description": str(row.get("å“é …", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0")),
                        "total_qty": str(row.get("ç¸½æ•¸", "0"))
                    })
                
                quantity_mismatch_items.append({
                    "po_no": po_no,
                    "item": item,
                    "xls_count": int(xls_count),
                    "csv_count": int(csv_count),
                    "total_sod": float(total_sod),
                    "xls_data": xls_rows,
                    "csv_data": csv_rows
                })
                
                logger.info(f"âš ï¸ ç­†æ•¸ä¸ç¬¦: PO={po_no}, Item={item}, XLS={xls_count}ç­†, CSV={csv_count}ç­†")

    logger.info(f"\nğŸ“Š ç¬¬ä¸€è¼ªæƒæçµæœ:")
    logger.info(f"   åˆ†æ‰¹å•é¡Œ PO: {len(mismatch_po_set)} å€‹ - {list(mismatch_po_set)}")
    logger.info(f"   åˆä½µå•é¡Œ PO: {len(merge_po_set)} å€‹ - {list(merge_po_set)}")
    logger.info("="*80 + "\n")

    # ğŸ”´ ç¬¬äºŒè¼ª:è™•ç†æ²’æœ‰å•é¡Œçš„ PO (æ­£å¸¸æ¯”å°)
    logger.info("\n" + "="*80)
    logger.info("ç¬¬äºŒè¼ªè™•ç†ï¼šæ­£å¸¸æ¯”å°ï¼ˆæ’é™¤æœ‰å•é¡Œçš„ POï¼‰")
    logger.info("="*80)
    
    for file_info in saved_files:
        po_no = file_info["po_no"]
        
        # âœ…âœ…âœ… é—œéµä¿®æ”¹ï¼šå¦‚æœé€™å€‹ PO æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œï¼Œè·³éæ­£å¸¸æ¯”å°
        if po_no in mismatch_po_set:
            logger.info(f"â­•ï¸ è·³é PO {po_no} çš„æ­£å¸¸æ¯”å° (æœ‰åˆ†æ‰¹å•é¡Œ)")
            continue
        
        if po_no in merge_po_set:
            logger.info(f"â­•ï¸ è·³é PO {po_no} çš„æ­£å¸¸æ¯”å° (æœ‰åˆä½µå•é¡Œ)")
            continue
        
        csv_path = file_info["file"]
        df_po = pd.read_csv(csv_path, encoding="utf-8-sig", dtype=str).fillna("")
        df_po["PO Item æ¡è³¼å–®é …æ¬¡"] = df_po["PO Item æ¡è³¼å–®é …æ¬¡"].str.zfill(4)

        buyer_related = df_buyer[df_buyer["PO No."].str.contains(po_no, regex=False, na=False)].copy()

        matched_list = []
        conflict_list = []

        for row_idx, row in df_po.iterrows():
            item = row["PO Item æ¡è³¼å–®é …æ¬¡"]
            desc = clean_name(row.get("Description å“å", ""))
            delivery = row["Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"]
            qty = row["SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"]

            logger.info(f"è™•ç† PO {po_no} çš„ Item {item}")

            item_match = buyer_related[buyer_related["Item"] == item]
            
            if not item_match.empty:
                buyer_row = item_match.iloc[0]
                buyer_item = buyer_row["Item"]
                buyer_desc = clean_name(buyer_row["å“é …"])
                buyer_id = buyer_row.get("Id", "")

                logger.info(f"  æ‰¾åˆ° Buyer Item {buyer_item}, å“å: {buyer_desc}")

                df_buyer.loc[item_match.index, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = delivery
                df_buyer.loc[item_match.index, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = qty

                if buyer_desc == desc:
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    matched_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc or buyer_desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âœ… ç›¸åŒ",
                        "diff_type": "none"
                    })
                    logger.info(f"  âœ… å®Œå…¨ç›¸åŒ")
                else:
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    conflict_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âš ï¸ å“åä¸ç›¸åŒ",
                        "diff_type": "desc"
                    })
                    logger.info(f"  âš ï¸ å“åä¸åŒ")
            else:
                logger.info(f"  âŒ Buyer ä¸­æ‰¾ä¸åˆ° Item {item}")
                buyer_id = ''
                conflict_list.append({
                    "id": buyer_id,
                    "po_no": po_no,
                    "item": item,
                    "buyer_description": None,
                    "po_description": desc,
                    "delivery_date": delivery,
                    "sod_qty": qty,
                    "status": "âš ï¸ Buyer_detail æ²’æœ‰é€™ç­†è³‡æ–™",
                    "diff_type": "missing"
                })

        all_group_results.append({
            "po_no": po_no,
            "matched": matched_list,
            "conflict": conflict_list
        })
        
        logger.info(f"ğŸ“Š PO {po_no} çµ±è¨ˆ: ç›¸åŒ={len(matched_list)}, è¡çª={len(conflict_list)}")

    logger.info("="*80 + "\n")

    # ğŸ¯ æ ¹æ“šæƒ…æ³è¿”å›ä¸åŒçš„çµæœ
    
    # âœ… å„ªå…ˆè¿”å›åˆä½µç¢ºèª
    if merged_items:
        logger.info(f"ğŸ”„ ç™¼ç¾ {len(merged_items)} å€‹é …ç›®éœ€è¦åˆä½µ")
        
        # è½‰æ›è³‡æ–™
        merged_items_clean = []
        for item in merged_items:
            merged_items_clean.append({
                "po_no": str(item["po_no"]),
                "item": str(item["item"]),
                "xls_count": int(item["xls_count"]),
                "csv_count": int(item["csv_count"]),
                "xls_data": item["xls_data"],
                "csv_data": item["csv_data"],
                "action_type": "merge"
            })
        
        return jsonify({
            "status": "merge_confirmation_needed",
            "message": f"ç™¼ç¾ {len(merged_items)} å€‹é …ç›®å¾åˆ†æ‰¹è®Šå›å–®ç­†,éœ€è¦ç¢ºèª",
            "merge_items": merged_items_clean,
            "total_merge_items": len(merged_items),
            "groups": all_group_results,  # âœ… åªåŒ…å«æ²’å•é¡Œçš„ PO
            "has_normal_items": len(all_group_results) > 0
        })
    
    # âœ… å…¶æ¬¡è¿”å›åˆ†æ‰¹èª¿æ•´
    if quantity_mismatch_items:
        logger.info(f"ğŸš¨ ç™¼ç¾ {len(quantity_mismatch_items)} å€‹é …ç›®ç­†æ•¸ä¸ç¬¦")
        logger.info(f"âœ… åŒæ™‚è™•ç†äº† {len(all_group_results)} å€‹æ­£å¸¸ PO")
        
        return jsonify({
            "status": "quantity_mismatch",
            "message": f"ç™¼ç¾ {len(quantity_mismatch_items)} å€‹é …ç›®çš„ç­†æ•¸ä¸ç¬¦,éœ€è¦æ‰‹å‹•èª¿æ•´",
            "mismatches": quantity_mismatch_items,
            "total_mismatches": len(quantity_mismatch_items),
            "require_manual_adjustment": True,
            "groups": all_group_results,  # âœ… åªåŒ…å«æ²’å•é¡Œçš„ PO
            "has_normal_items": len(all_group_results) > 0
        })

    # æ²’æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œ,æ­£å¸¸è¿”å›
    logger.info(f"\nâœ… ç¸½å…±è™•ç† {len(all_group_results)} å€‹ PO")
    
    return jsonify({
        "status": "ok",
        "groups": all_group_results,
        "saved_files": saved_files,
        "has_mismatch": False
    })

# èµ°åˆ†æ‰¹
@app.route("/api/confirm_quantity_update", methods=["POST"])
def confirm_quantity_update():
    """è™•ç†ç­†æ•¸ä¸ç¬¦æ™‚ä½¿ç”¨è€…ç¢ºèªçš„æ›´æ–°(ä¿ç•™æ‰€æœ‰åŸå§‹æ¬„ä½)"""
    
    logger.info("=" * 80)
    logger.info("ğŸš€ confirm_quantity_update å‡½æ•¸è¢«èª¿ç”¨")
    logger.info("=" * 80)
    
    try:
        data = request.get_json()
        logger.info(f"ğŸ“¥ æ”¶åˆ°çš„å®Œæ•´è³‡æ–™: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        po_no = data.get("po_no")
        item = data.get("item")
        new_rows = data.get("rows")
        expected_total = data.get("expected_total")
        
        logger.info(f"ğŸ“¦ è§£æå¾Œçš„åƒæ•¸:")
        logger.info(f"   PO No: {po_no}")
        logger.info(f"   Item: {item}")
        logger.info(f"   æ–°ç­†æ•¸: {len(new_rows) if new_rows else 0}")
        logger.info(f"   é æœŸç¸½å’Œ: {expected_total}")
        
        if not po_no or not item or not new_rows:
            error_msg = f"ç¼ºå°‘å¿…è¦åƒæ•¸: po_no={po_no}, item={item}, new_rows={len(new_rows) if new_rows else 'None'}"
            logger.error(f"âŒ {error_msg}")
            return jsonify({
                "status": "error",
                "message": error_msg
            }), 400
        
        def smart_number_format(value):
            """æ™ºèƒ½æ ¼å¼åŒ–æ•¸å­—"""
            try:
                num = float(value)
                if num.is_integer():
                    return str(int(num))
                else:
                    return str(num)
            except (ValueError, TypeError):
                return str(value)
        
        try:
            with buyer_file_lock:
                # ğŸ“Š è¼‰å…¥ Buyer_detail.csv
                logger.info(f"ğŸ“‚ é–‹å§‹è¼‰å…¥ {BUYER_FILE}")
                
                if not os.path.exists(BUYER_FILE):
                    logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {BUYER_FILE}")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {BUYER_FILE}"
                    }), 500
                
                df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str).fillna("")
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ Buyer_detail.csv,å…± {len(df_buyer)} ç­†è³‡æ–™")
                
                # âš ï¸ è¨ˆç®—å¯¦éš› SOD ç¸½å’Œ
                try:
                    actual_total = sum(float(row.get("sod_qty", 0)) for row in new_rows)
                    sod_diff = actual_total - expected_total
                    
                    logger.info(f"ğŸ”¢ SOD ç¸½å’Œè¨ˆç®—:")
                    logger.info(f"   é æœŸ: {expected_total}")
                    logger.info(f"   å¯¦éš›: {actual_total}")
                    logger.info(f"   å·®ç•°: {sod_diff}")
                    
                    if abs(sod_diff) > 0.01:
                        logger.warning(f"âš ï¸ SOD ç¸½å’Œä¸ç¬¦!é æœŸ={expected_total}, å¯¦éš›={actual_total}, å·®ç•°={sod_diff}")
                        logger.warning(f"   ä½¿ç”¨è€…é¸æ“‡å¼·åˆ¶å„²å­˜,å°‡ç¹¼çºŒè™•ç†...")
                    else:
                        logger.info(f"âœ… SOD ç¸½å’Œæ­£ç¢º:{actual_total}")
                except Exception as e:
                    logger.error(f"âŒ è¨ˆç®— SOD ç¸½å’Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    logger.error(traceback.format_exc())
                
                # ğŸ—‘ï¸ åˆªé™¤èˆŠçš„è³‡æ–™
                logger.info(f"ğŸ” é–‹å§‹å°‹æ‰¾è¦åˆªé™¤çš„è³‡æ–™ (PO={po_no}, Item={item}, ç‹€æ…‹=V)")
                
                old_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                
                deleted_count = old_mask.sum()
                logger.info(f"ğŸ“Š æ‰¾åˆ° {deleted_count} ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
                
                if deleted_count == 0:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™")
                    logger.error(f"   æŸ¥è©¢æ¢ä»¶: PO No.={po_no}, Item={item}, é–‹å–®ç‹€æ…‹=V")
                    
                    po_exists = df_buyer[df_buyer["PO No."].str.strip() == po_no]
                    logger.error(f"   è©² PO å…±æœ‰ {len(po_exists)} ç­†è³‡æ–™")
                    
                    if len(po_exists) > 0:
                        items = po_exists["Item"].str.strip().unique()
                        logger.error(f"   è©² PO çš„ Items: {list(items)}")
                        statuses = po_exists["é–‹å–®ç‹€æ…‹"].unique()
                        logger.error(f"   è©² PO çš„ç‹€æ…‹: {list(statuses)}")
                    else:
                        logger.error(f"   âŒ è©² PO å®Œå…¨ä¸å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­")
                    
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™ (PO {po_no} - Item {item}, ç‹€æ…‹=V)"
                    }), 404
                
                # ğŸ¯ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬
                logger.info(f"ğŸ“‹ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬")
                first_old_row = df_buyer[old_mask].iloc[0]
                
                # ğŸ“ è¨˜éŒ„è¢«åˆªé™¤çš„è³‡æ–™
                deleted_data = []
                for idx, row in df_buyer[old_mask].iterrows():
                    deleted_data.append({
                        "å“é …": row.get("å“é …", ""),
                        "è¦æ ¼": row.get("è¦æ ¼", ""),
                        "å–®åƒ¹": row.get("å–®åƒ¹", ""),
                        "ç¸½åƒ¹": row.get("ç¸½åƒ¹", ""),
                        "äº¤æœŸ": row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", ""),
                        "SOD": row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "")
                    })
                
                logger.info(f"ğŸ—‘ï¸ å°‡åˆªé™¤çš„è³‡æ–™:")
                for i, d in enumerate(deleted_data, 1):
                    logger.info(f"   ç¬¬ {i} ç­†: {d}")
                
                # åˆªé™¤èˆŠè³‡æ–™
                logger.info(f"ğŸ—‘ï¸ åŸ·è¡Œåˆªé™¤æ“ä½œ...")
                df_buyer = df_buyer[~old_mask]
                logger.info(f"âœ… åˆªé™¤å®Œæˆ,å‰©é¤˜ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’° è¨ˆç®—å–®åƒ¹
                try:
                    unit_price_str = str(first_old_row.get("å–®åƒ¹", "0")).replace(",", "").strip()
                    unit_price = float(unit_price_str) if unit_price_str else 0.0
                    logger.info(f"ğŸ’° åŸå§‹å–®åƒ¹: {unit_price}")
                except Exception as e:
                    logger.error(f"âŒ è§£æå–®åƒ¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    unit_price = 0.0
                
                # â• æº–å‚™æ–°è³‡æ–™
                logger.info(f"â• é–‹å§‹æº–å‚™æ–°è³‡æ–™...")
                new_data_rows = []
                
                for idx, row_data in enumerate(new_rows):
                    logger.info(f"   è™•ç†ç¬¬ {idx+1}/{len(new_rows)} ç­†æ–°è³‡æ–™...")
                    
                    try:
                        sod_qty = float(row_data.get("sod_qty", 0))
                    except:
                        sod_qty = 0.0
                    
                    sod_qty_formatted = smart_number_format(sod_qty)
                    total_price = unit_price * sod_qty
                    total_price_formatted = smart_number_format(total_price)
                    unit_price_formatted = smart_number_format(unit_price)
                    
                    new_row = {
                        "Id": first_old_row.get("Id", ""),
                        "é–‹å–®ç‹€æ…‹": "V",
                        "äº¤è²¨é©—è­‰": first_old_row.get("äº¤è²¨é©—è­‰", ""),
                        "User": first_old_row.get("User", ""),
                        "ePR No.": first_old_row.get("ePR No.", ""),
                        "PO No.": po_no,
                        "Item": item,
                        "å“é …": row_data.get("description", first_old_row.get("å“é …", "")),
                        "è¦æ ¼": first_old_row.get("è¦æ ¼", ""),
                        "æ•¸é‡": sod_qty_formatted,
                        "ç¸½æ•¸": sod_qty_formatted,
                        "å–®åƒ¹": unit_price_formatted,
                        "ç¸½åƒ¹": total_price_formatted,
                        "å‚™è¨»": f"åˆ†æ‰¹{idx+1}/{len(new_rows)}" if len(new_rows) > 1 else first_old_row.get("å‚™è¨»", ""),
                        "å­—æ•¸": first_old_row.get("å­—æ•¸", ""),
                        "isEditing": "False",
                        "backup": "{}",
                        "_alertedItemLimit": "",
                        "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": row_data.get("delivery", ""),
                        "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": sod_qty_formatted,
                        "é©—æ”¶æ•¸é‡": "",
                        "æ‹’æ”¶æ•¸é‡": "",
                        "ç™¼ç¥¨æœˆä»½": "",
                        "WBS": first_old_row.get("WBS", ""),
                        "éœ€æ±‚æ—¥": first_old_row.get("éœ€æ±‚æ—¥", ""),
                        "RTé‡‘é¡": "",
                        "RTç¸½é‡‘é¡": "",
                        "é©—æ”¶ç‹€æ…‹": "X"
                    }
                    new_data_rows.append(new_row)
                    logger.info(f"      å“é …: {new_row['å“é …']}, SOD: {new_row['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']}, ç¸½åƒ¹: {new_row['ç¸½åƒ¹']}")
                
                logger.info(f"âœ… æº–å‚™å®Œæˆ {len(new_data_rows)} ç­†æ–°è³‡æ–™")
                
                # ğŸ”´ğŸ”´ğŸ”´ ç›´æ¥åŠ åˆ°æœ€å¾Œé¢ ğŸ”´ğŸ”´ğŸ”´
                logger.info(f"ğŸ“¥ å°‡æ–°è³‡æ–™æ·»åŠ åˆ°æœ€å¾Œ...")
                df_buyer = pd.concat([
                    df_buyer,
                    pd.DataFrame(new_data_rows)
                ], ignore_index=True)
                logger.info(f"âœ… æ·»åŠ å®Œæˆ,ç¾æœ‰ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’¾ å„²å­˜å›æª”æ¡ˆ
                logger.info(f"ğŸ’¾ é–‹å§‹å„²å­˜åˆ° {BUYER_FILE}...")
                df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
                logger.info(f"âœ… å„²å­˜å®Œæˆ")
        
        except Timeout:
            logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
            return jsonify({
                "status": "error",
                "message": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"
            }), 503

        # ğŸ“ æº–å‚™å›å‚³è¨Šæ¯
        success_msg = f"æˆåŠŸæ›´æ–° PO {po_no} Item {item}: åˆªé™¤ {deleted_count} ç­†,æ–°å¢ {len(new_data_rows)} ç­† (å·²åŠ åˆ°æœ€å¾Œ)"
        
        if abs(sod_diff) > 0.01:
            success_msg += f" (âš ï¸ SODå·®ç•°: {sod_diff:+.2f})"
        
        logger.info("=" * 80)
        logger.info(f"âœ… {success_msg}")
        logger.info("=" * 80)

        # ğŸ—‘ï¸ åˆªé™¤æš«å­˜çš„ CSV æª”æ¡ˆ
        cleanup_temp_csv_files(po_no)
        
        return jsonify({
            "status": "success",
            "message": success_msg,
            "deleted_count": int(deleted_count),
            "inserted_count": len(new_data_rows),
            "sod_info": {
                "expected": float(expected_total),
                "actual": float(actual_total),
                "difference": float(sod_diff),
                "is_match": abs(sod_diff) < 0.01
            },
            "preserved_fields": {
                "unit_price": float(unit_price),
                "specification": first_old_row.get("è¦æ ¼", ""),
                "wbs": first_old_row.get("WBS", ""),
                "epr_no": first_old_row.get("ePR No.", "")
            }
        })
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ confirm_quantity_update ç™¼ç”Ÿåš´é‡éŒ¯èª¤")
        logger.error(f"âŒ éŒ¯èª¤è¨Šæ¯: {str(e)}")
        logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
        logger.error("âŒ å®Œæ•´å †ç–Šè¿½è¹¤:")
        logger.error(traceback.format_exc())
        logger.error("=" * 80)
        
        return jsonify({
            "status": "error",
            "message": f"æ›´æ–°å¤±æ•—: {str(e)}"
        }), 500

# èµ°åˆä½µ
@app.route("/api/confirm_merge", methods=["POST"])
def confirm_merge():
    """è™•ç†åˆ†æ‰¹è®Šå›åˆä½µçš„ç¢ºèªæ›´æ–°"""
    
    logger.info("=" * 80)
    logger.info("ğŸ”„ confirm_merge å‡½æ•¸è¢«èª¿ç”¨")
    logger.info("=" * 80)
    
    try:
        data = request.get_json()
        logger.info(f"ğŸ“¥ æ”¶åˆ°çš„å®Œæ•´è³‡æ–™: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        po_no = data.get("po_no")
        item = data.get("item")
        xls_data = data.get("xls_data")  # æ–°çš„ XLS è³‡æ–™ (é€šå¸¸åªæœ‰ 1 ç­†)
        
        logger.info(f"ğŸ“¦ è§£æå¾Œçš„åƒæ•¸:")
        logger.info(f"   PO No: {po_no}")
        logger.info(f"   Item: {item}")
        logger.info(f"   æ–°ç­†æ•¸: {len(xls_data) if xls_data else 0}")
        
        if not po_no or not item or not xls_data:
            error_msg = f"ç¼ºå°‘å¿…è¦åƒæ•¸: po_no={po_no}, item={item}, xls_data={len(xls_data) if xls_data else 'None'}"
            logger.error(f"âŒ {error_msg}")
            return jsonify({
                "status": "error",
                "message": error_msg
            }), 400
        
        def smart_number_format(value):
            """æ™ºèƒ½æ ¼å¼åŒ–æ•¸å­—"""
            try:
                num = float(value)
                if num.is_integer():
                    return str(int(num))
                else:
                    return str(num)
            except (ValueError, TypeError):
                return str(value)
        
        try:
            with buyer_file_lock:
                # ğŸ“Š è¼‰å…¥ Buyer_detail.csv
                logger.info(f"ğŸ“‚ é–‹å§‹è¼‰å…¥ {BUYER_FILE}")
                
                if not os.path.exists(BUYER_FILE):
                    logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {BUYER_FILE}")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {BUYER_FILE}"
                    }), 500
                
                df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str).fillna("")
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ Buyer_detail.csv,å…± {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ—‘ï¸ åˆªé™¤èˆŠçš„åˆ†æ‰¹è³‡æ–™
                logger.info(f"ğŸ” é–‹å§‹å°‹æ‰¾è¦åˆªé™¤çš„è³‡æ–™ (PO={po_no}, Item={item}, å‚™è¨»åŒ…å«'åˆ†æ‰¹', ç‹€æ…‹=V)")
                
                old_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["å‚™è¨»"].str.contains("åˆ†æ‰¹", na=False)) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                
                deleted_count = old_mask.sum()
                logger.info(f"ğŸ“Š æ‰¾åˆ° {deleted_count} ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
                
                if deleted_count == 0:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™ (PO {po_no} - Item {item})"
                    }), 404
                
                # ğŸ¯ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬
                logger.info(f"ğŸ“‹ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬")
                first_old_row = df_buyer[old_mask].iloc[0]
                
                # ğŸ“ è¨˜éŒ„è¢«åˆªé™¤çš„è³‡æ–™
                deleted_data = []
                for idx, row in df_buyer[old_mask].iterrows():
                    deleted_data.append({
                        "å“é …": row.get("å“é …", ""),
                        "äº¤æœŸ": row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", ""),
                        "SOD": row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", ""),
                        "å‚™è¨»": row.get("å‚™è¨»", "")
                    })
                
                logger.info(f"ğŸ—‘ï¸ å°‡åˆªé™¤çš„è³‡æ–™:")
                for i, d in enumerate(deleted_data, 1):
                    logger.info(f"   ç¬¬ {i} ç­†: {d}")
                
                # åˆªé™¤èˆŠè³‡æ–™
                logger.info(f"ğŸ—‘ï¸ åŸ·è¡Œåˆªé™¤æ“ä½œ...")
                df_buyer = df_buyer[~old_mask]
                logger.info(f"âœ… åˆªé™¤å®Œæˆ,å‰©é¤˜ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’° è¨ˆç®—å–®åƒ¹
                try:
                    unit_price_str = str(first_old_row.get("å–®åƒ¹", "0")).replace(",", "").strip()
                    unit_price = float(unit_price_str) if unit_price_str else 0.0
                    logger.info(f"ğŸ’° åŸå§‹å–®åƒ¹: {unit_price}")
                except Exception as e:
                    logger.error(f"âŒ è§£æå–®åƒ¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    unit_price = 0.0
                
                # â• æº–å‚™æ–°è³‡æ–™ (åˆä½µå¾Œçš„è³‡æ–™)
                logger.info(f"â• é–‹å§‹æº–å‚™æ–°è³‡æ–™...")
                new_data_rows = []
                
                for idx, row_data in enumerate(xls_data):
                    logger.info(f"   è™•ç†ç¬¬ {idx+1}/{len(xls_data)} ç­†æ–°è³‡æ–™...")
                    
                    try:
                        sod_qty = float(row_data.get("sod_qty", 0))
                    except:
                        sod_qty = 0.0
                    
                    sod_qty_formatted = smart_number_format(sod_qty)
                    total_price = unit_price * sod_qty
                    total_price_formatted = smart_number_format(total_price)
                    unit_price_formatted = smart_number_format(unit_price)
                    
                    # æ¸…ç†å‚™è¨» (ç§»é™¤ã€Œåˆ†æ‰¹ã€å­—æ¨£)
                    old_note = first_old_row.get("å‚™è¨»", "")
                    new_note = old_note.replace("åˆ†æ‰¹1/2", "").replace("åˆ†æ‰¹2/2", "").strip()
                    
                    new_row = {
                        "Id": first_old_row.get("Id", ""),
                        "é–‹å–®ç‹€æ…‹": "V",
                        "äº¤è²¨é©—è­‰": first_old_row.get("äº¤è²¨é©—è­‰", ""),
                        "User": first_old_row.get("User", ""),
                        "ePR No.": first_old_row.get("ePR No.", ""),
                        "PO No.": po_no,
                        "Item": item,
                        "å“é …": row_data.get("description", first_old_row.get("å“é …", "")),
                        "è¦æ ¼": first_old_row.get("è¦æ ¼", ""),
                        "æ•¸é‡": sod_qty_formatted,
                        "ç¸½æ•¸": sod_qty_formatted,
                        "å–®åƒ¹": unit_price_formatted,
                        "ç¸½åƒ¹": total_price_formatted,
                        "å‚™è¨»": new_note,  # å·²ç§»é™¤ã€Œåˆ†æ‰¹ã€å­—æ¨£
                        "å­—æ•¸": first_old_row.get("å­—æ•¸", ""),
                        "isEditing": "False",
                        "backup": "{}",
                        "_alertedItemLimit": "",
                        "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": row_data.get("delivery", ""),
                        "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": sod_qty_formatted,
                        "é©—æ”¶æ•¸é‡": "",
                        "æ‹’æ”¶æ•¸é‡": "",
                        "ç™¼ç¥¨æœˆä»½": "",
                        "WBS": first_old_row.get("WBS", ""),
                        "éœ€æ±‚æ—¥": first_old_row.get("éœ€æ±‚æ—¥", ""),
                        "RTé‡‘é¡": "",
                        "RTç¸½é‡‘é¡": "",
                        "é©—æ”¶ç‹€æ…‹": "X"
                    }
                    new_data_rows.append(new_row)
                    logger.info(f"      å“é …: {new_row['å“é …']}, SOD: {new_row['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']}, ç¸½åƒ¹: {new_row['ç¸½åƒ¹']}")
                
                logger.info(f"âœ… æº–å‚™å®Œæˆ {len(new_data_rows)} ç­†æ–°è³‡æ–™")
                
                # ğŸ”´ğŸ”´ğŸ”´ ç›´æ¥åŠ åˆ°æœ€å¾Œé¢ ğŸ”´ğŸ”´ğŸ”´
                logger.info(f"ğŸ”¥ å°‡æ–°è³‡æ–™æ·»åŠ åˆ°æœ€å¾Œ...")
                df_buyer = pd.concat([
                    df_buyer,
                    pd.DataFrame(new_data_rows)
                ], ignore_index=True)
                logger.info(f"âœ… æ·»åŠ å®Œæˆ,ç¾æœ‰ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’¾ å„²å­˜å›æª”æ¡ˆ
                logger.info(f"ğŸ’¾ é–‹å§‹å„²å­˜åˆ° {BUYER_FILE}...")
                df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
                logger.info(f"âœ… å„²å­˜å®Œæˆ")
        
        except Timeout:
            logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
            return jsonify({
                "status": "error",
                "message": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"
            }), 503

        # ğŸ“ æº–å‚™å›å‚³è¨Šæ¯
        success_msg = f"æˆåŠŸåˆä½µ PO {po_no} Item {item}: åˆªé™¤ {deleted_count} ç­†åˆ†æ‰¹è³‡æ–™,æ–°å¢ {len(new_data_rows)} ç­†åˆä½µè³‡æ–™"
        
        logger.info("=" * 80)
        logger.info(f"âœ… {success_msg}")
        logger.info("=" * 80)

        # ğŸ—‘ï¸ åˆªé™¤æš«å­˜çš„ CSV æª”æ¡ˆ
        cleanup_temp_csv_files(po_no)
        
        return jsonify({
            "status": "success",
            "message": success_msg,
            "deleted_count": int(deleted_count),
            "inserted_count": len(new_data_rows),
            "merge_info": {
                "po_no": po_no,
                "item": item,
                "old_count": int(deleted_count),
                "new_count": len(new_data_rows)
            }
        })
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ confirm_merge ç™¼ç”Ÿåš´é‡éŒ¯èª¤")
        logger.error(f"âŒ éŒ¯èª¤è¨Šæ¯: {str(e)}")
        logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
        logger.error("âŒ å®Œæ•´å †ç–Šè¿½è¹¤:")
        logger.error(traceback.format_exc())
        logger.error("=" * 80)
        
        return jsonify({
            "status": "error",
            "message": f"åˆä½µå¤±æ•—: {str(e)}"
        }), 500

# æ­£å¸¸æµç¨‹
@app.route("/api/save_override_all", methods=["POST"])
def save_override_all():
    """
    Version 31 - å“åå„ªå…ˆæ¯”å°é‚è¼¯ï¼ˆå·²æ”¹é€²è™•ç†é‡è¤‡ Itemï¼‰
    æ”¹é€²ï¼šå„ªå…ˆä»¥å“åç›¸ä¼¼åº¦ç‚ºä¸»è¦æ¯”å°ä¾æ“šï¼ŒItem ä½œç‚ºæ¬¡è¦åƒè€ƒ
    å„ªå…ˆé †åºï¼šåŒ PO å…§çš„å“åé«˜åº¦ç›¸ä¼¼ > Item ç›¸åŒ > å“åä¸­åº¦ç›¸ä¼¼ > æ–°å¢
    """
    # å‚™ä»½
    # backup_files()
    data = request.get_json()
    rows = data.get("rows", [])
    confirm_override = data.get("confirm_override", False)  # æ˜¯å¦å·²ç¢ºèªè¦†è“‹

    if not rows:
        return jsonify({"status": "error", "msg": "âŒ æ²’æœ‰æ”¶åˆ°ä»»ä½•è³‡æ–™"}), 400

    def clean_text(x):
        """æ¸…ç†æ–‡å­—ï¼šç§»é™¤æ›è¡Œå’Œç©ºç™½"""
        return str(x).replace("\n", "").replace("\r", "").strip()
    
    def calculate_similarity(text1, text2):
        """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-100)"""
        text1_clean = clean_text(text1).lower()
        text2_clean = clean_text(text2).lower()
        return SequenceMatcher(None, text1_clean, text2_clean).ratio() * 100
    
    # è™•ç† pandas int64 è½‰æ›å•é¡Œ
    def convert_to_json_serializable(obj):
        """å°‡ pandas çš„ç‰¹æ®Šé¡å‹è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„é¡å‹"""
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_json_serializable(item) for item in obj]
        elif hasattr(obj, 'item'):  # numpy/pandas æ•¸å€¼é¡å‹
            return obj.item()
        elif pd.isna(obj):  # NaN å€¼
            return None
        else:
            return obj

    # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–ä¿è­·è®€å–æ“ä½œ
    try:
        with buyer_file_lock:
            # è®€å–ä¸¦è™•ç†è³‡æ–™
            df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
            df_buyer["PO No."] = df_buyer["PO No."].str.strip()
            df_buyer["Item"] = (
                df_buyer["Item"]
                .str.replace(r"\.0$", "", regex=True)
                .str.strip()
                .apply(lambda x: x.zfill(4) if x.isdigit() else x)
            )
            
            # ğŸ”´ é‡è¦ï¼šå»ºç«‹ä¸€å€‹åªåŒ…å«ç‹€æ…‹ç‚º V çš„è³‡æ–™ç´¢å¼•
            df_active = df_buyer[df_buyer["é–‹å–®ç‹€æ…‹"] == "V"].copy()
            logger.info(f"ç¸½è³‡æ–™ç­†æ•¸: {len(df_buyer)}, æœ‰æ•ˆè³‡æ–™(ç‹€æ…‹=V): {len(df_active)}")

    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ è®€å– Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"è®€å–æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500


    # âœ… ğŸ†• **åœ¨é€™è£¡æ·»åŠ åˆå§‹åŒ– all_group_results**
    all_group_results = []  # ç”¨ä¾†æ”¶é›†æ‰€æœ‰ PO çš„æ¯”å°çµæœ

    updated_count = 0
    inserted_count = 0
    failed = []
    need_confirm_items = []  # éœ€è¦ç¢ºèªçš„é …ç›®
    auto_updated_items = []  # ğŸ†• è‡ªå‹•æ›´æ–°çš„é …ç›®
    matching_output = []  # æ¯”å°çµæœè¼¸å‡º

    new_item = ''
    epr_no = 0
    po_no_new = ''
    
    # è¼¸å‡ºé–‹å§‹è¨Šæ¯
    logger.info("\n" + "="*80)
    logger.info("é–‹å§‹è™•ç†è³‡æ–™æ¯”å° (Version 31 - å“åå„ªå…ˆ)")
    logger.info("="*80)

    for row_num, row in enumerate(rows, 1):
        id_ = row.get("id", "").strip()
        po_no_new = row.get("po_no", "").strip()
        item_new = row.get("item", "").strip()
        
        # ç¢ºä¿ item æ ¼å¼ä¸€è‡´ï¼ˆ4ä½æ•¸ï¼‰
        if item_new.isdigit():
            item_new = item_new.zfill(4)
            
        new_delivery = row.get("delivery_date", "").strip()
        new_qty = row.get("sod_qty", "").strip()
        new_desc = row.get("po_description", "").strip()
        new_desc_clean = clean_text(new_desc)

        target_idx = None
        match_reason = ""
        
        # è¼¸å‡ºç•¶å‰è™•ç†é …ç›®
        logger.info(f"\n[ç¬¬ {row_num} ç­†]")
        logger.info(f"  æ–°è³‡æ–™ => PO: {po_no_new}, Item: {item_new}")
        logger.info(f"  å“å: {new_desc[:50]}{'...' if len(new_desc) > 50 else ''}")
        
        # ğŸ” Version 31 æ ¸å¿ƒæ”¹è®Šï¼šå…ˆæ‰¾å“åç›¸ä¼¼åº¦ï¼Œå†è€ƒæ…® Item
        # æ­¥é©Ÿ1ï¼šå…ˆåœ¨åŒ PO å…§æ‰¾è³‡æ–™ï¼ˆåªæ‰¾ç‹€æ…‹ç‚º V çš„ï¼‰
        po_group = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        
        if not po_group.empty:
            logger.info(f"     åœ¨ PO {po_no_new} æ‰¾åˆ° {len(po_group)} ç­†è³‡æ–™")
            
            # ğŸ”¥ Version 31ï¼šè¨ˆç®—æ‰€æœ‰é …ç›®çš„å“åç›¸ä¼¼åº¦
            similarity_scores = []
            for idx, row_data in po_group.iterrows():
                existing_desc = row_data["å“é …"]
                existing_item = row_data["Item"]
                similarity = calculate_similarity(new_desc, existing_desc)
                similarity_scores.append({
                    'index': idx,
                    'item': existing_item,
                    'desc': existing_desc,
                    'similarity': similarity,
                    'item_match': (existing_item == item_new),  # è¨˜éŒ„ Item æ˜¯å¦ç›¸åŒ
                    'delivery_date': row_data.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")  # åŠ å…¥äº¤æœŸè³‡è¨Š
                })
            
            # ğŸ”´ğŸ”´ğŸ”´ é€™è£¡æ˜¯ä¿®æ”¹çš„é‡é» ğŸ”´ğŸ”´ğŸ”´
            # Version 31 æ”¹é€²ï¼šè™•ç†ç›¸åŒ Item çš„å¤šç­†è³‡æ–™
            # å¦‚æœæœ‰å¤šç­†å®Œå…¨ç›¸åŒçš„ Itemï¼Œå„ªå…ˆè€ƒæ…®å“åç›¸ä¼¼åº¦ï¼Œå†è€ƒæ…®äº¤æœŸ
            item_matches = [s for s in similarity_scores if s['item_match']]
            if len(item_matches) > 1:
                logger.info(f"     ç™¼ç¾ {len(item_matches)} ç­†ç›¸åŒçš„ Item {item_new}")
                
                # ğŸ†• è¨ˆç®—æ¯ç­†çš„å“åç›¸ä¼¼åº¦
                for match in item_matches:
                    match['name_similarity'] = calculate_similarity(new_desc, match['desc'])
                    
                    # è™•ç†äº¤æœŸ
                    try:
                        date_str = match['delivery_date'].strip()
                        if date_str:
                            date_str = date_str.split(' ')[0]
                            date_str = date_str.replace('/', '-')
                            match['parsed_date'] = date_str
                        else:
                            match['parsed_date'] = '1900-01-01'
                    except:
                        match['parsed_date'] = '1900-01-01'
                    
                    logger.info(f"       - Index {match['index']}: å“åç›¸ä¼¼åº¦ {match['name_similarity']:.1f}%, äº¤æœŸ {match['delivery_date']}")
                
                # ğŸ†• æ”¹é€²çš„é¸æ“‡é‚è¼¯
                # 1. å…ˆæ‰¾å“åå®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„ï¼ˆâ‰¥95%ï¼‰
                exact_matches = [m for m in item_matches if m['name_similarity'] >= 95]
                
                if exact_matches:
                    # å¦‚æœæœ‰å“åå¹¾ä¹ç›¸åŒçš„ï¼Œå¾ä¸­é¸æ“‡äº¤æœŸæœ€æ–°çš„
                    exact_matches.sort(key=lambda x: x.get('parsed_date', '1900-01-01'), reverse=True)
                    newest_match = exact_matches[0]
                    logger.info(f"     => âœ… é¸æ“‡å“åç›¸åŒä¸”äº¤æœŸæœ€æ–°çš„è³‡æ–™")
                    logger.info(f"        Index {newest_match['index']}")
                    logger.info(f"        å“åç›¸ä¼¼åº¦: {newest_match['name_similarity']:.1f}%")
                    logger.info(f"        äº¤æœŸ: {newest_match['delivery_date']}")
                else:
                    # å¦‚æœæ²’æœ‰å“åç›¸åŒçš„ï¼Œé¸æ“‡å“åæœ€ç›¸ä¼¼çš„ï¼ˆä½†è¦è­¦å‘Šï¼‰
                    item_matches.sort(key=lambda x: (x['name_similarity'], x.get('parsed_date', '1900-01-01')), reverse=True)
                    newest_match = item_matches[0]
                    
                    if newest_match['name_similarity'] < 60:
                        logger.info(f"     => âš ï¸âš ï¸ è­¦å‘Šï¼šç›¸åŒ Item ä½†å“åå·®ç•°å¾ˆå¤§ï¼")
                        logger.info(f"        Index {newest_match['index']}")
                        logger.info(f"        å“åç›¸ä¼¼åº¦åƒ…: {newest_match['name_similarity']:.1f}%")
                        logger.info(f"        åŸå“å: {newest_match['desc'][:50]}...")
                        logger.info(f"        æ–°å“å: {new_desc[:50]}...")
                        logger.info(f"        å»ºè­°æ‰‹å‹•æª¢æŸ¥ï¼")
                    else:
                        logger.info(f"     => âš ï¸ é¸æ“‡å“åæœ€ç›¸ä¼¼çš„è³‡æ–™")
                        logger.info(f"        Index {newest_match['index']}")
                        logger.info(f"        å“åç›¸ä¼¼åº¦: {newest_match['name_similarity']:.1f}%")
                
                # å°‡é¸ä¸­çš„è³‡æ–™ç§»åˆ° similarity_scores çš„æœ€å‰é¢
                similarity_scores = [s for s in similarity_scores if not s['item_match']]
                similarity_scores.insert(0, newest_match)
            else:
                # æ’åºï¼šå…ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œç›¸ä¼¼åº¦ç›¸åŒæ™‚ Item ç›¸åŒçš„å„ªå…ˆ
                similarity_scores.sort(key=lambda x: (x['similarity'], x['item_match']), reverse=True)
            # ğŸ”´ğŸ”´ğŸ”´ ä¿®æ”¹çµæŸ ğŸ”´ğŸ”´ğŸ”´
            
            # è¼¸å‡ºç›¸ä¼¼åº¦æ’åï¼ˆé™¤éŒ¯ç”¨ï¼‰
            logger.info(f"     å“åç›¸ä¼¼åº¦æ’åï¼š")
            for i, score in enumerate(similarity_scores[:3], 1):  # é¡¯ç¤ºå‰3å
                item_marker = " [Itemç›¸åŒ]" if score['item_match'] else ""
                logger.info(f"       {i}. Item {score['item']}: {score['similarity']:.1f}%{item_marker} - {score['desc'][:30]}...")
            
            # å–å¾—æœ€é«˜ç›¸ä¼¼åº¦çš„é …ç›®
            best_match = similarity_scores[0]
            best_similarity = best_match['similarity']
            best_idx = best_match['index']
            best_item = best_match['item']
            best_desc = best_match['desc']
            
            # ğŸ¯ æ ¹æ“šç›¸ä¼¼åº¦å’Œ Item æ˜¯å¦ç›¸åŒä¾†æ±ºå®šè™•ç†æ–¹å¼
            if best_similarity >= 95:  # å“åå¹¾ä¹å®Œå…¨ç›¸åŒ
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = "å“åå®Œå…¨ç›¸åŒ+Itemç›¸åŒ"
                    logger.info(f"  âœ… å“åå®Œå…¨ç›¸åŒä¸” Item ç›¸åŒï¼ˆç›¸ä¼¼åº¦ {best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                    
                    # ğŸ†• åŠ å…¥è‡ªå‹•æ›´æ–°æ¸…å–®
                    auto_updated_items.append({
                        "row": row_num,
                        "po_no": po_no_new,
                        "new_item": item_new,
                        "old_item": best_item,
                        "new_desc": new_desc,
                        "old_desc": best_desc,
                        "similarity": round(best_similarity, 1),
                        "new_delivery": new_delivery,
                        "new_qty": new_qty,
                        "target_index": int(best_idx),
                        "reason": "å“åèˆ‡Itemå®Œå…¨ç›¸åŒ",
                        "action_type": "auto_updated"
                    })
                else:
                    match_reason = f"å“åå®Œå…¨ç›¸åŒ(Item:{best_item}â†’{item_new})"
                    logger.info(f"  âš ï¸ å“åå®Œå…¨ç›¸åŒä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–° Item")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åç›¸åŒä½†Itemä¸åŒ",
                            "action_type": "update_item_change"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸åŒä½†Itemä¸åŒ"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–° Item")
                        
            elif best_similarity >= 80:  # å“åé«˜åº¦ç›¸ä¼¼
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼+Itemç›¸åŒ({best_similarity:.0f}%)"
                    logger.info(f"  âœ… å“åé«˜åº¦ç›¸ä¼¼ä¸” Item ç›¸åŒï¼ˆ{best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                    
                    # ğŸ†• åŠ å…¥è‡ªå‹•æ›´æ–°æ¸…å–®
                    auto_updated_items.append({
                        "row": row_num,
                        "po_no": po_no_new,
                        "new_item": item_new,
                        "old_item": best_item,
                        "new_desc": new_desc,
                        "old_desc": best_desc,
                        "similarity": round(best_similarity, 1),
                        "new_delivery": new_delivery,
                        "new_qty": new_qty,
                        "target_index": int(best_idx),
                        "reason": f"å“åé«˜åº¦ç›¸ä¼¼({best_similarity:.0f}%)ä¸”Itemç›¸åŒ",
                        "action_type": "auto_updated"
                    })
                else:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼(Item:{best_item}â†’{item_new})"
                    logger.info(f"  âš ï¸ å“åé«˜åº¦ç›¸ä¼¼ä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    logger.info(f"     åŸå“å: {best_desc[:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åé«˜åº¦ç›¸ä¼¼ä½†Itemä¸åŒ",
                            "action_type": "update_high_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸ä¼¼{best_similarity:.0f}%"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            elif best_similarity >= 60:  # å“åä¸­åº¦ç›¸ä¼¼
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item ç›¸åŒçš„é …ç›®
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match and item_match['similarity'] >= 40:
                    # å¦‚æœæœ‰ Item ç›¸åŒä¸”ç›¸ä¼¼åº¦ä¸æ˜¯å¤ªä½ï¼Œå„ªå…ˆé¸æ“‡ Item ç›¸åŒçš„
                    target_idx = item_match['index']
                    match_reason = f"Itemç›¸åŒ+å“åç›¸ä¼¼({item_match['similarity']:.0f}%)"
                    logger.info(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒçš„é …ç›®ï¼Œå“åç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%")
                    logger.info(f"     åŸå“å: {item_match['desc'][:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå·®ç•°è¼ƒå¤§",
                            "action_type": "update_medium_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åå·®ç•°{item_match['similarity']:.0f}%"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                else:
                    # å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒ
                    target_idx = best_idx
                    match_reason = f"å“åä¸­åº¦ç›¸ä¼¼({best_similarity:.0f}%)"
                    logger.info(f"  âš ï¸ å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     ç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    logger.info(f"     åŸå“å: {best_desc[:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åä¸­åº¦ç›¸ä¼¼",
                            "action_type": "update_medium_similarity",
                            "warning": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åä¸­åº¦ç›¸ä¼¼"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            else:  # ç›¸ä¼¼åº¦ < 60%
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item å®Œå…¨ç›¸åŒçš„
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match:
                    # Item ç›¸åŒä½†å“åç›¸ä¼¼åº¦ä½
                    logger.info(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒä½†å“åå·®ç•°å¾ˆå¤§ï¼ˆç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%ï¼‰")
                    logger.info(f"     åŸå“å: {item_match['desc'][:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     âš ï¸âš ï¸ å“åå·®ç•°å¾ˆå¤§ï¼éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå®Œå…¨ä¸åŒ",
                            "action_type": "update_low_similarity",
                            "warning": True,
                            "critical": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": f"Itemç›¸åŒ(ç›¸ä¼¼åº¦{item_match['similarity']:.0f}%)",
                            "action": "å¾…ç¢ºèª",
                            "note": f"âŒå“åå·®ç•°æ¥µå¤§"
                        })
                        continue
                    else:
                        target_idx = item_match['index']
                        match_reason = f"Itemç›¸åŒ(å“åå·®ç•°å¤§)"
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡å¼·åˆ¶æ›´æ–°")
                else:
                    # æ²’æœ‰ä»»ä½•åŒ¹é…ï¼Œå»ºè­°æ–°å¢
                    target_idx = None
        
        # æ­¥é©Ÿ2ï¼šå¦‚æœåœ¨åŒ PO å…§æ‰¾ä¸åˆ°åŒ¹é…ï¼Œè©¢å•æ˜¯å¦æ–°å¢
        if target_idx is None and not po_group.empty:
            logger.info(f"  âš ï¸  åœ¨ PO {po_no_new} å…§æ‰¾ä¸åˆ°ç›¸ä¼¼çš„å“åæˆ–ç›¸åŒçš„ Item")
            logger.info(f"     æ–°Item: {item_new}, æ–°å“å: {new_desc[:40]}...")
            
            if not confirm_override:
                logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ–°å¢ç‚ºæ–°é …ç›®")
                
                # åˆ—å‡ºç¾æœ‰çš„é …ç›®ä¾›åƒè€ƒ
                existing_items = []
                for score in similarity_scores[:5]:  # é¡¯ç¤ºå‰5å€‹ç›¸ä¼¼åº¦æœ€é«˜çš„
                    existing_items.append({
                        "item": score['item'],
                        "desc": score['desc'][:50],
                        "similarity": round(score['similarity'], 1)
                    })
                
                need_confirm_items.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "new_item": item_new,
                    "new_desc": new_desc,
                    "existing_items": existing_items,
                    "new_delivery": new_delivery,
                    "new_qty": new_qty,
                    "reason": "ç„¡ç›¸ä¼¼é …ç›®",
                    "action_type": "add_new"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡åŒ¹é…",
                    "action": "å¾…ç¢ºèª",
                    "note": "å»ºè­°æ–°å¢"
                })
                continue
            else:
                logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ–°å¢ç‚ºæ–°é …ç›®")
        
        # æ­¥é©Ÿ3ï¼šå…¶ä»–æ¯”å°æ–¹å¼ï¼ˆIDã€æ¨¡ç³Šæ¯”å°ç­‰ï¼‰- åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None and id_:
            candidates = df_active[df_active["Id"] == id_].copy()
            
            if len(candidates) > 1:
                candidates["å“é …_clean"] = candidates["å“é …"].apply(clean_text)
                exact_match = candidates[candidates["å“é …_clean"] == new_desc_clean]
                if len(exact_match) == 1:
                    target_idx = exact_match.index[0]
                    match_reason = "ID+å“é …åŒ¹é…"
                    logger.info(f"  âœ… ID+å“é …åŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")
            elif len(candidates) == 1:
                target_idx = candidates.index[0]
                match_reason = "IDåŒ¹é…"
                logger.info(f"  âœ… IDåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # æ­¥é©Ÿ4ï¼šPO + å“é …æ¨¡ç³Šæ¯”å° - åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None:
            po_match = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
            po_match = po_match[po_match["å“é …"].apply(lambda x: fuzzy_in(x, new_desc_clean))]

            if not po_match.empty:
                target_idx = po_match.index[0]
                match_reason = "PO+å“é …æ¨¡ç³ŠåŒ¹é…"
                logger.info(f"  âœ… PO+å“é …æ¨¡ç³ŠåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # ğŸ” å¦‚æœæ‰¾åˆ°åŒ¹é…é …ç›®ï¼ŒåŸ·è¡Œæ›´æ–°
        if target_idx is not None:
            # è¨˜éŒ„åŸå§‹å€¼ï¼ˆç”¨æ–¼è¼¸å‡ºï¼‰
            old_values = {
                "po": df_buyer.at[target_idx, "PO No."],
                "item": df_buyer.at[target_idx, "Item"],
                "delivery": df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"],
                "qty": df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"],
                "desc": df_buyer.at[target_idx, "å“é …"]
            }
            
            # åŸ·è¡Œæ›´æ–°
            df_buyer.at[target_idx, "PO No."] = po_no_new
            df_buyer.at[target_idx, "Item"] = item_new
            df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = new_delivery
            df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = new_qty
            if new_desc:
                df_buyer.at[target_idx, "å“é …"] = new_desc
            updated_count += 1
            
            # è¼¸å‡ºè®Šæ›´è©³æƒ…
            if old_values["po"] != po_no_new:
                logger.info(f"     POè®Šæ›´: {old_values['po']} â†’ {po_no_new}")
            if old_values["item"] != item_new:
                logger.info(f"     Itemè®Šæ›´: {old_values['item']} â†’ {item_new}")
            if old_values["delivery"] != new_delivery:
                logger.info(f"     äº¤æœŸè®Šæ›´: {old_values['delivery']} â†’ {new_delivery}")
            if old_values["qty"] != new_qty:
                logger.info(f"     æ•¸é‡è®Šæ›´: {old_values['qty']} â†’ {new_qty}")
            
            matching_output.append({
                "row": row_num,
                "po": po_no_new,
                "item": item_new if old_values["item"] == item_new else f"{old_values['item']}â†’{item_new}",
                "match": match_reason,
                "action": "å·²æ›´æ–°",
                "note": "å“åå·²è®Šæ›´" if old_values["desc"] != new_desc else ""
            })
            continue

        # ğŸ†• å¦‚æœéƒ½æ‰¾ä¸åˆ° â†’ æ–°å¢è³‡æ–™ï¼ˆä½†è¦æª¢æŸ¥ PO æ˜¯å¦å­˜åœ¨æ–¼ç‹€æ…‹ V çš„è³‡æ–™ä¸­ï¼‰
        po_matches = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        if po_matches.empty:
            # å†æª¢æŸ¥æ˜¯å¦æœ‰ç‹€æ…‹ç‚º X çš„ç›¸åŒ PO
            po_cancelled = df_buyer[
                (df_buyer["é–‹å–®ç‹€æ…‹"] == "X") & 
                (df_buyer["PO No."].apply(lambda x: is_po_in_record(x, po_no_new)))
            ]
            
            if not po_cancelled.empty:
                logger.info(f"  âš ï¸  æ‰¾åˆ° PO {po_no_new} ä½†ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰ï¼Œç„¡æ³•æ›´æ–°")
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": f"PO {po_no_new} ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "POå·²å–æ¶ˆ",
                    "action": "å¤±æ•—",
                    "note": "ç‹€æ…‹ç‚ºX"
                })
                continue
            else:
                # ğŸ”´ Version 31ï¼šPO å®Œå…¨ä¸å­˜åœ¨çš„æƒ…æ³
                logger.info(f"  âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®ï¼šPO {po_no_new}")
                
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡PO",
                    "action": "å¤±æ•—",
                    "note": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                continue
        
        # ğŸ†“ æ¨æ¸¬ Idï¼ˆå–é¦–ç­†ï¼‰
        possible_ids = po_matches["Id"].dropna().unique().tolist()
        id_ = possible_ids[0] if possible_ids else row.get("id") or row.get("Id", "")
        id_ = str(id_).strip()

        # ğŸ‘¤ å–åŒçµ„ç¬¬ä¸€ç­†çš„è³‡è¨Š
        user = po_matches["User"].iloc[0] if not po_matches.empty else ""
        epr_no = po_matches["ePR No."].iloc[0] if not po_matches.empty else ""
        wbs_no = po_matches["WBS"].iloc[0] if not po_matches.empty else ""
        need_day_no = po_matches["éœ€æ±‚æ—¥"].iloc[0] if not po_matches.empty else ""

        logger.info(f"  ğŸ†• æ‰¾ä¸åˆ°åŒ¹é…é …ç›® => æ–°å¢è³‡æ–™")
        logger.info(f"     æ–°å¢åˆ° ePR No.: {epr_no}")

        # â• æ–°å¢æ–°çš„ä¸€ç­†è³‡æ–™
        new_row = {
            "Id": id_,
            "é–‹å–®ç‹€æ…‹": "V",
            "äº¤è²¨é©—è­‰": "",
            "User": user,
            "ePR No.": epr_no,
            "PO No.": po_no_new,
            "Item": item_new,
            "å“é …": new_desc,
            "è¦æ ¼": "",
            "æ•¸é‡": new_qty,
            "ç¸½æ•¸": new_qty,
            "å–®åƒ¹": "",
            "ç¸½åƒ¹": "",
            "å‚™è¨»": "",
            "å­—æ•¸": "",
            "isEditing": "False",
            "backup": "{}",
            "_alertedItemLimit": "",
            "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": new_delivery,
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": new_qty,
            "é©—æ”¶æ•¸é‡": "",
            "æ‹’æ”¶æ•¸é‡": "",
            "ç™¼ç¥¨æœˆä»½": "",
            "WBS": wbs_no,
            "éœ€æ±‚æ—¥": need_day_no,
            "RTé‡‘é¡": '',
            "RTç¸½é‡‘é¡": '',
            "é©—æ”¶ç‹€æ…‹": "X" 
        }

        # ğŸ“Œ æ‰¾é€™å€‹ id çš„æœ€å¾Œä¸€ç­†ä½ç½®
        same_id_idx = df_buyer[df_buyer["Id"] == id_].index
        insert_pos = same_id_idx[-1] + 1 if len(same_id_idx) > 0 else len(df_buyer)

        # âœ¨ æ’å…¥åˆ°åŸ df_buyer ä¸­æŒ‡å®šä½ç½®
        df_buyer = pd.concat([
            df_buyer.iloc[:insert_pos],
            pd.DataFrame([new_row]),
            df_buyer.iloc[insert_pos:]
        ], ignore_index=True)
        
        new_item = 'æ–°å¢ç‰©ä»¶'
        inserted_count += 1
        
        matching_output.append({
            "row": row_num,
            "po": po_no_new,
            "item": item_new,
            "match": "ç„¡åŒ¹é…",
            "action": "å·²æ–°å¢",
            "note": f"ePR:{epr_no}"
        })

    # è¼¸å‡ºæ¯”å°çµæœæ‘˜è¦
    logger.info("\n" + "="*80)
    logger.info("æ¯”å°çµæœæ‘˜è¦ (Version 31 - å“åå„ªå…ˆ)")
    logger.info("="*80)
    logger.info(f"ç¸½è™•ç†ç­†æ•¸: {len(rows)}")
    logger.info(f"æ›´æ–°ç­†æ•¸: {updated_count}")
    logger.info(f"æ–°å¢ç­†æ•¸: {inserted_count}")
    logger.info(f"å¤±æ•—ç­†æ•¸: {len(failed)}")
    logger.info(f"å¾…ç¢ºèªç­†æ•¸: {len(need_confirm_items)}")
    logger.info(f"è‡ªå‹•æ›´æ–°ç­†æ•¸: {len(auto_updated_items)}")  # 
    
    # è¼¸å‡ºè©³ç´°æ¯”å°è¡¨æ ¼
    if matching_output:
        logger.info("\nè©³ç´°æ¯”å°çµæœ:")
        logger.info("-"*80)
        logger.info(f"{'ç­†æ•¸':<5} {'PO No.':<15} {'Item':<10} {'æ¯”å°æ–¹å¼':<20} {'è™•ç†':<8} {'å‚™è¨»'}")
        logger.info("-"*80)
        for item in matching_output:
            logger.info(f"{item['row']:<5} {item['po']:<15} {item['item']:<10} {item['match']:<20} {item['action']:<8} {item['note']}")
    
    logger.info("="*80 + "\n")

    # å¦‚æœæœ‰éœ€è¦ç¢ºèªçš„é …ç›®ï¼Œå›å‚³çµ¦å‰ç«¯
    if need_confirm_items and not confirm_override:
        # æª¢æŸ¥æ˜¯å¦æœ‰é—œéµç¢ºèªé …ï¼ˆå“åå®Œå…¨ä¸åŒï¼‰
        critical_items = [item for item in need_confirm_items if item.get("critical", False)]
        warning_items = [item for item in need_confirm_items if item.get("warning", False)]
        
        # æ ¹æ“šä¸åŒçš„ç¢ºèªåŸå› å’Œå‹•ä½œé¡å‹ï¼Œç”¢ç”Ÿä¸åŒçš„è¨Šæ¯
        action_types = set(item.get("action_type", "") for item in need_confirm_items)
        
        if critical_items:
            msg = f"âš ï¸ ç™¼ç¾ {len(critical_items)} å€‹å“åå®Œå…¨ä¸åŒçš„é …ç›®éœ€è¦ç‰¹åˆ¥ç¢ºèª"
        elif "update_low_similarity" in action_types:
            msg = f"âŒ ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸ä¼¼åº¦æ¥µä½çš„é …ç›®éœ€è¦ç¢ºèª"
        elif "update_item_change" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸åŒä½†Itemä¸åŒçš„é …ç›®éœ€è¦ç¢ºèª"
        elif "add_new" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹é …ç›®å¯èƒ½éœ€è¦æ–°å¢"
        else:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹éœ€è¦ç¢ºèªçš„é …ç›®"
        
        return jsonify(convert_to_json_serializable({
            "status": "confirm_needed",
            "msg": msg,
            "items": need_confirm_items,
            "auto_updated": auto_updated_items,  # ğŸ†• æ–°å¢è‡ªå‹•æ›´æ–°çš„é …ç›®
            "updated": updated_count,
            "inserted": inserted_count,
            "matching_output": matching_output,
            "has_critical": len(critical_items) > 0,
            "has_warning": len(warning_items) > 0
        }))

    # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–ä¿è­·å„²å­˜æ“ä½œ
    try:
        with buyer_file_lock:
            # å„²å­˜å›æª”æ¡ˆ
            df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–é€²è¡Œå„²å­˜,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,ç„¡æ³•å„²å­˜,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ å„²å­˜ Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"å„²å­˜æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500
    
    # åˆªé™¤æš«å­˜æª”æ¡ˆ
    if po_no_new:
        temp_file = f"uploads/{po_no_new}.csv"
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    # ğŸ”´ æª¢æŸ¥æ˜¯å¦æ‰€æœ‰é …ç›®éƒ½å¤±æ•—ä¸”åŸå› éƒ½æ˜¯ã€Œ360è¡¨å–®ç„¡æ­¤é …ç›®ã€
    if failed and len(failed) == len(rows):
        all_not_found = all(f.get("reason") == "360è¡¨å–®ç„¡æ­¤é …ç›®" for f in failed)
        if all_not_found:
            return jsonify(convert_to_json_serializable({
                "status": "not_found",
                "msg": "âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®",
                "failed": failed,
                "matching_output": matching_output
            }))
    
    # å›å‚³çµæœ
    if new_item == 'æ–°å¢ç‰©ä»¶':
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"æœ‰æ–°å¢çš„ç‰©ä»¶(item)éœ€è¦ç¶­è­·ï¼ŒePR No. å–®è™Ÿç‚º {epr_no}ã€‚æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output,
            "auto_updated": auto_updated_items  # ğŸ†• ä¹Ÿåœ¨æˆåŠŸæ™‚å›å‚³
        }))
    else:
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"âœ… æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output,
            "auto_updated": auto_updated_items  # ğŸ†• ä¹Ÿåœ¨æˆåŠŸæ™‚å›å‚³
        }))




# eRT é©—æ”¶è¡¨å–®
# === è¨­å®š logger ï¼Œé‡å° eRT é©—æ”¶è¡¨å–®===
log_file_path = "buyer_detail_update_log.log"
logging.basicConfig(
    filename=log_file_path,
    filemode='a',
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    encoding='utf-8'
)
logger = logging.getLogger("BuyerDetailUpdater")

# 2025/11/03ä¿®æ­£
@app.route('/api/update-buyer-items', methods=['POST'])
def update_buyer_items():
    """
    æ›´æ–° Buyer_detail.csv - å…ˆåˆªé™¤åŒ Id çš„æ‰€æœ‰èˆŠè³‡æ–™ï¼Œå†å¯«å…¥æ–°è³‡æ–™
    """
    try:
        data = request.json
        item_id = data.get('Id') # type: ignore
        new_items = data.get('items', []) # type: ignore
        username = data.get('username', '') # type: ignore
        
        if not item_id:
            return jsonify({'status': 'error', 'success': False, 'message': 'ç¼ºå°‘ Id'}), 400
        
        if not new_items or len(new_items) == 0:
            return jsonify({'status': 'error', 'success': False, 'message': 'æ²’æœ‰è¦æ›´æ–°çš„è³‡æ–™'}), 400
        
        logger.info(f"ğŸ”„ ä½¿ç”¨è€… {username} è«‹æ±‚æ›´æ–° Id: {item_id}ï¼Œå…± {len(new_items)} ç­†è³‡æ–™")
        
        # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–
        with buyer_file_lock:
            logger.info(f"ğŸ”’ å·²å–å¾—æª”æ¡ˆé–ï¼Œé–‹å§‹æ›´æ–°...")
            
            # è®€å– CSV
            df = pd.read_csv(BUYER_FILE, encoding='utf-8-sig', dtype=str)
            df.columns = df.columns.str.strip()
            
            # â­ æ­¥é©Ÿ1: åˆªé™¤åŒ Id çš„æ‰€æœ‰èˆŠè³‡æ–™
            mask = df['Id'].astype(str).str.strip() == str(item_id).strip()
            old_count = mask.sum()
            
            if old_count > 0:
                logger.info(f"ğŸ“ æ‰¾åˆ° {old_count} ç­†èˆŠè³‡æ–™ï¼Œæº–å‚™åˆªé™¤")
            
            df = df[~mask]
            logger.info(f"âœ… å·²åˆªé™¤ {old_count} ç­†èˆŠè³‡æ–™")
            
            # â­ æ­¥é©Ÿ2: æº–å‚™æ–°è³‡æ–™
            new_rows = []
            for item in new_items:
                # ç§»é™¤å‰ç«¯çš„ç‰¹æ®Šæ¬„ä½
                item.pop('backup', None)
                item.pop('isEditing', None)
                item.pop('_alertedItemLimit', None)
                
                # ç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½å­˜åœ¨
                new_row = {}
                for col in df.columns:
                    new_row[col] = str(item.get(col, '')) if item.get(col) is not None else ''
                new_rows.append(new_row)
                logger.info(f"  æ–°è³‡æ–™: Item={item.get('Item')}, å“é …={item.get('å“é …')}, ç¸½åƒ¹={item.get('ç¸½åƒ¹')}")
            
            # â­ æ­¥é©Ÿ3: åŠ å…¥æ–°è³‡æ–™
            new_df = pd.DataFrame(new_rows)
            df = pd.concat([df, new_df], ignore_index=True)
            logger.info(f"âœ… å·²åŠ å…¥ {len(new_items)} ç­†æ–°è³‡æ–™")
            
            # ç¢ºä¿æ¬„ä½é †åºæ­£ç¢º
            final_columns = ['Id', 'é–‹å–®ç‹€æ…‹', 'äº¤è²¨é©—è­‰', 'User', 'ePR No.', 'PO No.', 'Item', 'å“é …', 'è¦æ ¼', 
                           'æ•¸é‡', 'ç¸½æ•¸', 'å–®åƒ¹', 'ç¸½åƒ¹', 'å‚™è¨»', 'å­—æ•¸', 'isEditing', 'backup', '_alertedItemLimit', 
                           'Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ', 'SOD Qty å» å•†æ‰¿è«¾æ•¸é‡', 'é©—æ”¶æ•¸é‡', 'æ‹’æ”¶æ•¸é‡', 
                           'ç™¼ç¥¨æœˆä»½', 'WBS', 'éœ€æ±‚æ—¥', 'RTé‡‘é¡', 'RTç¸½é‡‘é¡', 'é©—æ”¶ç‹€æ…‹']
            
            # ç¢ºä¿æ‰€æœ‰æ¬„ä½å­˜åœ¨
            for col in final_columns:
                if col not in df.columns:
                    df[col] = ''
            
            # é‡æ–°æ’åº
            df = df[final_columns]
            
            # å„²å­˜
            df.to_csv(BUYER_FILE, index=False, encoding='utf-8-sig', na_rep='')
            
            logger.info(f"âœ… æ›´æ–°æˆåŠŸ! (åˆªé™¤ {old_count} ç­† + æ–°å¢ {len(new_items)} ç­†)")
            logger.info(f"ğŸ”“ é‡‹æ”¾æª”æ¡ˆé–")
        
        # â­â­â­ é—œéµä¿®æ”¹ï¼šå°‡ int64 è½‰æ›æˆ int â­â­â­
        return jsonify({
            'status': 'success',
            'success': True,
            'message': 'æ›´æ–°æˆåŠŸ',
            'msg': f'å·²åˆªé™¤ {old_count} ç­†èˆŠè³‡æ–™ï¼Œæ–°å¢ {len(new_items)} ç­†è³‡æ–™',
            'deleted_count': int(old_count),      # â­ åŠ  int() è½‰æ›
            'added_count': len(new_items)
        }), 200
        
    except Timeout:
        error_msg = 'æª”æ¡ˆæ­£åœ¨è¢«å…¶ä»–ç¨‹åºä½¿ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦'
        logger.info(f"â±ï¸ {error_msg}")
        return jsonify({'status': 'error', 'success': False, 'message': error_msg}), 503
        
    except Exception as e:
        error_msg = str(e)
        logger.info(f"âŒ æ›´æ–°å¤±æ•—: {error_msg}")
        traceback.print_exc()
        return jsonify({'status': 'error', 'success': False, 'message': error_msg}), 500


# 2025/11/03ä¿®æ­£
@app.route('/api/delete-buyer-item-exact', methods=['POST'])
def delete_buyer_item_exact():
    """
    åˆªé™¤ Buyer_detail.csv - ç²¾ç¢ºæ¯”å°æ‰€æœ‰æ¬„ä½ï¼Œæ‰¾åˆ°å®Œå…¨ä¸€æ¨£çš„æ‰åˆªé™¤
    """
    try:
        data = request.json
        item_to_delete = data.get('item', {}) # type: ignore
        username = data.get('username', '') # type: ignore
        
        if not item_to_delete or not item_to_delete.get('Id'):
            return jsonify({'status': 'error', 'success': False, 'message': 'ç¼ºå°‘è³‡æ–™æˆ– Id'}), 400
        
        item_id = item_to_delete.get('Id')
        logger.info(f"ğŸ—‘ï¸ ä½¿ç”¨è€… {username} è«‹æ±‚åˆªé™¤ Id: {item_id}, Item: {item_to_delete.get('Item')}")
        
        # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–
        with buyer_file_lock:
            logger.info(f"ğŸ”’ å·²å–å¾—æª”æ¡ˆé–ï¼Œé–‹å§‹åˆªé™¤...")
            
            # è®€å– CSV
            df = pd.read_csv(BUYER_FILE, encoding='utf-8-sig', dtype=str)
            df.columns = df.columns.str.strip()
            
            # è¨˜éŒ„åˆªé™¤å‰çš„ç­†æ•¸
            df_before_count = len(df)
            
            # â­ å»ºç«‹æ¯”å°é®ç½©ï¼šæ¯”å°æ‰€æœ‰æ¬„ä½
            # éœ€è¦æ¯”å°çš„æ¬„ä½ï¼ˆæ’é™¤å‰ç«¯ç‰¹æ®Šæ¬„ä½ï¼‰
            ignore_fields = ['backup', 'isEditing', '_alertedItemLimit']
            
            # æ‰¾åˆ°æ‰€æœ‰éœ€è¦æ¯”å°çš„æ¬„ä½
            fields_to_compare = [col for col in df.columns if col not in ignore_fields]
            
            logger.info(f"ğŸ“‹ æº–å‚™æ¯”å° {len(fields_to_compare)} å€‹æ¬„ä½")
            
            # å»ºç«‹æ¯”å°æ¢ä»¶
            mask = pd.Series([True] * len(df))
            
            matched_fields = []
            for field in fields_to_compare:
                if field in item_to_delete:
                    expected_value = str(item_to_delete[field]) if item_to_delete[field] is not None else ''
                    
                    # è™•ç† CSV ä¸­çš„ NaN
                    actual_values = df[field].fillna('').astype(str)
                    
                    # æ¯”å°
                    field_match = actual_values == expected_value
                    mask = mask & field_match
                    
                    if expected_value != '':  # åªè¨˜éŒ„éç©ºå€¼çš„æ¬„ä½
                        matched_fields.append(f"{field}={expected_value}")
            
            matched_count = mask.sum()
            
            if matched_count == 0:
                logger.info(f"âŒ æ‰¾ä¸åˆ°å®Œå…¨ç¬¦åˆçš„è³‡æ–™")
                logger.info(f"   æ¯”å°æ¢ä»¶: {', '.join(matched_fields[:5])}...")
                return jsonify({
                    'status': 'error',
                    'success': False,
                    'message': 'æ‰¾ä¸åˆ°å®Œå…¨ç¬¦åˆçš„è³‡æ–™'
                }), 404
            
            if matched_count > 1:
                logger.info(f"âš ï¸ è­¦å‘Šï¼šæ‰¾åˆ° {matched_count} ç­†å®Œå…¨ç›¸åŒçš„è³‡æ–™ï¼Œå°‡å…¨éƒ¨åˆªé™¤")
            
            # è¨˜éŒ„è¦åˆªé™¤çš„è³‡æ–™
            deleted_data = df[mask].to_dict('records')
            logger.info(f"ğŸ“ æº–å‚™åˆªé™¤ {matched_count} ç­†è³‡æ–™:")
            for idx, item in enumerate(deleted_data):
                logger.info(f"  è³‡æ–™ {idx+1}: Item={item.get('Item')}, å“é …={item.get('å“é …')}, ç¸½åƒ¹={item.get('ç¸½åƒ¹')}")
            
            # â­ åŸ·è¡Œåˆªé™¤
            df = df[~mask]
            df_after_count = len(df)
            
            # å„²å­˜
            df.to_csv(BUYER_FILE, index=False, encoding='utf-8-sig', na_rep='')
            
            deleted_count = df_before_count - df_after_count
            logger.info(f"âœ… åˆªé™¤æˆåŠŸ! å…±åˆªé™¤ {deleted_count} ç­†è³‡æ–™ (ç¸½ç­†æ•¸: {df_before_count} â†’ {df_after_count})")
            logger.info(f"ğŸ”“ é‡‹æ”¾æª”æ¡ˆé–")
        
        return jsonify({
            'status': 'success',
            'success': True,
            'message': 'åˆªé™¤æˆåŠŸ',
            'msg': f'æˆåŠŸåˆªé™¤ {deleted_count} ç­†è³‡æ–™',
            'deleted_count': int(deleted_count)  # â­ è½‰æ›æˆ int
        }), 200
        
    except Timeout:
        error_msg = 'æª”æ¡ˆæ­£åœ¨è¢«å…¶ä»–ç¨‹åºä½¿ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦'
        logger.info(f"â±ï¸ {error_msg}")
        return jsonify({'status': 'error', 'success': False, 'message': error_msg}), 503
        
    except Exception as e:
        error_msg = str(e)
        logger.info(f"âŒ åˆªé™¤å¤±æ•—: {error_msg}")
        traceback.print_exc()
        return jsonify({'status': 'error', 'success': False, 'message': error_msg}), 500
    

@app.route("/api/update_delivery_receipt", methods=["POST"])
def upload_buyer_detail():
    # å‚™ä»½
    # backup_files()
    file = request.files.get('file')
    if not file or not file.filename:
        print("âŒ æ²’æœ‰æ”¶åˆ°æª”æ¡ˆ")
        return jsonify({"status": "fail", "message": "No file uploaded"}), 400

    filename = file.filename
    ext = os.path.splitext(filename)[-1].lower()
    print(f"ğŸ“ æ”¶åˆ°æª”æ¡ˆï¼š{filename}ï¼ˆå‰¯æª”åï¼š{ext}ï¼‰")

    if ext not in ['.xlsx', '.xls']:
        return jsonify({"status": "fail", "message": "Invalid file type"}), 400

    try:
        engine = 'openpyxl' if ext == '.xlsx' else 'xlrd'
        df = pd.read_excel(file, engine=engine)  # é€™è¡Œæœ€å®¹æ˜“å ±éŒ¯
        df.to_csv("static/data/delivery_receipt.csv", index=False, encoding="utf-8-sig")
        print("ğŸ’¾ å·²å„²å­˜ç‚º static/data/delivery_receipt.csvï¼Œé–‹å§‹é€²è¡Œå° Buyer detail è©²è¡¨æ•¸æ“šæ›´æ–°")


        output_df = pd.read_csv("static/data/delivery_receipt.csv", encoding="utf-8-sig", dtype=str)
        buyer_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)

        # === æ¬„ä½æ¨™æº–åŒ– ===
        output_df.columns = output_df.columns.str.replace("\ufeff", "").str.strip()
        buyer_df.columns = buyer_df.columns.str.strip()


        # === ç¢ºèªä¸¦è™•ç†å¿…è¦çš„æ¬„ä½ ===
        required_buyer_cols = ["PO No.", "å“é …", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "ç™¼ç¥¨æœˆä»½"]
        required_output_cols = ["PONO", "å“å", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "æ”¶æ–™æ—¥æœŸ"]

        # æª¢æŸ¥ buyer_df æ˜¯å¦æœ‰å¿…è¦çš„æ¬„ä½
        if not all(col in buyer_df.columns for col in required_buyer_cols):
            missing_cols = [col for col in required_buyer_cols if col not in buyer_df.columns]
            print(f"éŒ¯èª¤: {BUYER_FILE} ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            logger.error(f"{BUYER_FILE} ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            exit()

        # æª¢æŸ¥ output_df æ˜¯å¦æœ‰å¿…è¦çš„æ¬„ä½
        if not all(col in output_df.columns for col in required_output_cols):
            missing_cols = [col for col in required_output_cols if col not in output_df.columns]
            print(f"éŒ¯èª¤: delivery_receipt.csv ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            logger.error(f"delivery_receipt.csv ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            exit()

        # === è³‡æ–™æ¸…ç†èˆ‡é è™•ç† ===
        # æ¸…ç† output_df çš„ PONO å’Œ å“å
        output_df["PONO_clean"] = output_df["PONO"].astype(str).str.strip().str.upper()
        output_df["å“å_clean"] = output_df["å“å"].astype(str).str.strip()

        # æ ¹æ“šä½ çš„è¦æ±‚ï¼Œç›´æ¥è¤‡è£½æ”¶æ–™æ—¥æœŸåˆ°æ–°çš„ç™¼ç¥¨æœˆä»½æ¬„ä½ï¼Œä¸é€²è¡Œä»»ä½•æ ¼å¼æ›´æ”¹ã€‚
        output_df["ç™¼ç¥¨æœˆä»½_from_output"] = output_df["æ”¶æ–™æ—¥æœŸ"].astype(str).str.strip()

        output_df.rename(columns={
            "é©—æ”¶æ•¸é‡": "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡": "æ‹’æ”¶æ•¸é‡_from_output"
        }, inplace=True)

        # æ¸…ç† buyer_df çš„ PO No. å’Œ å“é …
        buyer_df["PO_clean"] = buyer_df["PO No."].astype(str).str.strip().str.upper()
        buyer_df["å“é …_clean"] = buyer_df["å“é …"].astype(str).str.strip()

        # é¸æ“‡ output_df éœ€è¦çš„æ¬„ä½é€²è¡Œåˆä½µ
        output_cols_to_merge = [
            "PONO_clean",
            "å“å_clean",
            "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡_from_output",
            "ç™¼ç¥¨æœˆä»½_from_output"
        ]
        output_subset = output_df[output_cols_to_merge].copy()

        # === åˆä½µè³‡æ–™ ===
        merged = pd.merge(
            buyer_df,
            output_subset,
            how='left',
            left_on=["PO_clean", "å“é …_clean"],
            right_on=["PONO_clean", "å“å_clean"],
            suffixes=("", "_from_output")
        )

        # === æ›´æ–°æ¬„ä½ ===
        columns_to_update = {
            "é©—æ”¶æ•¸é‡": "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡": "æ‹’æ”¶æ•¸é‡_from_output",
            "ç™¼ç¥¨æœˆä»½": "ç™¼ç¥¨æœˆä»½_from_output"
        }

        update_count = 0
        for idx, row in merged.iterrows():
            updated = False
            log_info = {}
            
            po = row.get("PO No.", "N/A")
            item = row.get("Item", "N/A") # ä½¿ç”¨ 'Item' æ¬„ä½
            
            # é€™è£¡ç¢ºä¿ 'å“é …' æ¬„ä½æœ‰å€¼ï¼Œå¦å‰‡ä»¥ 'Item' æ›¿ä»£
            item_for_log = row.get("å“é …", "N/A")
            if item_for_log == 'N/A' or pd.isna(item_for_log):
                item_for_log = item

            for target_col, source_col in columns_to_update.items():
                new_val = row.get(source_col, "")
                old_val = row.get(target_col, "")

                if pd.isna(new_val): new_val = ""
                if pd.isna(old_val): old_val = ""

                clean_new_val = str(new_val).strip()
                clean_old_val = str(old_val).strip()

                if clean_new_val != "" and clean_new_val != clean_old_val:
                    merged.at[idx, target_col] = clean_new_val # type: ignore
                    updated = True
                    log_info[target_col] = {"old": old_val, "new": clean_new_val}

            if updated:
                update_count += 1
                # ä½¿ç”¨ä½ è¦æ±‚çš„æ—¥èªŒæ ¼å¼
                logger.info(
                    f"{{'{po}'}} æ›´æ”¹ -> item: {item_for_log}, "
                    f"é©—æ”¶æ•¸é‡: {log_info.get('é©—æ”¶æ•¸é‡', {}).get('new', row.get('é©—æ”¶æ•¸é‡', ''))}, "
                    f"æ‹’æ”¶æ•¸é‡: {log_info.get('æ‹’æ”¶æ•¸é‡', {}).get('new', row.get('æ‹’æ”¶æ•¸é‡', ''))}, "
                    f"ç™¼ç¥¨æœˆä»½: {log_info.get('ç™¼ç¥¨æœˆä»½', {}).get('new', row.get('ç™¼ç¥¨æœˆä»½', ''))}"
                )

        # === ç§»é™¤ä¸­ç¹¼æ¬„ä½ ===
        merged.drop(columns=[c for c in merged.columns if c.endswith("_clean") or c.endswith("_from_output")], inplace=True)

        # === æœ€çµ‚è¼¸å‡ºæ¬„ä½æ¸…å–® ===
        final_columns = ['Id', 'é–‹å–®ç‹€æ…‹', 'äº¤è²¨é©—è­‰', 'User', 'ePR No.', 'PO No.', 'Item', 'å“é …', 'è¦æ ¼', 'æ•¸é‡', 'ç¸½æ•¸', 'å–®åƒ¹', 'ç¸½åƒ¹', 'å‚™è¨»', 'å­—æ•¸', 'isEditing', 'backup', '_alertedItemLimit', 
                         'Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ', 'SOD Qty å» å•†æ‰¿è«¾æ•¸é‡', 'é©—æ”¶æ•¸é‡', 'æ‹’æ”¶æ•¸é‡', 'ç™¼ç¥¨æœˆä»½', "WBS", "éœ€æ±‚æ—¥", "RTé‡‘é¡", "RTç¸½é‡‘é¡", "é©—æ”¶ç‹€æ…‹"]

        # ç¢ºä¿åªä¿ç•™éœ€è¦çš„æ¬„ä½
        final_df = merged[final_columns].copy()

        # === å„²å­˜æ›´æ–°å¾Œçš„æª”æ¡ˆ ===
        final_df.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")

        print(f"ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚")
        print(f"æª”æ¡ˆå·²æ›´æ–°ã€‚ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚")
        os.remove("static/data/delivery_receipt.csv")

        return jsonify({"status": "success", "msg": f"æª”æ¡ˆå·²æ›´æ–°ã€‚ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚"})
    except Exception as e:
        traceback.print_exc()  # å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š
        return jsonify({"status": "error", "message": str(e)}), 500
    

@app.route("/api/buyer_detail", methods=["GET"])
def get_buyer_details():
    """
    è®€å– Buyer_detail.csv æª”æ¡ˆä¸¦ä»¥ JSON æ ¼å¼å›å‚³ã€‚
    """
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(BUYER_FILE):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {BUYER_FILE}")
        return jsonify({"error": f"æ‰¾ä¸åˆ°æª”æ¡ˆ {BUYER_FILE}"}), 500
        
    try:
        # ä½¿ç”¨ pandas è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)
        
        # ç¢ºä¿æ¬„ä½åç¨±æ²’æœ‰å¤šé¤˜çš„ç©ºç™½
        df.columns = df.columns.str.strip()

        # === ä¿®æ­£ï¼šå°‡ DataFrame ä¸­çš„ NaN å€¼æ›¿æ›ç‚º None ===
        # è®€å– CSV æ™‚ dtype=str æœƒå°‡ NaN è®€æˆå­—ä¸² 'nan'ã€‚
        df = df.replace({np.nan: None, 'nan': None})
        
        # å°‡ DataFrame è½‰æ›æˆ JSON æ ¼å¼çš„åˆ—è¡¨
        data_json = df.to_dict('records')
        
        return jsonify(data_json)
    except Exception as e:
        print(f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return jsonify({"error": f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}), 500



# ç‰©æ–™æ”¶è²¨å–®
from datetime import datetime
import hashlib
from bs4 import BeautifulSoup
import email
from email import policy
from email.parser import BytesParser
import logging
# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Buyer CSV è·¯å¾‘è¨­å®šï¼ˆèˆ‡ app.py åŒå±¤ç´šï¼‰
BUYER_CSV_PATH = 'static/data/Buyer_detail.csv'

# è¨­å®š
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB æª”æ¡ˆå¤§å°é™åˆ¶
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['PROCESSED_FOLDER'] = 'processed'
app.config['ALLOWED_EXTENSIONS'] = {'mhtml'}  

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['PROCESSED_FOLDER'], exist_ok=True)

def allowed_file(filename):
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºå…è¨±çš„æ ¼å¼"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def generate_unique_filename(original_name, new_name):
    """ç”Ÿæˆå”¯ä¸€çš„æª”å"""
    # å–å¾—å‰¯æª”å
    ext = original_name.rsplit('.', 1)[1].lower()
    
    # å¦‚æœæ–°æª”åæ²’æœ‰å‰¯æª”åï¼ŒåŠ ä¸ŠåŸå§‹å‰¯æª”å
    if not new_name.endswith(f'.{ext}'):
        new_name = f"{new_name}.{ext}"
    
    # ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    unique_id = hashlib.md5(f"{new_name}{timestamp}".encode()).hexdigest()[:8]
    
    # çµ„åˆæœ€çµ‚æª”å
    name_without_ext = new_name.rsplit('.', 1)[0]
    final_name = f"{name_without_ext}_{unique_id}.{ext}"
    
    return secure_filename(final_name)

class MHTMLParser:
    """MHTML æª”æ¡ˆè§£æå™¨"""
    
    def __init__(self, file_path):
        self.file_path = file_path
        self.boundary = None
        self.parts = []
        self.html_content = None
        self.metadata = {}
        self.gridview_data = None
        
    def parse(self):
        """è§£æ MHTML æª”æ¡ˆ"""
        try:
            with open(self.file_path, 'rb') as f:
                # ä½¿ç”¨ email æ¨¡çµ„è§£æ MHTML
                try:
                    msg = BytesParser(policy=policy.default).parse(f)
                except Exception as e:
                    logger.error(f"Email parser error: {str(e)}")
                    # å¦‚æœ email è§£æå¤±æ•—ï¼Œå˜—è©¦ç›´æ¥è®€å–ç‚º HTML
                    f.seek(0)
                    content = f.read()
                    self.html_content = content.decode('utf-8', errors='ignore')
                    
                    if self.html_content:
                        result = self._extract_html_info()
                        gridview_data = self._parse_gridview()
                        if gridview_data:
                            result['gridview_data'] = gridview_data # type: ignore
                        return result
                    return {'error': f'ç„¡æ³•è§£ææª”æ¡ˆ: {str(e)}'}
                
                # æå–åŸºæœ¬è³‡è¨Š
                self.metadata['subject'] = msg.get('Subject', '')
                self.metadata['date'] = msg.get('Date', '')
                self.metadata['from'] = msg.get('From', '')
                self.metadata['content_type'] = msg.get_content_type()
                
                # è™•ç†å¤šéƒ¨åˆ†å…§å®¹
                if msg.is_multipart():
                    self._parse_multipart(msg)
                else:
                    # å–®ä¸€éƒ¨åˆ† MHTML
                    content = msg.get_payload(decode=True)
                    if content:
                        self.html_content = content.decode('utf-8', errors='ignore') # type: ignore
                
                # è§£æ HTML å…§å®¹
                if self.html_content:
                    result = self._extract_html_info()
                    
                    # ç‰¹åˆ¥è™•ç† GridView è¡¨æ ¼
                    gridview_data = self._parse_gridview()
                    if gridview_data:
                        result['gridview_data'] = gridview_data # type: ignore
                    
                    return result
                    
            return self.metadata
            
        except Exception as e:
            logger.error(f"è§£æéŒ¯èª¤: {str(e)}")
            logger.error(traceback.format_exc())
            return {'error': str(e)}
    
    def _parse_multipart(self, msg):
        """è§£æå¤šéƒ¨åˆ† MHTML"""
        for part in msg.walk():
            content_type = part.get_content_type()
            
            # å°‹æ‰¾ HTML å…§å®¹
            if content_type == 'text/html':
                content = part.get_payload(decode=True)
                if content:
                    self.html_content = content.decode('utf-8', errors='ignore')
                    
            # æ”¶é›†å…¶ä»–è³‡æº
            elif not part.is_multipart():
                self.parts.append({
                    'content_type': content_type,
                    'content_location': part.get('Content-Location', ''),
                    'content_id': part.get('Content-ID', ''),
                    'size': len(part.get_payload())
                })
    
    def _extract_html_info(self):
        """å¾ HTML å…§å®¹æå–è³‡è¨Š"""
        if not self.html_content:
            return None  # æˆ– raise ValueError("html_content ç‚ºç©º")
        
        soup = BeautifulSoup(self.html_content, 'html.parser')
        
        # æå–æ¨™é¡Œ
        title_tag = soup.find('title')
        self.metadata['title'] = title_tag.string if title_tag else ''  # type: ignore
        
        # æå– meta æ¨™ç±¤
        meta_tags = {}
        for meta in soup.find_all('meta'):
            name = meta.get('name') or meta.get('property')
            content = meta.get('content')
            if name and content:
                meta_tags[name] = content
            
            # ç‰¹åˆ¥è™•ç† charset
            if meta.get('charset'):
                self.metadata['encoding'] = meta.get('charset')
            elif meta.get('http-equiv', '').lower() == 'content-type':
                content = meta.get('content', '')
                if 'charset=' in content:
                    self.metadata['encoding'] = content.split('charset=')[-1].strip()
        
        self.metadata['meta_tags'] = meta_tags
        
        # çµ±è¨ˆè³‡è¨Š
        self.metadata['statistics'] = {
            'total_parts': len(self.parts),
            'html_size': len(self.html_content) if self.html_content else 0
        }
        
        # æå–æ–‡å­—å…§å®¹æ‘˜è¦ï¼ˆå‰500å­—ï¼‰
        text_content = soup.get_text(strip=True)
        self.metadata['content_preview'] = text_content[:500] if text_content else ''
        
        return self.metadata
    
    def _parse_gridview(self):
        """è§£æ GridView è¡¨æ ¼è³‡æ–™"""
        if not self.html_content:
            return None
        
        try:
            soup = BeautifulSoup(self.html_content, 'html.parser')
            
            # å°‹æ‰¾ GridView Wrapper
            wrapper = soup.find('div', {'id': 'ContentPlaceHolder1_GridView2Wrapper'})
            if not wrapper:
                logger.debug("No GridView wrapper found")
                return None
            
            gridview_data = {
                'headers': [],
                'rows': [],
                'statistics': {}
            }
            
            # è§£æè¡¨é ­
            header_table = wrapper.find('table', {'id': 'ContentPlaceHolder1_GridView2Copy'}) # type: ignore
            if header_table:
                header_row = header_table.find('tr', {'id': lambda x: x and 'HeaderCopy' in x}) # type: ignore
                if header_row:
                    for th in header_row.find_all('th'): # type: ignore
                        # æ¸…ç†è¡¨é ­æ–‡å­—
                        header_text = th.get_text(strip=True)
                        header_text = header_text.replace('\n', ' ').replace('\r', '')
                        # åˆ†é›¢ä¸­è‹±æ–‡
                        if 'br' in str(th):
                            parts = [t.strip() for t in th.strings]
                            header_dict = {
                                'zh': parts[0] if len(parts) > 0 else '',
                                'en': parts[1] if len(parts) > 1 else '',
                                'full': header_text
                            }
                        else:
                            header_dict = {
                                'zh': header_text,
                                'en': '',
                                'full': header_text
                            }
                        gridview_data['headers'].append(header_dict)
            
            # è§£æè³‡æ–™è¡Œ
            data_table = wrapper.find('table', {'id': 'ContentPlaceHolder1_GridView2'}) # type: ignore
            if data_table:
                # æ‰¾æ‰€æœ‰çš„ trï¼Œæ’é™¤è¡¨é ­
                all_rows = data_table.find_all('tr') # type: ignore
                data_rows = []
                
                for row in all_rows:
                    # è·³éè¡¨é ­è¡Œï¼ˆé€šå¸¸æœ‰ç‰¹å®šçš„ style æˆ– idï¼‰
                    if row.get('id') and 'Header' in row.get('id', ''):
                        continue
                    if row.get('style') and 'display: none' in row.get('style', ''):
                        continue
                    # ç¢ºä¿æœ‰ td å…ƒç´ 
                    if row.find_all('td'):
                        data_rows.append(row)
                
                for row in data_rows:
                    row_data = []
                    cells = row.find_all('td')
                    
                    for idx, td in enumerate(cells):
                        cell_data = {
                            'value': '',
                            'inputs': [],
                            'spans': []
                        }
                        
                        # æå– input å…ƒç´ 
                        inputs = td.find_all('input')
                        for inp in inputs:
                            input_data = {
                                'type': inp.get('type', ''),
                                'name': inp.get('name', ''),
                                'id': inp.get('id', ''),
                                'title': inp.get('title', ''),
                                'src': inp.get('src', '')
                            }
                            # æ¨™è¨˜æŒ‰éˆ•é¡å‹ï¼Œä¸ä¾è³´å¤–éƒ¨åœ–ç‰‡
                            if input_data['src']:
                                if 'edit' in input_data['src'].lower() or 'ç·¨è¼¯' in input_data.get('title', ''):
                                    input_data['button_type'] = 'edit'
                                elif 'delete' in input_data['src'].lower() or 'åˆªé™¤' in input_data.get('title', ''):
                                    input_data['button_type'] = 'delete'
                                else:
                                    input_data['button_type'] = 'unknown'
                            
                            cell_data['inputs'].append(input_data)
                        
                        # æå– span å…ƒç´ 
                        spans = td.find_all('span')
                        for span in spans:
                            span_text = span.get_text(strip=True)
                            cell_data['spans'].append({
                                'id': span.get('id', ''),
                                'class': span.get('class', []),
                                'text': span_text
                            })
                            if not cell_data['value']:  # ä½¿ç”¨ç¬¬ä¸€å€‹ span çš„æ–‡å­—ä½œç‚ºå€¼
                                cell_data['value'] = span_text
                        
                        # å¦‚æœæ²’æœ‰ spanï¼Œå–æ•´å€‹ td çš„æ–‡å­—
                        if not cell_data['value']:
                            cell_data['value'] = td.get_text(strip=True)
                        
                        row_data.append(cell_data)
                    
                    # å°‡è³‡æ–™è¡ŒåŠ å…¥çµæœ
                    if row_data:
                        # å»ºç«‹çµæ§‹åŒ–çš„è³‡æ–™ç‰©ä»¶
                        structured_row = {}
                        for i, header in enumerate(gridview_data['headers']):
                            if i < len(row_data):
                                # å»ºç«‹æ›´å®‰å…¨çš„æ¬„ä½åç¨±
                                if header.get('en'):
                                    field_name = re.sub(r'[^a-zA-Z0-9_]', '_', header['en'].lower())
                                else:
                                    field_name = f'field_{i}'
                                structured_row[field_name] = row_data[i]['value']
                        
                        # åŠ å…¥åŸå§‹è³‡æ–™å’Œçµæ§‹åŒ–è³‡æ–™
                        gridview_data['rows'].append({
                            'raw_data': row_data,
                            'structured_data': structured_row
                        })
            
            # çµ±è¨ˆè³‡è¨Š
            gridview_data['statistics'] = {
                'total_columns': len(gridview_data['headers']),
                'total_rows': len(gridview_data['rows']),
                'has_data': len(gridview_data['rows']) > 0
            }
            
            return gridview_data
            
        except Exception as e:
            logger.error(f"GridView parsing error: {str(e)}")
            logger.error(traceback.format_exc())
            return None

@app.route('/api/upload-mhtml', methods=['POST'])
def upload_mhtml():
    """è™•ç† MHTML æª”æ¡ˆä¸Šå‚³ä¸¦è‡ªå‹•èˆ‡ Buyer CSV æ¯”å°"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆ
        if 'file' not in request.files:
            logger.error("No file in request")
            return jsonify({'success': False, 'error': 'æ²’æœ‰æª”æ¡ˆè¢«ä¸Šå‚³'}), 400
        
        file = request.files['file']
        new_name = request.form.get('newName', '')
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
        if file.filename == '':
            logger.error("Empty filename")
            return jsonify({'success': False, 'error': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400
        
        # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
        if not allowed_file(file.filename):
            logger.error(f"Invalid file type: {file.filename}")
            return jsonify({'success': False, 'error': 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼'}), 400
        
        # ç”Ÿæˆæ–°çš„å”¯ä¸€æª”å
        if new_name:
            filename = generate_unique_filename(file.filename, new_name)
        else:
            filename = generate_unique_filename(file.filename, file.filename)
        
        logger.info(f"Processing file: {filename}")
        
        # å„²å­˜æª”æ¡ˆåˆ°ä¸Šå‚³è³‡æ–™å¤¾
        upload_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(upload_path)
        
        # ç§»å‹•ä¸¦é‡æ–°å‘½åæª”æ¡ˆåˆ°è™•ç†è³‡æ–™å¤¾
        processed_path = os.path.join(app.config['PROCESSED_FOLDER'], filename)
        
        # å¦‚æœç›®æ¨™æª”æ¡ˆå·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤
        if os.path.exists(processed_path):
            os.remove(processed_path)
        
        os.rename(upload_path, processed_path)
        
        # è§£æ MHTML æª”æ¡ˆ
        parser = MHTMLParser(processed_path)
        extracted_info = parser.parse()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è§£æéŒ¯èª¤
        if 'error' in extracted_info: # type: ignore
            logger.error(f"Parser error: {extracted_info['error']}") # type: ignore
            # å³ä½¿æœ‰éŒ¯èª¤ï¼Œé‚„æ˜¯è¿”å›éƒ¨åˆ†è³‡è¨Š
        
        # åŠ å…¥æª”æ¡ˆè³‡è¨Š
        file_stats = os.stat(processed_path)
        extracted_info.update({ # type: ignore
            'original_filename': file.filename,
            'saved_filename': filename,
            'file_size': file_stats.st_size,
            'upload_time': datetime.now().isoformat(),
            'file_path': processed_path
        })
        
        # æ ¼å¼åŒ–æª”æ¡ˆå¤§å°
        size = file_stats.st_size
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                extracted_info['file_size_formatted'] = f"{size:.2f} {unit}" # type: ignore
                break
            size /= 1024.0
        
        # å¦‚æœæœ‰ GridView è³‡æ–™ï¼Œå¦å¤–å„²å­˜ç‚º JSON
        if 'gridview_data' in extracted_info and extracted_info['gridview_data']: # type: ignore
            json_filename = filename.rsplit('.', 1)[0] + '_gridview.json'
            json_path = os.path.join(app.config['PROCESSED_FOLDER'], json_filename)
            
            try:
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(extracted_info['gridview_data'], f, ensure_ascii=False, indent=2) # type: ignore
                
                extracted_info['gridview_json_file'] = json_filename # type: ignore
                logger.info(f"GridView data saved to: {json_filename}")
                
                # è‡ªå‹•èˆ‡ Buyer_detail.csv æ¯”å°
                if os.path.exists(BUYER_CSV_PATH):
                    logger.info("é–‹å§‹èˆ‡ Buyer_detail.csv æ¯”å°...")
                    comparison_result = compare_with_buyer_csv(extracted_info['gridview_data']) # type: ignore
                    extracted_info['comparison_result'] = comparison_result # type: ignore
                else:
                    logger.warning(f"æ‰¾ä¸åˆ° {BUYER_CSV_PATH}")
                    extracted_info['comparison_result'] = { # type: ignore
                        'error': f'æ‰¾ä¸åˆ° {BUYER_CSV_PATH} æª”æ¡ˆ',
                        'needs_buyer_csv': True
                    }
                    
            except Exception as e:
                logger.error(f"Failed to save GridView JSON or compare: {str(e)}")
        
        return jsonify({
            'success': True,
            'message': 'æª”æ¡ˆä¸Šå‚³ä¸¦è§£ææˆåŠŸ',
            'data': extracted_info
        })
        
    except Exception as e:
        logger.error(f"ä¸Šå‚³éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
        }), 500

def compare_with_buyer_csv(gridview_data):
    """èˆ‡ Buyer CSV æ¯”å° PO No. å’Œå“å"""
    try:
        # è®€å– Buyer CSV
        try:
            buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig')
        except:
            try:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8')
            except:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='big5')
        
        print(f"Buyer CSV columns: {list(buyer_df.columns)}")
        print(f"Buyer CSV shape: {buyer_df.shape}")
        # æ¸…ç† PO No. æ¬„ä½ï¼Œç§»é™¤ .0 å¾Œç¶´
        buyer_df['PO No.'] = buyer_df['PO No.'].apply(lambda x: str(x).replace('.0', '') if pd.notna(x) and str(x).endswith('.0') else str(x) if pd.notna(x) else '')

        # é¡¯ç¤ºä¿®æ­£å¾Œçš„çµæœ
        valid_pos = [po for po in buyer_df['PO No.'].tolist() if po != 'nan' and po != '']
        print(f"ä¿®æ­£å¾Œæœ‰æ•ˆ PO æ•¸é‡: {len(valid_pos)}")
        print(f"ä¿®æ­£å¾Œå‰10å€‹ PO: {valid_pos[:10]}")
        
        # è§£æ GridView è³‡æ–™
        headers = gridview_data.get('headers', [])
        rows = gridview_data.get('rows', [])
        
        print(f"GridView rows count: {len(rows)}")
        
        # æ‰¾å‡ºé—œéµæ¬„ä½çš„ç´¢å¼•
        po_index = -1
        desc_index = -1
        qty_index = -1
        accept_qty_index = -1
        amount_index = -1
        
        for i, header in enumerate(headers):
            header_text = header.get('en', '').lower() or header.get('zh', '').lower() or header.get('full', '').lower()
            # PO No. æ¬„ä½ï¼ˆé¿å…åŒ¹é…åˆ° Demo POè™Ÿï¼‰
            if ('po' in header_text and 'no' in header_text) or 'poè™Ÿç¢¼' in header_text:
                if 'demo' not in header_text.lower():  # æ’é™¤ Demo POè™Ÿ
                    po_index = i
                    print(f"Found PO index at {i}: {header}")
            elif 'description' in header_text or 'å“å' in header_text:
                desc_index = i
                print(f"Found Description index at {i}: {header}")
            elif 'accept' in header_text and 'qty' in header_text or 'é©—æ”¶æ•¸é‡' in header_text:
                accept_qty_index = i
            elif 'qty' in header_text or 'æ•¸é‡' in header_text or 'æ”¶è²¨æ•¸é‡' in header_text:
                if accept_qty_index == -1:
                    qty_index = i
            elif 'amount' in header_text or 'ç¸½é‡‘é¡' in header_text:
                amount_index = i
        
        # å…ˆæå–æ‰€æœ‰ PO è™Ÿç¢¼ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å¤šå€‹ä¸åŒçš„ PO
        po_numbers = []
        for row_idx, row in enumerate(rows):
            raw_data = row.get('raw_data', [])
            
            if po_index >= 0 and po_index < len(raw_data):
                po_no = str(raw_data[po_index].get('value', '')).strip()
                print(f"Row {row_idx} - Extracted PO: '{po_no}'")
                if po_no:  # åªè¨˜éŒ„éç©ºçš„ PO è™Ÿç¢¼
                    po_numbers.append(po_no)
            else:
                logger.warning(f"Row {row_idx} - PO index {po_index} out of range")
        
        # æª¢æŸ¥ PO è™Ÿç¢¼çš„å”¯ä¸€æ€§
        unique_pos = list(set(po_numbers))
        print(f"ç™¼ç¾çš„å”¯ä¸€ PO è™Ÿç¢¼: {unique_pos}")
        
        # æ·»åŠ å³æ™‚æª¢æŸ¥ï¼šæª¢æŸ¥é€™äº› PO æ˜¯å¦åœ¨ Buyer CSV ä¸­å­˜åœ¨
        print("=" * 60)
        print("ğŸ” å³æ™‚æª¢æŸ¥ PO æ˜¯å¦å­˜åœ¨æ–¼ Buyer CSV ä¸­...")
        print("=" * 60)
        
        # ç²å– Buyer CSV ä¸­æ‰€æœ‰çš„ PO è™Ÿç¢¼
        buyer_pos = buyer_df['PO No.'].astype(str).str.strip().tolist()
        print(f"ğŸ“Š Buyer CSV ä¸­ç¸½å…±æœ‰ {len(buyer_pos)} ç­† PO è³‡æ–™")
        print(f"ğŸ“Š Buyer CSV ä¸­å‰10å€‹ PO è™Ÿç¢¼ç¯„ä¾‹: {buyer_pos[:10]}")
        
        # æª¢æŸ¥æ¯å€‹æå–çš„ PO
        found_pos = []
        missing_pos = []
        
        for po_no in unique_pos:
            if po_no in buyer_pos:
                found_pos.append(po_no)
                print(f"âœ… PO {po_no} åœ¨ Buyer CSV ä¸­æ‰¾åˆ°")
                
                # é¡¯ç¤ºåŒ¹é…çš„è©³ç´°è³‡è¨Š
                matching_rows = buyer_df[buyer_df['PO No.'].astype(str).str.strip() == po_no]
                print(f"   â””â”€ å…±æ‰¾åˆ° {len(matching_rows)} ç­†åŒ¹é…è¨˜éŒ„")
                for idx, row in matching_rows.iterrows():
                    print(f"   â””â”€ ç¬¬ {idx} è¡Œ: å“é …='{row.get('Item', '')}' æˆ– '{row.get('å“é …', '')}'")
            else:
                missing_pos.append(po_no)
                print(f"âŒ PO {po_no} åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ°")
        
        print("=" * 60)
        print(f"ğŸ“ˆ æª¢æŸ¥çµæœçµ±è¨ˆ:")
        print(f"   âœ… æ‰¾åˆ°çš„ PO: {len(found_pos)} å€‹ â†’ {found_pos}")
        print(f"   âŒ ç¼ºå¤±çš„ PO: {len(missing_pos)} å€‹ â†’ {missing_pos}")
        print("=" * 60)
        
        # å¦‚æœæœ‰ç¼ºå¤±çš„ POï¼Œé€²ä¸€æ­¥æª¢æŸ¥å¯èƒ½çš„å•é¡Œ
        if missing_pos:
            print("ğŸ”§ é€²ä¸€æ­¥è¨ºæ–·ç¼ºå¤±çš„ PO:")
            for missing_po in missing_pos:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„ POï¼ˆå¯èƒ½æ˜¯æ ¼å¼å•é¡Œï¼‰
                similar_pos = [po for po in buyer_pos if missing_po in po or po in missing_po]
                if similar_pos:
                    print(f"   âš ï¸  PO {missing_po} æ‰¾ä¸åˆ°ï¼Œä½†ç™¼ç¾ç›¸ä¼¼çš„: {similar_pos[:5]}")
                else:
                    print(f"   âŒ PO {missing_po} å®Œå…¨æ‰¾ä¸åˆ°ç›¸ä¼¼çš„")
            print("=" * 60)

        # å¦‚æœåªæœ‰ä¸€å€‹ PO æˆ–æœ‰æ˜é¡¯å¤šæ•¸ï¼Œç¹¼çºŒåŸä¾†çš„æ¯”å°é‚è¼¯
        # è®€å– Buyer CSV
        # å»ºç«‹æ¯”å°çµæœ
        comparison_items = []
        total_amount = 0
        
        # åˆ—å‡ºç¬¬ä¸€ç­†è³‡æ–™çš„åŸå§‹å…§å®¹ä»¥é™¤éŒ¯
        if len(rows) > 0:
            first_row = rows[0].get('raw_data', [])
            print(f"First row raw data length: {len(first_row)}")
            for i, cell in enumerate(first_row[:10]):  # åªåˆ—å‰10å€‹æ¬„ä½
                print(f"Cell {i}: {cell.get('value', 'NO VALUE')}")
        
        for row_idx, row in enumerate(rows):
            item = {}
            raw_data = row.get('raw_data', [])
            
            # æå–è³‡æ–™
            if po_index >= 0 and po_index < len(raw_data):
                item['po_no'] = str(raw_data[po_index].get('value', '')).strip()
                print(f"Row {row_idx} - Extracted PO from index {po_index}: '{item['po_no']}'")
            else:
                logger.warning(f"Row {row_idx} - PO index {po_index} out of range (row has {len(raw_data)} cells)")
                
            if desc_index >= 0 and desc_index < len(raw_data):
                item['description'] = str(raw_data[desc_index].get('value', '')).strip()
                print(f"Row {row_idx} - Extracted Description from index {desc_index}: '{item['description']}'")
                
            if qty_index >= 0 and qty_index < len(raw_data):
                item['qty'] = raw_data[qty_index].get('value', '')
                
            if accept_qty_index >= 0 and accept_qty_index < len(raw_data):
                item['accept_qty'] = raw_data[accept_qty_index].get('value', '')
            elif qty_index >= 0:
                item['accept_qty'] = item.get('qty', '0')
                
            if amount_index >= 0 and amount_index < len(raw_data):
                item['amount'] = raw_data[amount_index].get('value', '')
            
            # è¨ˆç®— RT é‡‘é¡å’Œ RT ç¸½é‡‘é¡
            try:
                amount_str = str(item.get('amount', '0'))
                amount_str = amount_str.replace(',', '').replace('$', '').replace('TWD', '').strip()
                amount_value = float(amount_str) if amount_str and amount_str != '-' else 0
                
                accept_qty_str = str(item.get('accept_qty', '0'))
                accept_qty_str = accept_qty_str.replace(',', '').strip()
                accept_qty_value = float(accept_qty_str) if accept_qty_str and accept_qty_str != '0' and accept_qty_str != '-' else 1
                
                item['rt_amount'] = amount_value / accept_qty_value if accept_qty_value > 0 else 0
                item['rt_total_amount'] = amount_value
                
                total_amount += amount_value
            except Exception as e:
                logger.error(f"è¨ˆç®— RT é‡‘é¡éŒ¯èª¤: {str(e)}")
                item['rt_amount'] = 0
                item['rt_total_amount'] = 0
            
            # åœ¨ Buyer CSV ä¸­å°‹æ‰¾åŒ¹é…
            matched_in_buyer = False
            buyer_row_index = -1
            
            if item.get('po_no'):
                # åœ¨ Buyer CSV çš„ PO No. æ¬„ä½ä¸­å°‹æ‰¾
                for idx, buyer_row in buyer_df.iterrows():
                    buyer_po = str(buyer_row.get('PO No.', '')).strip()
                    
                    if buyer_po == item['po_no']:
                        matched_in_buyer = True
                        buyer_row_index = idx
                        print(f"âœ“ Matched PO {item['po_no']} at Buyer CSV row {idx}")
                        logger.debug(f"æ‰€æœ‰æ¬„ä½: {buyer_row.to_dict()}")
                        # æª¢æŸ¥å“åæ˜¯å¦ä¹ŸåŒ¹é…
                        col_item = next((col for col in buyer_row.index if col.strip().replace(' ', '') in ['Item']), 'Item')
                        col_desc = next((col for col in buyer_row.index if col.strip().replace(' ', '') in ['å“é …', 'å“å']), 'å“é …')

                        buyer_item = str(buyer_row.get(col_item, '')).strip()
                        buyer_desc = str(buyer_row.get(col_desc, '')).strip()
                        
                        if item.get('description'):
                            if buyer_item != item['description'] and buyer_desc != item['description']:
                                logger.warning(f"PO matched but description different: Buyer='{buyer_item}' or '{buyer_desc}', GridView='{item['description']}'")
                        break
                
                if not matched_in_buyer:
                    logger.warning(f"âœ— PO {item['po_no']} not found in Buyer CSV")
            else:
                logger.warning(f"Row {row_idx} has no PO number")
            
            item['matched_in_buyer'] = matched_in_buyer
            item['buyer_row_index'] = buyer_row_index
            comparison_items.append(item)
        
        # çµ±è¨ˆçµæœ
        matched_count = sum(1 for item in comparison_items if item['matched_in_buyer'])
        unmatched_count = len(comparison_items) - matched_count
        
        print(f"æ¯”å°çµæœ: ç¸½é …ç›®={len(comparison_items)}, åŒ¹é…={matched_count}, æœªåŒ¹é…={unmatched_count}")
        
        return {
            'items': comparison_items,
            'summary': {
                'total_items': len(comparison_items),
                'matched_count': matched_count,
                'unmatched_count': unmatched_count,
                'total_amount': total_amount,
                'buyer_csv_rows': len(buyer_df),
                'buyer_csv_columns': list(buyer_df.columns)
            }
        }
        
    except Exception as e:
        logger.error(f"æ¯”å°éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'error': str(e),
            'items': [],
            'summary': {}
        }
    

    
# åœ¨ app.py ä¸­æ–°å¢ä»¥ä¸‹è·¯ç”±å’ŒåŠŸèƒ½

import shutil

@app.route('/api/cleanup-processed', methods=['POST'])
def cleanup_processed():
    """æ¸…ç† processed è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    try:
        processed_folder = app.config['PROCESSED_FOLDER']
        upload_folder = app.config['UPLOAD_FOLDER']
        
        total_files_removed = 0
        
        # æ¸…ç† processed è³‡æ–™å¤¾
        if os.path.exists(processed_folder):
            files_before = os.listdir(processed_folder)
            if files_before:
                for filename in files_before:
                    file_path = os.path.join(processed_folder, filename)
                    try:
                        if os.path.isfile(file_path) or os.path.islink(file_path):
                            os.unlink(file_path)
                            total_files_removed += 1
                            print(f"å·²åˆªé™¤ processed æ–‡ä»¶: {filename}")
                        elif os.path.isdir(file_path):
                            shutil.rmtree(file_path)
                            total_files_removed += 1
                            print(f"å·²åˆªé™¤ processed ç›®éŒ„: {filename}")
                    except Exception as e:
                        logger.error(f"ç„¡æ³•åˆªé™¤ processed æ–‡ä»¶ {file_path}: {str(e)}")
        
        # åŒæ™‚æ¸…ç† upload è³‡æ–™å¤¾ï¼ˆé é˜²æªæ–½ï¼‰
        if os.path.exists(upload_folder):
            upload_files = os.listdir(upload_folder)
            if upload_files:
                for filename in upload_files:
                    file_path = os.path.join(upload_folder, filename)
                    try:
                        if os.path.isfile(file_path) or os.path.islink(file_path):
                            os.unlink(file_path)
                            total_files_removed += 1
                            print(f"å·²åˆªé™¤ upload æ–‡ä»¶: {filename}")
                        elif os.path.isdir(file_path):
                            shutil.rmtree(file_path)
                            total_files_removed += 1
                            print(f"å·²åˆªé™¤ upload ç›®éŒ„: {filename}")
                    except Exception as e:
                        logger.error(f"ç„¡æ³•åˆªé™¤ upload æ–‡ä»¶ {file_path}: {str(e)}")
        
        # é©—è­‰æ¸…ç†çµæœ
        processed_files_after = os.listdir(processed_folder) if os.path.exists(processed_folder) else []
        upload_files_after = os.listdir(upload_folder) if os.path.exists(upload_folder) else []
        
        print(f"æ¸…ç†å®Œæˆ: åˆªé™¤äº† {total_files_removed} å€‹æ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸæ¸…ç† {total_files_removed} å€‹æ–‡ä»¶',
            'files_removed': total_files_removed,
            'remaining_processed_files': len(processed_files_after),
            'remaining_upload_files': len(upload_files_after)
        })
        
    except Exception as e:
        logger.error(f"æ¸…ç†è³‡æ–™å¤¾éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'æ¸…ç†å¤±æ•—: {str(e)}'
        }), 500

# ä¿®æ”¹ç¾æœ‰çš„ update_buyer_csv å‡½æ•¸ï¼Œåœ¨æˆåŠŸæ›´æ–°å¾Œè‡ªå‹•è§¸ç™¼æ¸…ç†
@app.route('/api/update-buyer-csv', methods=['POST'])
def update_buyer_csv():
    # å‚™ä»½
    # backup_files()
    """æ›´æ–° Buyer CSV æª”æ¡ˆä¸­çš„ RT é‡‘é¡"""
    try:
        data = request.json
        items_to_update = data.get('items', []) # type: ignore
        
        if not items_to_update:
            return jsonify({'success': False, 'error': 'æ²’æœ‰è¦æ›´æ–°çš„é …ç›®'}), 400
        
        print(f"æº–å‚™æ›´æ–° {len(items_to_update)} ç­†è³‡æ–™")
        
        # è®€å– Buyer CSV
        try:
            buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype=str)
        except:
            try:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8', dtype=str)
            except:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='big5', dtype=str)
        
        print(f"Buyer CSV åŸå§‹æ¬„ä½: {list(buyer_df.columns)}")
        
        # ç¢ºä¿æœ‰ RT ç›¸é—œæ¬„ä½
        if 'RTé‡‘é¡' not in buyer_df.columns:
            buyer_df['RTé‡‘é¡'] = ''
            print("æ–°å¢ RTé‡‘é¡ æ¬„ä½")
        if 'RTç¸½é‡‘é¡' not in buyer_df.columns:
            buyer_df['RTç¸½é‡‘é¡'] = ''
            print("æ–°å¢ RTç¸½é‡‘é¡ æ¬„ä½")
        
        updated_count = 0
        
        # æ›´æ–°æ¯å€‹é …ç›®
        for item in items_to_update:
            print(items_to_update)
            po_no = str(item.get('po_no', '')).strip()
            description = str(item.get('description', '')).strip()
            
            # å°‡é‡‘é¡è½‰æ›ç‚ºæ•´æ•¸å­—ä¸²ï¼ˆç§»é™¤å°æ•¸é»ï¼‰
            rt_amount = item.get('rt_amount', 0)
            rt_total_amount = item.get('rt_total_amount', 0)
            
            try:
                rt_amount_str = str(int(round(float(rt_amount))))
                rt_total_amount_str = str(int(round(float(rt_total_amount))))
            except:
                rt_amount_str = '0'
                rt_total_amount_str = '0'
            
            print(f"å˜—è©¦æ›´æ–°: PO={po_no}, å“å={description}, RTé‡‘é¡={rt_amount_str}, RTç¸½é‡‘é¡={rt_total_amount_str}")
            
            if po_no:
                mask_po = buyer_df['PO No.'].astype(str).str.strip() == po_no

                if 'å“é …' in buyer_df.columns:
                    mask_desc = buyer_df['å“é …'].astype(str).str.strip() == description
                    mask_both = mask_po & mask_desc
                else:
                    mask_desc = pd.Series([False] * len(buyer_df))
                    mask_both = mask_po  # fallback

                if mask_both.sum() > 0:
                    buyer_df.loc[mask_both, 'RTé‡‘é¡'] = rt_amount_str
                    buyer_df.loc[mask_both, 'RTç¸½é‡‘é¡'] = rt_total_amount_str
                    updated_count += mask_both.sum()
                    print(f"âœ“ å®Œæ•´åŒ¹é… PO={po_no}, å“é …={description} â†’ æˆåŠŸæ›´æ–°")
                elif mask_po.sum() > 0:
                    buyer_df.loc[mask_po, 'RTé‡‘é¡'] = rt_amount_str
                    buyer_df.loc[mask_po, 'RTç¸½é‡‘é¡'] = rt_total_amount_str
                    updated_count += mask_po.sum()
                    logger.warning(f"âš ï¸ PO={po_no} åœ¨ Buyer CSV ä¸­æœ‰ {mask_po.sum()} ç­†ï¼Œä½†å“é …ä¸åŒ (GridView: {description}, Buyer: {buyer_df.loc[mask_po, 'å“é …'].unique().tolist()})ï¼Œä»å¼·åˆ¶æ›´æ–° RT é‡‘é¡")
                else:
                    logger.warning(f"âœ— åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ° PO {po_no}")
        
        # å„²å­˜æ›´æ–°å¾Œçš„ CSV
        if updated_count > 0:
            try:
                buyer_df.to_csv(BUYER_CSV_PATH, index=False, encoding='utf-8-sig', na_rep='')
                print(f"æˆåŠŸå„²å­˜æ›´æ–°å¾Œçš„ Buyer CSVï¼Œå…±æ›´æ–° {updated_count} ç­†è³‡æ–™")
                
                # é©—è­‰æ›´æ–°æ˜¯å¦æˆåŠŸ
                verify_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype=str)
                print(f"é©—è­‰: æ›´æ–°å¾Œçš„ CSV æœ‰ {len(verify_df)} ç­†è³‡æ–™")
                
                # æª¢æŸ¥ç‰¹å®š PO çš„æ›´æ–°çµæœ
                for item in items_to_update[:3]:
                    po_no = str(item.get('po_no', '')).strip()
                    if po_no:
                        verify_rows = verify_df[verify_df['PO No.'].astype(str).str.strip() == po_no]
                        if len(verify_rows) > 0:
                            print(f"é©—è­‰ PO {po_no}ï¼šå…±æœ‰ {len(verify_rows)} ç­†è³‡æ–™")
                            for idx, row in verify_rows.iterrows():
                                print(f"- å“é …={row.get('å“é …', '')}, RTé‡‘é¡={row.get('RTé‡‘é¡', '')}, RTç¸½é‡‘é¡={row.get('RTç¸½é‡‘é¡', '')}")
                                
                return jsonify({
                    'success': True,
                    'message': f'æˆåŠŸæ›´æ–° {int(updated_count)} ç­†è³‡æ–™',
                    'updated_count': int(updated_count),
                    'total_items': int(len(items_to_update)),
                    'should_cleanup': True,  # ç¢ºä¿æ·»åŠ æ¨™è¨˜å‘Šè¨´å‰ç«¯éœ€è¦æ¸…ç†
                    'cleanup_required': True  # é¡å¤–çš„æ¸…ç†æ¨™è¨˜
                })
                
            except Exception as e:
                logger.error(f"å„²å­˜ CSV å¤±æ•—: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'å„²å­˜æª”æ¡ˆå¤±æ•—: {str(e)}'
                }), 500
        else:
            # æä¾›æ›´è©³ç´°çš„éŒ¯èª¤è³‡è¨Š
            logger.warning("æ²’æœ‰ä»»ä½•è³‡æ–™è¢«æ›´æ–°")
            
            # çµ±è¨ˆå„ç¨®æƒ…æ³
            empty_po_count = sum(1 for item in items_to_update if not str(item.get('po_no', '')).strip())
            valid_po_count = len(items_to_update) - empty_po_count
            
            error_details = {
                'total_items': len(items_to_update),
                'valid_po_count': valid_po_count,
                'empty_po_count': empty_po_count,
                'updated_count': 0
            }
            
            if empty_po_count > 0:
                error_message = f'å…± {len(items_to_update)} ç­†è³‡æ–™ä¸­æœ‰ {empty_po_count} ç­†ç¼ºå°‘ PO è™Ÿç¢¼ï¼Œ{valid_po_count} ç­†åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ°åŒ¹é…é …ç›®'
            else:
                error_message = f'åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…çš„ PO è™Ÿç¢¼ (å…±æª¢æŸ¥ {len(items_to_update)} ç­†)'
            
            return jsonify({
                'success': False,
                'message': error_message,
                'error': 'å¾Œå°æŸ¥ç„¡æ­¤è³‡æ–™ï¼Œè«‹æª¢æŸ¥ PO è™Ÿç¢¼æ˜¯å¦æ­£ç¢º',
                'details': error_details,
                'updated_count': 0
            })
        
    except Exception as e:
        logger.error(f"æ›´æ–° Buyer CSV éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500





from flask import Flask, send_file, jsonify, Response
from io import BytesIO
import tempfile



# eRT åŠŸèƒ½
# 1.py
@app.route('/api/get_unaccounted_amount', methods=['GET'])
def get_unaccounted_amount():
    """
    çµ±è¨ˆå°šæœªå…¥å¸³çš„é‡‘é¡ (ä»¥ä»Šå¤©æ—¥æœŸç‚ºåŸºæº–)
    æ¢ä»¶ï¼š
    1. æœ‰æ‰¿è«¾äº¤æœŸ
    2. æ‰¿è«¾äº¤æœŸ <= ä»Šå¤©
    3. ç™¼ç¥¨æœˆä»½ç‚ºç©º
    4. ç§»é™¤ WBS
    """
    import datetime
    file_path = BUYER_CSV_PATH
    if not os.path.exists(file_path):
        return {"file": file_path, "unaccounted_amount": 0, "rows": []}

    try:
        df = pd.read_csv(file_path, encoding="utf-8-sig", dtype=str).fillna("")

        # æ—¥æœŸè™•ç†
        today = datetime.datetime.now().strftime("%Y%m%d")

        def clean_date(val):
            val = str(val).strip().replace("/", "").replace("-", "")
            return val if val.isdigit() and len(val) == 8 else ""

        df["äº¤æœŸ_clean"] = df["Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"].apply(clean_date)
        df["ç™¼ç¥¨æœˆä»½"] = df["ç™¼ç¥¨æœˆä»½"].astype(str).str.strip()

        # éæ¿¾æ¢ä»¶
        mask = (
            (df["äº¤æœŸ_clean"] != "") &
            (df["äº¤æœŸ_clean"] <= today) &
            (df["ç™¼ç¥¨æœˆä»½"] == "") & 
            (df["WBS"] == "")
        )
        filtered = df[mask].copy()

        # è™•ç†é‡‘é¡ï¼šå„ªå…ˆç”¨ã€Œç¸½åƒ¹ã€ï¼Œè‹¥æ²’æœ‰å°±ç”¨ æ•¸é‡*å–®åƒ¹
        def calc_amount(row):
            try:
                if row.get("RTç¸½é‡‘é¡") and str(row["RTç¸½é‡‘é¡"]).strip():
                    return float(str(row["RTç¸½é‡‘é¡"]).replace(",", "").replace("$", "").strip())
                elif row.get("ç¸½åƒ¹") and str(row["ç¸½åƒ¹"]).strip():
                    return float(str(row["ç¸½åƒ¹"]).replace(",", "").replace("$", "").strip())
                else:
                    qty = float(str(row.get("æ•¸é‡", "0")).replace(",", "").strip() or 0)
                    price = float(str(row.get("å–®åƒ¹", "0")).replace(",", "").strip() or 0)
                    return qty * price
            except:
                return 0.0

        filtered["é‡‘é¡"] = filtered.apply(calc_amount, axis=1)
        total_amount = round(filtered["é‡‘é¡"].sum(), 2)

        rows = []
        for _, row in filtered.iterrows():
            rows.append({
                "ePR No.": row.get("ePR No.", ""),
                "PO No.": row.get("PO No.", ""),
                "Item": row.get("Item", ""),
                "å“é …": row.get("å“é …", ""),
                "ç¸½åƒ¹": str(row.get("ç¸½åƒ¹", "")),
                "RTç¸½é‡‘é¡": str(row.get("RTç¸½é‡‘é¡", "")),
                "æ‰¿è«¾äº¤æœŸ": row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", ""),
                "ç™¼ç¥¨æœˆä»½": row.get("ç™¼ç¥¨æœˆä»½", ""),
                "è¨ˆç®—é‡‘é¡": row["é‡‘é¡"]  # æ”¹æˆé€™æ¨£
            })

        # print(f"{'PO No.':<12}{'Item':<8}{'å“é …':<30}{'äº¤æœŸ':<12}{'SOD Qty':<10}{'ç¸½åƒ¹':<12}")
        # # å°å‡ºæ¯ä¸€è¡Œ
        # for r in rows:
        #     print(f"{r['PO No.']:<12}{r['Item']:<8}{r['å“é …']:<30}{r['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ']:<12}{r['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']:<10}{r['ç¸½åƒ¹']:<12,.0f}")
        # print(total_amount)
        return jsonify({
            "unaccounted_amount": int(total_amount),
            "rows": rows
        })
    
    except Exception as e:
        return {"file": file_path, "error": str(e)}
    

# 3.py
@app.route("/api/accounting_summary", methods=["GET"])
def get_accounting_summary():
    from datetime import datetime
    import numpy as np
    
    # è®€å–CSVæª”æ¡ˆ
    df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype='str')

    # ç¯©é¸æ¢ä»¶ï¼šePR Noã€PO Noã€éœ€æ±‚æ—¥éƒ½ä¸ç‚ºç©ºå€¼
    filtered_df = df[
        (df['ePR No.'].notna()) & 
        (df['PO No.'].notna()) & 
        (df['éœ€æ±‚æ—¥'].notna()) &
        (df['ePR No.'] != '') & 
        (df['PO No.'] != '') & 
        (df['éœ€æ±‚æ—¥'] != '')
    ]

    def filter_for_accounting(df, target_month, target_year=2025):
        """
        ç¯©é¸æœªå…¥å¸³è³‡æ–™ - æ­£ç¢ºé‚è¼¯
        å¦‚æœç™¼ç¥¨æœˆä»½å’Œæ‰¿è«¾äº¤æœŸåœ¨åŒå€‹æœˆä»½ â†’ å·²å…¥å¸³ï¼Œç›´æ¥å¿½ç•¥
        """
        # å»ºç«‹ç›®æ¨™æœˆä»½çš„æœ€å¾Œä¸€å¤©
        if target_month == 12:
            target_date_end = datetime(target_year + 1, 1, 1)
        else:
            target_date_end = datetime(target_year, target_month + 1, 1)
        
        # è¤‡è£½è³‡æ–™é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        accounting_df = df.copy()
        
        # è§£ææ—¥æœŸ
        accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] = pd.to_datetime(accounting_df['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ'], errors='coerce', format='mixed')
        accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] = pd.to_datetime(accounting_df['ç™¼ç¥¨æœˆä»½'], errors='coerce', format='mixed')
        
        # è™•ç†æ•¸å­—æ ¼å¼çš„éœ€æ±‚æ—¥ (å¦‚ 20250523)
        def parse_numeric_date(val):
            if pd.isna(val):
                return pd.NaT
            try:
                val_str = str(int(float(val)))
                if len(val_str) == 8:
                    year = int(val_str[:4])
                    month = int(val_str[4:6])
                    day = int(val_str[6:8])
                    return pd.Timestamp(year, month, day)
            except:
                pass
            return pd.NaT
        
        accounting_df['éœ€æ±‚æ—¥_æ—¥æœŸ'] = accounting_df['éœ€æ±‚æ—¥'].apply(parse_numeric_date)
        
        # 0. æ™‚é–“å›æ¨é‚è¼¯ï¼šæ’é™¤åœ¨ç›®æ¨™æœˆä»½æˆ–ä¹‹å‰å·²é–‹ç™¼ç¥¨çš„è³‡æ–™
        target_date_start = datetime(target_year, target_month, 1)
        if target_month == 12:
            target_date_end = datetime(target_year + 1, 1, 1)
        else:
            target_date_end = datetime(target_year, target_month + 1, 1)
        
        # æ’é™¤å·²å…¥å¸³çš„è³‡æ–™ï¼šç™¼ç¥¨æ—¥æœŸåœ¨ç›®æ¨™æœˆä»½æˆ–ä¹‹å‰çš„
        already_paid_mask = (
            accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].notna() & 
            (accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] < target_date_end)
        )
        
        # æ’é™¤å·²å…¥å¸³çš„è³‡æ–™
        accounting_df = accounting_df[~already_paid_mask]
        
        # 1. EPR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        condition1 = (accounting_df['ePR No.'].notna()) & (accounting_df['PO No.'].notna()) & \
                    (accounting_df['ePR No.'] != '') & (accounting_df['PO No.'] != '')
        
        # 2. æ‰£é™¤ WBS æ¬„ä½æœ‰å€¼çš„è³‡æ–™ï¼ˆWBS å¿…é ˆç‚ºç©ºï¼‰
        condition2 = accounting_df['WBS'].isna() | (accounting_df['WBS'] == '')
        
        # 3. æ‰¿è«¾äº¤æœŸå¿…é ˆæœ‰å€¼ä¸”åœ¨ç•¶æœˆæˆ–ä¹‹å‰
        # condition3 = accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'].notna() & (accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] < target_date_end)
        condition3 = (
            accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'].notna() &
            (accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] >= target_date_start) &
            (accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] < target_date_end)
        )
        # # 4. éœ€æ±‚æ—¥å¿…é ˆåœ¨ç•¶æœˆä¹‹å‰  
        # condition4 = accounting_df['éœ€æ±‚æ—¥_æ—¥æœŸ'] < target_date_start
        
        # å¥—ç”¨æ‰€æœ‰æ¢ä»¶
        # final_condition = condition1 & condition2 & condition3 & condition4
        final_condition = condition1 & condition2 & condition3 
        result_df = accounting_df[final_condition]
        
        # è¨ˆç®—é‡‘é¡ (RTç¸½é‡‘é¡å„ªå…ˆï¼Œç©ºçš„å°±ç”¨ç¸½åƒ¹)
        result_df = result_df.copy()
        result_df['æœ€çµ‚é‡‘é¡_æ•¸å€¼'] = np.where(
            result_df['RTç¸½é‡‘é¡'].notna() & (result_df['RTç¸½é‡‘é¡'].astype(str).str.strip() != ''),
            result_df['RTç¸½é‡‘é¡'],
            result_df['ç¸½åƒ¹']
        )
        result_df['æœ€çµ‚é‡‘é¡_æ•¸å€¼'] = pd.to_numeric(
            result_df['æœ€çµ‚é‡‘é¡_æ•¸å€¼'].astype(str).str.replace(',', '').str.replace('$', ''), 
            errors='coerce'
        )
        total_amount = result_df['æœ€çµ‚é‡‘é¡_æ•¸å€¼'].sum()
        
        # æº–å‚™è©³ç´°è³‡æ–™è¡Œåˆ—è¡¨
        detailed_rows = []
        for _, row in result_df.iterrows():
            # Convert numpy types to native Python types
            amount = row.get("æœ€çµ‚é‡‘é¡_æ•¸å€¼", 0)
            if pd.isna(amount):
                amount = 0
            elif isinstance(amount, (int, np.integer)):
                amount = int(amount)
            elif isinstance(amount, (float, np.floating)):
                amount = float(amount)
                
            detailed_rows.append({
                "ePR No.": str(row.get("ePR No.", "")),
                "PO No.": str(row.get("PO No.", "")),
                "å“é …": str(row.get("å“é …", "")),
                "ç¸½åƒ¹": str(row.get("ç¸½åƒ¹", "")),
                "RTç¸½é‡‘é¡": str(row.get("RTç¸½é‡‘é¡", "")),
                "æ‰¿è«¾äº¤æœŸ": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                "éœ€æ±‚æ—¥": str(row.get("éœ€æ±‚æ—¥", "")),
                "ç™¼ç¥¨æœˆä»½": str(row.get("ç™¼ç¥¨æœˆä»½", "")),
                "WBS": str(row.get("WBS", "")),
                "è¨ˆç®—é‡‘é¡": amount
            })
        
        # Convert numpy/pandas types to native Python types
        total_amount_py = int(total_amount) if not pd.isna(total_amount) else 0
        
        return {
            "result_df": result_df,
            "total_amount": total_amount_py,
            "detailed_rows": detailed_rows,
            "conditions": {
                "condition1_count": int(condition1.sum()),
                "condition2_count": int(condition2.sum()), 
                "condition3_count": int(condition3.sum()),
                # "condition4_count": int(condition4.sum()),
                "already_paid_count": int(already_paid_mask.sum()),
                "final_condition_count": int(final_condition.sum())
            },
            "date_ranges": {
                "target_date_start": target_date_start.isoformat(),
                "target_date_end": target_date_end.isoformat(),
                "target_year": int(target_year),
                "target_month": int(target_month)
            }
        }

    # ç²å–è³‡æ–™ä¸­æ‰€æœ‰å¯èƒ½çš„å¹´æœˆä»½ (åªåˆ°ç•¶å‰æœˆä»½)
    def get_all_year_months(df):
        current_date = datetime.now()
        current_year = current_date.year
        current_month = current_date.month
        
        all_dates = []
        all_dates.extend(pd.to_datetime(df['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ'], errors='coerce').dropna())
        all_dates.extend(pd.to_datetime(df['éœ€æ±‚æ—¥'], errors='coerce').dropna())
        all_dates.extend(pd.to_datetime(df['ç™¼ç¥¨æœˆä»½'], errors='coerce').dropna())
        
        year_months = set()
        for date in all_dates:
            year, month = date.year, date.month
            if (year < current_year) or (year == current_year and month <= current_month):
                year_months.add((year, month))
        return sorted(list(year_months))

    # è¨ˆç®—æ‰€æœ‰æœˆä»½
    all_year_months = get_all_year_months(filtered_df)
    json_summary = {}
    detailed_data = {}
    all_conditions = {}
    all_date_ranges = {}

    for year, month in all_year_months:
        result_data = filter_for_accounting(df, month, year)
        year_month_key = f"{year}å¹´{month}æœˆ"
        
        json_summary[year_month_key] = result_data["total_amount"]
        detailed_data[year_month_key] = result_data["detailed_rows"]
        all_conditions[year_month_key] = result_data["conditions"]
        all_date_ranges[year_month_key] = result_data["date_ranges"]
    
    # Convert all year_months tuples to native Python types
    all_year_months_py = [(int(year), int(month)) for year, month in all_year_months]
    
    # å›å‚³æ‰€æœ‰ç‰©ä»¶
    return jsonify({
        "summary": json_summary,
        "detailed_data": detailed_data,
        "conditions": all_conditions,
        "date_ranges": all_date_ranges,
        "meta": {
            "total_months": len(all_year_months),
            "all_year_months": all_year_months_py,
            "original_df_count": int(len(df)),
            "filtered_df_count": int(len(filtered_df)),
            "csv_path": str(BUYER_CSV_PATH)
        }
    })


# 2_4.py
@app.route("/api/monthly_actual_accounting", methods=["GET"])
def get_monthly_actual_accounting():
    import datetime
    # è®€å–CSVæª”æ¡ˆ
    df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype='str')

    # ç¯©é¸æ¢ä»¶ï¼šePR Noã€PO Noã€éœ€æ±‚æ—¥éƒ½ä¸ç‚ºç©ºå€¼
    filtered_df = df[
        (df['ePR No.'].notna()) & 
        (df['PO No.'].notna()) & 
        (df['éœ€æ±‚æ—¥'].notna()) &
        (df['ePR No.'] != '') & 
        (df['PO No.'] != '') & 
        (df['éœ€æ±‚æ—¥'] != '')
    ].copy()

    def calculate_monthly_actual_accounting(df):
        """
        è¨ˆç®—æ¯æœˆå¯¦éš›å…¥å¸³é‡‘é¡
        æ¢ä»¶ï¼š
        1. ePR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        2. ä¸åŒ…å«WBS (WBSå¿…é ˆç‚ºç©º)
        3. ç™¼ç¥¨æœˆä»½ä¸ç‚ºç©º (æœ‰ç™¼ç¥¨æ‰ç®—å…¥å¸³)
        """
        accounting_df = df.copy()

        # 1. EPR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        condition1 = (
            accounting_df['ePR No.'].notna() &
            accounting_df['PO No.'].notna() &
            (accounting_df['ePR No.'] != '') &
            (accounting_df['PO No.'] != '')
        )

        # 2. ä¸å«WBS (WBS å¿…é ˆç‚ºç©º)
        condition2 = accounting_df['WBS'].isna() | (accounting_df['WBS'] == '')

        # 3. ç™¼ç¥¨æœˆä»½ä¸ç‚ºç©º
        accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] = pd.to_datetime(accounting_df['ç™¼ç¥¨æœˆä»½'], errors='coerce', format='mixed')
        condition3 = accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].notna()

        # å¥—ç”¨æ‰€æœ‰æ¢ä»¶
        final_condition = condition1 & condition2 & condition3
        result_df = accounting_df[final_condition].copy()

        # åŠ å…¥ç™¼ç¥¨å¹´æœˆæ¬„ä½ (æ ¼å¼: YYYY-MM)
        result_df['ç™¼ç¥¨å¹´æœˆ'] = result_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].dt.to_period('M').astype(str)

        # è½‰æ› RTç¸½é‡‘é¡ ç‚ºæ•¸å€¼
        result_df['RTç¸½é‡‘é¡_æ•¸å€¼'] = pd.to_numeric(
            result_df['RTç¸½é‡‘é¡'].astype(str).str.replace(',', '').str.replace('$', ''),
            errors='coerce'
        )

        return result_df

    # ==== åŸ·è¡Œè¨ˆç®— ====
    actual_accounting_df = calculate_monthly_actual_accounting(filtered_df)

    if len(actual_accounting_df) == 0:
        return jsonify({
            "æœ¬æœˆ": {"amount": 0, "details": []},
            "ä¸Šæœˆ": {"amount": 0, "details": []}
        })

    # ==== æº–å‚™è©³ç´°è³‡æ–™ ====
    # åªå–éœ€è¦çš„æ¬„ä½
    detail_columns = [
        'ePR No.', 'PO No.', 'å“é …', 'ç¸½åƒ¹', 'RTç¸½é‡‘é¡',
        'Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ', 'éœ€æ±‚æ—¥', 'ç™¼ç¥¨æœˆä»½', 'WBS'
    ]

    # æŒ‰ã€Œç™¼ç¥¨å¹´æœˆã€åˆ†çµ„
    monthly_details = {}
    for period, group in actual_accounting_df.groupby('ç™¼ç¥¨å¹´æœˆ'):
        amount = group['RTç¸½é‡‘é¡_æ•¸å€¼'].sum()
        details = []
        for _, row in group.iterrows():
            # è½‰æ›é‡‘é¡ç‚ºæ•´æ•¸
            rt_amount_val = row['RTç¸½é‡‘é¡_æ•¸å€¼']
            rt_amount = int(rt_amount_val) if pd.notna(rt_amount_val) else 0

            detail = {col: str(row[col]) if pd.notna(row[col]) else "" for col in detail_columns}
            detail["è¨ˆç®—é‡‘é¡"] = str(rt_amount)
            details.append(detail)

        monthly_details[period] = {
            "amount": int(amount),
            "details": details
        }

    # ==== å–å¾—æœ¬æœˆèˆ‡ä¸Šæœˆ key (YYYY-MM) ====
    now = datetime.datetime.now()
    this_month_str = f"{now.year}-{now.month:02d}"
    last_month = now.month - 1 if now.month > 1 else 12
    last_year = now.year if now.month > 1 else now.year - 1
    last_month_str = f"{last_year}-{last_month:02d}"

    # å–å¾—è³‡æ–™ï¼Œè‹¥ç„¡å‰‡å›å‚³é è¨­
    this_data = monthly_details.get(this_month_str, {"amount": 0, "details": []})
    last_data = monthly_details.get(last_month_str, {"amount": 0, "details": []})

    # ==== å›å‚³çµæœ ====
    return jsonify({
        "æœ¬æœˆ": this_data,
        "ä¸Šæœˆ": last_data
    })



# é©—æ”¶å€å¡Š

import re
import json
import traceback
import logging
import quopri
from urllib.parse import unquote
import pandas as pd
import os
from acceptanceWeb.parse import accMHTMLParser
from MailFunction.acceptanceMail import send_mail


def normalize_name(name):
    """æ¨™æº–åŒ–å§“åï¼Œè™•ç†ç‰¹æ®Šå­—ç¬¦å•é¡Œ"""
    if not name or pd.isna(name):
        return name
    
    name_str = str(name).strip()
    
    # å°‡ éƒ­ä»»? æ”¹ç‚º éƒ­ä»»ç¾¤
    if name_str.startswith('éƒ­ä»»') and len(name_str) == 3:
        # å¦‚æœç¬¬ä¸‰å€‹å­—æ˜¯å•è™Ÿæˆ–å…¶ä»–ç‰¹æ®Šå­—ç¬¦ï¼Œçµ±ä¸€æ”¹ç‚ºç¾¤
        if name_str[2] not in ['ç¾¤']:  # å¦‚æœä¸æ˜¯ç¾¤å­—ï¼Œå°±æ”¹ç‚ºç¾¤
            logger.info(f"å§“åä¿®æ­£: '{name_str}' -> 'éƒ­ä»»ç¾¤'")
            return 'éƒ­ä»»ç¾¤'
    
    return name_str

def query_user_by_po_no(po_no):
    """æ ¹æ“š PO No. æŸ¥è©¢éœ€æ±‚è€…"""
    try:
        planned_purchase_df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        planned_purchase_df.fillna("", inplace=True)

        # è½‰æ› po_no ç‚ºå­—ä¸²é€²è¡Œæ¯”è¼ƒ
        po_no_str = str(po_no).strip()
        logger.info(f"æŸ¥è©¢éœ€æ±‚è€…ï¼ŒPO No: '{po_no_str}'")
        
        # æª¢æŸ¥ CSV ä¸­çš„æ¬„ä½åç¨±
        logger.info(f"Planned_Purchase CSV æ¬„ä½: {list(planned_purchase_df.columns)}")
        
        # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±åŒ¹é…
        po_column = None
        user_column = None
        
        for col in planned_purchase_df.columns:
            if 'PO' in col and 'No' in col:
                po_column = col
            if 'éœ€æ±‚è€…' in col:
                user_column = col
        
        if po_column is None or user_column is None:
            logger.warning(f"æ‰¾ä¸åˆ°å°æ‡‰æ¬„ä½ - POæ¬„ä½: {po_column}, éœ€æ±‚è€…æ¬„ä½: {user_column}")
            return None
        
        # æ¸…ç†å’Œè½‰æ› PO No. æ¬„ä½
        planned_purchase_df[po_column] = planned_purchase_df[po_column].astype(str).str.strip()
        
        # æŸ¥è©¢è³‡æ–™
        result = planned_purchase_df[planned_purchase_df[po_column] == po_no_str]
        
        logger.info(f"æŸ¥è©¢çµæœæ•¸é‡: {len(result)}")
        
        if not result.empty:
            user = result.iloc[0][user_column]
            # æ¨™æº–åŒ–å§“å
            normalized_user = normalize_name(user)
            logger.info(f"æ‰¾åˆ° PO {po_no} çš„éœ€æ±‚è€…: {user} -> {normalized_user}")
            return normalized_user
        else:
            # å¦‚æœç²¾ç¢ºåŒ¹é…å¤±æ•—ï¼Œå˜—è©¦éƒ¨åˆ†åŒ¹é…
            logger.info(f"ç²¾ç¢ºåŒ¹é…å¤±æ•—ï¼Œå˜—è©¦éƒ¨åˆ†åŒ¹é…...")
            partial_result = planned_purchase_df[planned_purchase_df[po_column].str.contains(po_no_str, na=False)]
            if not partial_result.empty:
                user = partial_result.iloc[0][user_column]
                normalized_user = normalize_name(user)
                logger.info(f"éƒ¨åˆ†åŒ¹é…æ‰¾åˆ° PO {po_no} çš„éœ€æ±‚è€…: {user} -> {normalized_user}")
                return normalized_user
            
            # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™ä¾›é™¤éŒ¯
            logger.info(f"å‰5ç­† PO No. è³‡æ–™: {planned_purchase_df[po_column].head().tolist()}")
            
    except Exception as e:
        logger.error(f"æŸ¥è©¢éœ€æ±‚è€…æ™‚ç™¼ç”ŸéŒ¯èª¤ (PO: {po_no}): {str(e)}")
        logger.error(traceback.format_exc())
    
    return None

def query_epr_no_by_po_no(po_no):
    """æ ¹æ“š PO No. æŸ¥è©¢ ePR No. - ä¸»è¦å¾ Buyer_detail.csv æŸ¥è©¢"""
    try:
        po_no_str = str(po_no).strip()
        logger.info(f"æŸ¥è©¢ ePR No.ï¼ŒPO No: '{po_no_str}'")
        
        # ä¸»è¦å¾ Buyer_detail.csv æŸ¥è©¢ï¼ˆé€™è£¡æœ‰å®Œæ•´çš„è³‡æ–™ï¼‰
        buyer_detail_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)
        if buyer_detail_df is not None:
            logger.info(f"å¾ Buyer_detail æŸ¥è©¢ (å…± {len(buyer_detail_df)} ç­†è³‡æ–™)")
            
            po_column = 'PO No.'
            epr_column = 'ePR No.'
            
            if po_column in buyer_detail_df.columns and epr_column in buyer_detail_df.columns:
                # è™•ç†æµ®é»æ•¸æ ¼å¼çš„ PO No.
                # å°‡æŸ¥è©¢çš„ PO No. è½‰æ›ç‚ºæ•¸å­—é€²è¡Œæ¯”è¼ƒ
                try:
                    po_no_numeric = po_no_str
                    logger.info(f"å°‡ PO No. è½‰æ›ç‚ºæ•¸å­—: {po_no_numeric}")
                    
                    # æŸ¥æ‰¾åŒ¹é…çš„è¨˜éŒ„ - ä½¿ç”¨æ•¸å­—æ¯”è¼ƒ
                    result = buyer_detail_df[
                        (buyer_detail_df[po_column].notna()) &  # ä¸æ˜¯ç©ºå€¼
                        (buyer_detail_df[po_column] == po_no_numeric)  # æ•¸å­—åŒ¹é…
                    ]
                    
                    logger.info(f"æ•¸å­—åŒ¹é…æŸ¥è©¢åˆ° {len(result)} ç­†çµæœ")
                    
                    if not result.empty:
                        epr_no = result.iloc[0][epr_column]
                        logger.info(f"æ‰¾åˆ°çš„ ePR No. åŸå§‹å€¼: '{epr_no}', é¡å‹: {type(epr_no)}")
                        
                        if pd.notna(epr_no) and str(epr_no).strip() not in ['', 'nan', 'NaN']:
                            try:
                                # è½‰æ› ePR No. ç‚ºæ•´æ•¸å­—ä¸²
                                if isinstance(epr_no, (int, float)):
                                    epr_result = str(int(epr_no))
                                else:
                                    epr_result = str(epr_no).strip()
                                
                                logger.info(f"âœ“ åœ¨ Buyer_detail æ‰¾åˆ° PO {po_no} çš„ ePR No.: {epr_result}")
                                return epr_result
                            except (ValueError, TypeError) as e:
                                logger.error(f"ePR No. è½‰æ›å¤±æ•—: {e}")
                                epr_result = str(epr_no).strip()
                                logger.info(f"âœ“ ä½¿ç”¨åŸå§‹å­—ä¸²æ ¼å¼ ePR No.: {epr_result}")
                                return epr_result
                        else:
                            logger.warning(f"Buyer_detail ä¸­æ‰¾åˆ°è¨˜éŒ„ä½† ePR No. ç‚ºç©º: '{epr_no}'")
                    else:
                        logger.warning(f"åœ¨ Buyer_detail ä¸­æœªæ‰¾åˆ° PO {po_no}")
                        # é¡¯ç¤ºå¯ç”¨çš„ PO No. ä¾›å°æ¯”
                        valid_pos = buyer_detail_df[buyer_detail_df[po_column].notna()][po_column].head(10).tolist()
                        logger.info(f"Buyer_detail ä¸­çš„æœ‰æ•ˆ PO No. æ¨£æœ¬: {valid_pos}")
                        
                except ValueError as e:
                    logger.error(f"PO No. è½‰æ›ç‚ºæ•¸å­—å¤±æ•—: {e}")
                    return None
            else:
                logger.error(f"Buyer_detail æ‰¾ä¸åˆ°å¿…è¦æ¬„ä½ - POæ¬„ä½: {po_column}, ePRæ¬„ä½: {epr_column}")
        else:
            logger.error("Buyer_detail.csv æœªè¼‰å…¥ï¼")
        
        # å¦‚æœ Buyer_detail æ‰¾ä¸åˆ°ï¼Œæ‰å˜—è©¦ Planned_Purchaseï¼ˆå‚™ç”¨ï¼‰
        logger.info("Buyer_detail æŸ¥è©¢å¤±æ•—ï¼Œå˜—è©¦ Planned_Purchase ä½œç‚ºå‚™ç”¨")
        planned_purchase_df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        if planned_purchase_df is not None:
            logger.info(f"å¾ Planned_Purchase æŸ¥è©¢ (å…± {len(planned_purchase_df)} ç­†è³‡æ–™)")
            
            po_column = 'PO No.'
            epr_column = 'ePR No.'
            
            if po_column in planned_purchase_df.columns and epr_column in planned_purchase_df.columns:
                try:
                    po_no_numeric = po_no_str
                    result = planned_purchase_df[
                        (planned_purchase_df[po_column].notna()) &
                        (planned_purchase_df[po_column] == po_no_numeric)
                    ]
                    
                    if not result.empty:
                        epr_no = result.iloc[0][epr_column]
                        if pd.notna(epr_no) and str(epr_no).strip() not in ['', 'nan', 'NaN']:
                            epr_result = str(int(float(epr_no))) if isinstance(epr_no, (int, float)) else str(epr_no).strip()
                            logger.info(f"âœ“ åœ¨ Planned_Purchase æ‰¾åˆ° PO {po_no} çš„ ePR No.: {epr_result}")
                            return epr_result
                except ValueError:
                    pass
                
    except Exception as e:
        logger.error(f"æŸ¥è©¢ ePR No. æ™‚ç™¼ç”ŸéŒ¯èª¤ (PO: {po_no}): {str(e)}")
        logger.error(traceback.format_exc())
    
    logger.error(f"âœ— æœ€çµ‚æœªæ‰¾åˆ° PO {po_no} çš„ ePR No.")
    return None



@app.route('/api/parse-mhtml', methods=['POST'])
def parse_mhtml():
    """è™•ç† MHTML æ–‡ä»¶ä¸Šå‚³å’Œè§£æ"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆ
        if 'file' not in request.files:
            return jsonify({'error': 'æ²’æœ‰ä¸Šå‚³æª”æ¡ˆ'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400
        
        # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
        if not file.filename.lower().endswith(('.mhtml', '.mht')): # type: ignore
            return jsonify({'error': 'æª”æ¡ˆæ ¼å¼ä¸æ­£ç¢ºï¼Œè«‹ä¸Šå‚³ MHTML æ ¼å¼æª”æ¡ˆ'}), 400
        
        # è®€å–æª”æ¡ˆå…§å®¹
        file_content = file.read().decode('utf-8', errors='ignore')
        
        logger.info(f"æ”¶åˆ°æª”æ¡ˆ: {file.filename}, å¤§å°: {len(file_content)} å­—å…ƒ")
        
        # å‰µå»ºè§£æå™¨ä¸¦è§£ææª”æ¡ˆ
        parser = accMHTMLParser()
        parsed_data = parser.parse_mhtml_file(file_content)
        
        # è¼¸å‡ºè§£æçµæœåˆ°å¾Œå°æ—¥èªŒ
        logger.info("=" * 50)
        logger.info("è§£æçµæœ:")
        logger.info(f"æª”æ¡ˆåç¨±: {file.filename}")
        logger.info(f"è§£æç­†æ•¸: {len(parsed_data)}")
        logger.info("-" * 30)
        
        for i, item in enumerate(parsed_data, 1):
            logger.info(f"ç¬¬ {i} ç­†:")
            logger.info(f"  RT No: {item.get('rtNo')}")
            logger.info(f"  é …æ¬¡: {item.get('itemNo')}")
            logger.info(f"  PO No: {item.get('poNo')}")
            logger.info(f"  ç§‘ç›®æŒ‡æ´¾: {item.get('itemAssign')} - {item.get('itemAssignName')}")
            logger.info(f"  å“å: {item.get('description')}")
            logger.info(f"  æ•¸é‡: {item.get('quantity')}")
            logger.info(f"  å–ä»¶è€…: {item.get('pickupPerson')}")
            logger.info("-" * 30)
        
        logger.info("=" * 50)
        
        # è¿”å›è§£æçµæœ
        return jsonify({
            'success': True,
            'filename': file.filename,
            'count': len(parsed_data),
            'data': parsed_data
        })
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
        logger.error(traceback.format_exc())
        return jsonify({'error': error_msg}), 500

@app.route('/api/get-user-epr-data', methods=['POST'])
def get_user_epr_data():
    """æ ¹æ“šPOè™Ÿç¢¼æŸ¥è©¢å°æ‡‰çš„Userå’ŒePR No."""
    try:
        data = request.get_json()
        po_numbers = data.get('poNumbers', [])

        planned_purchase_df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        buyer_detail_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)
        logger.info(f"æ”¶åˆ°æŸ¥è©¢è«‹æ±‚ï¼ŒPO è™Ÿç¢¼: {po_numbers}")
        logger.info(f"buyer_detail_df ç‹€æ…‹: {'å·²è¼‰å…¥' if buyer_detail_df is not None else 'æœªè¼‰å…¥'}")
        logger.info(f"planned_purchase_df ç‹€æ…‹: {'å·²è¼‰å…¥' if planned_purchase_df is not None else 'æœªè¼‰å…¥'}")
        
        if buyer_detail_df is not None:
            logger.info(f"Buyer_detail è³‡æ–™ç­†æ•¸: {len(buyer_detail_df)}")
            logger.info(f"Buyer_detail æ¬„ä½: {list(buyer_detail_df.columns)}")
            logger.info(f"Buyer_detail å‰3ç­† PO No.: {buyer_detail_df.iloc[:3]['PO No.'].tolist() if 'PO No.' in buyer_detail_df.columns else 'æ‰¾ä¸åˆ°PO No.æ¬„ä½'}")
        
        if planned_purchase_df is not None:
            logger.info(f"Planned_purchase è³‡æ–™ç­†æ•¸: {len(planned_purchase_df)}")
            logger.info(f"Planned_purchase æ¬„ä½: {list(planned_purchase_df.columns)}")
            logger.info(f"Planned_purchase å‰3ç­† PO No.: {planned_purchase_df.iloc[:3]['PO No.'].tolist() if 'PO No.' in planned_purchase_df.columns else 'æ‰¾ä¸åˆ°PO No.æ¬„ä½'}")
        
        result = {}
        
        for po_no in po_numbers:
            logger.info(f"é–‹å§‹è™•ç† PO: {po_no}")
            
            # æŸ¥è©¢éœ€æ±‚è€…å’Œ ePR No.
            user = query_user_by_po_no(po_no)
            epr_no = query_epr_no_by_po_no(po_no)
            
            result[str(po_no)] = {
                'user': user if user else 'æŸ¥ç„¡æ­¤è³‡è¨Š',
                'eprNo': epr_no if epr_no else 'æŸ¥ç„¡æ­¤è³‡è¨Š'
            }
            
            logger.info(f"PO {po_no} æœ€çµ‚çµæœ: User={result[str(po_no)]['user']}, ePR={result[str(po_no)]['eprNo']}")
        
        logger.info(f"å›å‚³å®Œæ•´çµæœ: {result}")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"æŸ¥è©¢Userå’ŒePRè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': str(e)}), 500


def get_notes_email(user_name, backend_file="Backend_data.json"):
    """ç”¨å§“åæŸ¥ Notes_ID (å®Œæ•´ä¿¡ç®±)"""
    with open(backend_file, "r", encoding="utf-8-sig") as f:
        data = json.load(f)
    for entry in data:
        if entry.get("å§“å") == user_name:
            first_supervisor = entry.get("ç¬¬ä¸€éšä¸»ç®¡", "").strip()
            if first_supervisor == "G9745 LC Wang ç‹åˆ©å“²":
                return entry.get("Notes_ID", "")
            else:
                # å¼·åˆ¶æ”¹æˆ Otis_Wang
                return "Otis_Wang@aseglobal.com"
    return None


def get_notes_prefix(user_name, backend_file="Backend_data.json"):
    with open(backend_file, "r", encoding="utf-8-sig") as f:
        data = json.load(f)
    
    for entry in data:
        if entry.get("å§“å") == user_name:
            notes_id = entry.get("Notes_ID", "")
            if "@" in notes_id:
                prefix = notes_id.split("@")[0]  # å– @ å‰
                if "_" in prefix:
                    return prefix.split("_")[0]   # å†å– _ å‰
                return prefix
            return notes_id  # å¦‚æœæ²’æœ‰ @ï¼Œå°±ç›´æ¥å›å‚³
    return None


def build_greeting(mail_data, backend_file="Backend_data.json"):
    """å»ºç«‹å•å€™èª,å¾ mail_data ä¸­æå–ä½¿ç”¨è€…ä¸¦è½‰æ›æˆè‹±æ–‡å"""
    # å¾ mail_data ä¸­æå–æ‰€æœ‰ user
    users = [item.get("user") for item in mail_data if item.get("user")]
    
    # å»é‡ä¸¦éæ¿¾ç©ºå€¼å’Œç„¡æ•ˆå€¼
    unique_users = list(dict.fromkeys([u for u in users if u and u not in ["æŸ¥ç„¡æ­¤è³‡è¨Š", "-", ""]]))
    
    logger.info(f"æå–åˆ°çš„ä½¿ç”¨è€…: {unique_users}")
    
    if not unique_users:
        return "Dear all"
    
    # ç²å–æ¯å€‹ä½¿ç”¨è€…çš„è‹±æ–‡å
    english_names = []
    for user in unique_users:
        english_name = get_notes_prefix(user, backend_file)
        if english_name:
            english_names.append(english_name)
            logger.info(f"ä½¿ç”¨è€… {user} -> è‹±æ–‡å: {english_name}")
        else:
            logger.warning(f"æ‰¾ä¸åˆ°ä½¿ç”¨è€… {user} çš„è‹±æ–‡å")
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ä»»ä½•è‹±æ–‡å,ä½¿ç”¨ Dear all
    if not english_names:
        logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•è‹±æ–‡å,ä½¿ç”¨ Dear all")
        return "Dear all"
    
    # çµ„åˆæˆ "Dear Name1, Name2, Name3"
    if len(english_names) == 1:
        greeting = f"Dear {english_names[0]}"
    else:
        greeting = f"Dear {', '.join(english_names)}"
    
    logger.info(f"ç”Ÿæˆçš„å•å€™èª: {greeting}")
    return greeting


@app.route('/api/save-mail', methods=['POST', 'OPTIONS'])
def save_mail():
    if request.method == 'OPTIONS':
        response = jsonify({'status': 'ok'})
        response.headers.add('Access-Control-Allow-Origin', '*')
        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')
        response.headers.add('Access-Control-Allow-Methods', 'POST, OPTIONS')
        return response
    
    try:
        data = request.get_json()
        # print(f"æ”¶åˆ°éƒµä»¶ä¿®æ”¹è³‡æ–™: {json.dumps(data, indent=2, ensure_ascii=False)}")
        
        # æå–éƒµä»¶è³‡æ–™
        mail_data = data.get('data', [])
        
        if not mail_data:
            return jsonify({
                'status': 'error',
                'message': 'æ²’æœ‰è¦ç™¼é€çš„è³‡æ–™'
            }), 400
        
        logger.info(f"æº–å‚™ç™¼é€éƒµä»¶,å…± {len(mail_data)} ç­†è³‡æ–™")
        
        # èª¿ç”¨ mail.py ç™¼é€éƒµä»¶
        try:
            # å–å¾— PO No. å­—ä¸²
            po_numbers = list({item.get('poNo', '-') for item in mail_data if item.get('poNo')})
            po_str = "ã€".join(po_numbers)


            # å–å¾— user prefix for To å­—ä¸²
            user_names = {item.get('user') for item in mail_data if item.get('user')}
            to_prefixes = []
            for name in user_names:
                prefix = get_notes_prefix(name, "Backend_data.json")
                if prefix:
                    to_prefixes.append(prefix)
            to_str = ", ".join(to_prefixes)

            users = [item.get("user") for item in mail_data if item.get("user")]
            unique_users = list(dict.fromkeys([u for u in users if u]))
            notes_emails = []
            for user in unique_users:
                email = get_notes_email(user, "Backend_data.json")
                if email:
                    notes_emails.append(email)

            # çµ„åˆæˆé€—è™Ÿåˆ†éš”çš„å­—ä¸²
            mail_name = ",".join(notes_emails)

            # âœ¨ ç”Ÿæˆå•å€™èª
            greeting = build_greeting(mail_data, "Backend_data.json")
            logger.info(f"ç”Ÿæˆçš„å•å€™èª: {greeting}")

            # æº–å‚™éƒµä»¶åƒæ•¸
            name = "é©—æ”¶é€šçŸ¥ç³»çµ±"
            mail_name = mail_name  # å¯ä»¥æ ¹æ“šéœ€è¦è¨­å®šæ”¶ä»¶äºº
            ccList = ""  # ä¿æŒæ·¨ç©º
            po_str = po_str
            to_str = to_str
            
            logger.info(f"POå­—ä¸²: {po_str}")
            logger.info(f"TO æ”¶ä»¶äºº: {mail_name}")
            logger.info(f"TO å­—ä¸²: {to_str}")
            logger.info(f"å•å€™èª: {greeting}")
            
            logger.info(f"PO: {po_str}\nTO: {mail_name}\nGreeting: {greeting}")
            
            # å‘¼å« send_mail å‡½æ•¸,ä¸¦å‚³å…¥ greeting
            # print(mail_data, "\n")
            # print(name, "\n")
            # print(mail_name, "\n")
            # print(ccList, "\n")
            # print(po_str, "\n")
            # print(to_str, "\n")
            # print(greeting, "\n")
            # def send_mail(mailList, mail_name, ccList, po_str, to_str, greeting="Dear "):
            send_mail(mail_data, mail_name, ccList, po_str, to_str, greeting)
            
            logger.info(f"âœ… éƒµä»¶ç™¼é€æˆåŠŸ,è™•ç†äº† {len(mail_data)} ç­†è³‡æ–™")
            
            return jsonify({
                "status": "success",
                "message": f"éƒµä»¶ç™¼é€æˆåŠŸ,å·²è™•ç† {len(mail_data)} ç­†è³‡æ–™",
                "timestamp": data.get('timestamp'),
                "saved_items_count": len(mail_data),
                "mail_sent": True,
                "greeting": greeting
            })
            
        except Exception as mail_error:
            logger.error(f"âŒ éƒµä»¶ç™¼é€å¤±æ•—: {str(mail_error)}")
            logger.error(traceback.format_exc())
            
            # å³ä½¿éƒµä»¶ç™¼é€å¤±æ•—,ä¹Ÿå›å‚³éƒ¨åˆ†æˆåŠŸçš„ç‹€æ…‹
            return jsonify({
                "status": "partial_success",
                "message": f"è³‡æ–™å·²ä¿å­˜ä½†éƒµä»¶ç™¼é€å¤±æ•—: {str(mail_error)}",
                "timestamp": data.get('timestamp'),
                "saved_items_count": len(mail_data),
                "mail_sent": False,
                "error_detail": str(mail_error)
            }), 200  # ç”¨200è€Œé500,å› ç‚ºè³‡æ–™è™•ç†æˆåŠŸäº†
        
    except Exception as e:
        logger.error(f"è™•ç†è«‹æ±‚å¤±æ•—: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'status': 'error',
            'message': f'è™•ç†å¤±æ•—: {str(e)}'
        }), 500



# 11/25 æ›´æ–°
# ========== é•·å®˜å¯©æ ¸ç›¸é—œ APIï¼ˆé›™é‡ç°½æ ¸ç‰ˆ - ä¸»ä»»ç°½æ ¸ + å”å”ç°½æ ¸ï¼Œç§»é™¤é•·å®˜ç¢ºèªæ¬„ä½ï¼‰==========
@app.route('/api/add-item-with-notification', methods=['POST'])
def add_item_with_notification():
    """æ–°å¢è³‡æ–™ï¼ˆä¸ç™¼é€éƒµä»¶ï¼‰"""
    try:
        data = request.json
        
        # è®€å–ç¾æœ‰ CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # â­ é¿å… nan å•é¡Œ
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
        
        # æº–å‚™æ–°å¢çš„è³‡æ–™
        new_row = {}
        for col in df.columns:
            new_row[col] = data.get(col, '')
        
        # é è¨­ç°½æ ¸ç‹€æ…‹ç‚º X (æœªç¢ºèª)
        new_row['ä¸»ä»»ç°½æ ¸'] = 'X'
        new_row['å”å”ç°½æ ¸'] = 'X'
        
        # æ–°å¢åˆ° DataFrame
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        
        # å„²å­˜ CSV
        df = df.fillna('')  # â­ å¯«å…¥å‰ç¢ºä¿æ²’æœ‰ nan
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        return jsonify({
            'status': 'success',
            'message': 'è³‡æ–™æ–°å¢æˆåŠŸ',
            'new_item_id': new_row.get('Id', '')
        })
        
    except Exception as e:
        logger.info(f"æ–°å¢è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/get-pending-approval-items', methods=['GET'])
def get_pending_approval_items():
    """å–å¾—æ‰€æœ‰å¾…é•·å®˜ç¢ºèªçš„è³‡æ–™"""
    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # â­ é¿å… nan å•é¡Œ
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
            df = df.fillna('')
            df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        # ç¯©é¸å‡ºå¾…å¯©æ ¸çš„è³‡æ–™ï¼š
        # 1. ä¸»ä»»ç°½æ ¸æˆ–å”å”ç°½æ ¸æœ‰ä»»ä¸€å€‹ä¸æ˜¯ V
        # 2. ä¸”æ²’æœ‰è¢«é€€å›ï¼ˆéƒ½ä¸æ˜¯ Rï¼‰
        def is_pending(row):
            director = str(row['ä¸»ä»»ç°½æ ¸']).strip() if pd.notna(row['ä¸»ä»»ç°½æ ¸']) else 'X'
            uncle = str(row['å”å”ç°½æ ¸']).strip() if pd.notna(row['å”å”ç°½æ ¸']) else 'X'
            
            # å¦‚æœæœ‰é€€å›å°±ä¸ç®—å¾…å¯©æ ¸
            if director == 'R' or uncle == 'R':
                return False
            # å¦‚æœå…©å€‹éƒ½æ˜¯ V å°±ä¸ç®—å¾…å¯©æ ¸
            if director == 'V' and uncle == 'V':
                return False
            # å…¶ä»–æƒ…æ³éƒ½æ˜¯å¾…å¯©æ ¸
            return True
        
        pending_items = df[df.apply(is_pending, axis=1)]
        
        # è½‰æ›ç‚º dict list
        items_list = []
        for _, row in pending_items.iterrows():
            item_dict = {}
            for col in df.columns:
                val = row[col]
                if pd.isna(val):
                    item_dict[col] = ''
                else:
                    item_dict[col] = str(val)
            items_list.append(item_dict)
        
        return jsonify({
            'status': 'success',
            'items': items_list,
            'count': len(items_list)
        })
        
    except Exception as e:
        logger.info(f"å–å¾—å¾…å¯©æ ¸è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/approve-items', methods=['POST'])
def approve_items():
    """æ‰¹æ¬¡ç¢ºèªè³‡æ–™ï¼ˆæ”¯æ´ä¸»ä»»ç°½æ ¸å’Œå”å”ç°½æ ¸ï¼‰"""
    try:
        data = request.json
        item_ids = data.get('item_ids', [])
        approve_director = data.get('approve_director', True)  # æ˜¯å¦ç¢ºèªä¸»ä»»ç°½æ ¸
        approve_uncle = data.get('approve_uncle', True)  # æ˜¯å¦ç¢ºèªå”å”ç°½æ ¸
        clear_remarks = data.get('clear_remarks', False)  # æ˜¯å¦æ¸…ç©ºé€€å›åŸå› 
        
        if not item_ids:
            return jsonify({
                'status': 'error',
                'message': 'æœªæä¾›è¦ç¢ºèªçš„è³‡æ–™ ID'
            }), 400
        
        # è®€å– CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # â­ é¿å… nan å•é¡Œ
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
        if 'å‚™è¨»' not in df.columns:
            df['å‚™è¨»'] = ''
        
        # æ›´æ–°æŒ‡å®š ID çš„ç°½æ ¸ç‹€æ…‹
        updated_count = 0
        for item_id in item_ids:
            mask = df['Id'].astype(str).str.strip() == str(item_id).strip()
            if mask.any():
                # æ ¹æ“šåƒæ•¸è¨­å®šå°æ‡‰çš„ç°½æ ¸ç‹€æ…‹
                if approve_director:
                    df.loc[mask, 'ä¸»ä»»ç°½æ ¸'] = 'V'
                if approve_uncle:
                    df.loc[mask, 'å”å”ç°½æ ¸'] = 'V'
                
                # é¸æ“‡æ€§æ¸…ç©ºé€€å›åŸå› 
                if clear_remarks:
                    current_remark = df.loc[mask, 'å‚™è¨»'].values[0]
                    current_remark = str(current_remark) if not pd.isna(current_remark) else ''
                    
                    # ç§»é™¤æ‰€æœ‰ã€ä¸»ä»»é€€å›ã€‘å’Œã€å”å”é€€å›ã€‘çš„éƒ¨åˆ†
                    new_remark = re.sub(r'ï¼›*ã€ä¸»ä»»é€€å›ã€‘[^ï¼›]*', '', current_remark)
                    new_remark = re.sub(r'ï¼›*ã€å”å”é€€å›ã€‘[^ï¼›]*', '', new_remark)
                    new_remark = new_remark.strip('ï¼›').strip()
                    
                    df.loc[mask, 'å‚™è¨»'] = new_remark
                
                updated_count += 1
        
        # å„²å­˜ CSV
        df = df.fillna('')  # â­ å¯«å…¥å‰ç¢ºä¿æ²’æœ‰ nan
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        # æ§‹å»ºè¨Šæ¯
        msg_parts = []
        if approve_director:
            msg_parts.append('ä¸»ä»»ç°½æ ¸')
        if approve_uncle:
            msg_parts.append('å”å”ç°½æ ¸')
        msg = f'æˆåŠŸç¢ºèª {updated_count} ç­†è³‡æ–™çš„ {" å’Œ ".join(msg_parts)}'
        
        return jsonify({
            'status': 'success',
            'message': msg,
            'updated_count': updated_count
        })
        
    except Exception as e:
        logger.info(f"ç¢ºèªè³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/reject-items', methods=['POST'])
def reject_items():
    """æ‰¹æ¬¡é€€å›è³‡æ–™ï¼ˆæ”¯æ´æŒ‡å®šé€€å›éšæ®µï¼šä¸»ä»»ç°½æ ¸æˆ–å”å”ç°½æ ¸ï¼‰"""
    try:
        data = request.json
        item_ids = data.get('item_ids', [])
        reject_reason = data.get('reject_reason', 'é•·å®˜é€€å›')
        reject_stage = data.get('reject_stage', 'director')  # 'director' æˆ– 'uncle'
        
        if not item_ids:
            return jsonify({
                'status': 'error',
                'message': 'æœªæä¾›è¦é€€å›çš„è³‡æ–™ ID'
            }), 400
        
        # è®€å– CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # â­ é¿å… nan å•é¡Œ
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
        if 'å‚™è¨»' not in df.columns:
            df['å‚™è¨»'] = ''
        
        # æ±ºå®šé€€å›æ¨™ç±¤
        stage_label = 'ä¸»ä»»' if reject_stage == 'director' else 'å”å”'
        stage_column = 'ä¸»ä»»ç°½æ ¸' if reject_stage == 'director' else 'å”å”ç°½æ ¸'
        
        # æ›´æ–°æŒ‡å®š ID çš„å‚™è¨»æ¬„ä½å’Œç°½æ ¸ç‹€æ…‹
        updated_count = 0
        for item_id in item_ids:
            mask = df['Id'].astype(str).str.strip() == str(item_id).strip()
            if mask.any():
                # è™•ç†å‚™è¨»
                current_remark = df.loc[mask, 'å‚™è¨»'].values[0]
                current_remark = '' if (pd.isna(current_remark) or 
                                       str(current_remark).strip() == '' or 
                                       str(current_remark) == 'nan') else str(current_remark)
                
                # åœ¨å‚™è¨»ä¸­åŠ å…¥é€€å›åŸå› ï¼ˆåŒ…å«æ˜¯å“ªå€‹éšæ®µé€€å›ï¼‰
                new_remark = f"{current_remark}ï¼›ã€{stage_label}é€€å›ã€‘{reject_reason}" if current_remark else f"ã€{stage_label}é€€å›ã€‘{reject_reason}"
                df.loc[mask, 'å‚™è¨»'] = new_remark
                
                # è¨­å®šå°æ‡‰ç°½æ ¸æ¬„ä½ç‚ºé€€å›æ¨™è¨˜
                df.loc[mask, stage_column] = 'R'
                
                updated_count += 1
        
        # å„²å­˜ CSV
        df = df.fillna('')  # â­ å¯«å…¥å‰ç¢ºä¿æ²’æœ‰ nan
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        return jsonify({
            'status': 'success',
            'message': f'æˆåŠŸé€€å› {updated_count} ç­†è³‡æ–™ï¼ˆ{stage_label}ç°½æ ¸é€€å›ï¼‰',
            'updated_count': updated_count
        })
        
    except Exception as e:
        logger.info(f"é€€å›è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/resubmit-items', methods=['POST'])
def resubmit_items():
    """é‡æ–°æäº¤ï¼ˆå°‡é€€å›çš„è³‡æ–™æ”¹ç‚ºå¾…å¯©æ ¸ï¼Œé‡ç½®å…©å€‹ç°½æ ¸ç‹€æ…‹ï¼‰"""
    try:
        data = request.json
        item_ids = data.get('item_ids', [])
        
        if not item_ids:
            return jsonify({
                'status': 'error',
                'message': 'æœªæä¾›è¦é‡æ–°æäº¤çš„è³‡æ–™ ID'
            }), 400
        
        # è®€å– CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # â­ é¿å… nan å•é¡Œ
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
        
        # æ›´æ–°æŒ‡å®š ID çš„ç°½æ ¸ç‹€æ…‹
        updated_count = 0
        for item_id in item_ids:
            mask = df['Id'].astype(str).str.strip() == str(item_id).strip()
            if mask.any():
                # å°‡æ‰€æœ‰ç°½æ ¸ç‹€æ…‹æ”¹å›å¾…å¯©æ ¸
                df.loc[mask, 'ä¸»ä»»ç°½æ ¸'] = 'X'
                df.loc[mask, 'å”å”ç°½æ ¸'] = 'X'
                updated_count += 1
        
        # å„²å­˜ CSV
        df = df.fillna('')  # â­ å¯«å…¥å‰ç¢ºä¿æ²’æœ‰ nan
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        return jsonify({
            'status': 'success',
            'message': f'æˆåŠŸé‡æ–°æäº¤ {updated_count} ç­†è³‡æ–™',
            'updated_count': updated_count
        })
        
    except Exception as e:
        logger.info(f"é‡æ–°æäº¤å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/get-all-items-with-approval', methods=['GET'])
def get_all_items_with_approval():
    """å–å¾—æ‰€æœ‰è³‡æ–™ï¼ˆè‡ªå‹•æ›´æ–°æœ‰ ePR No. çš„ç‚ºå·²ç¢ºèªï¼Œç¢ºä¿é›™ç°½æ ¸æ¬„ä½å­˜åœ¨ï¼‰"""
    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')
        
        # ç¢ºä¿ç°½æ ¸æ¬„ä½å­˜åœ¨
        columns_added = False
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
            columns_added = True
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
            columns_added = True
        
        # â­ è‡ªå‹•æ›´æ–°ï¼šæœ‰ ePR No. çš„è³‡æ–™ï¼Œå…©å€‹ç°½æ ¸éƒ½æ”¹ç‚º V
        if 'ePR No.' in df.columns:
            has_epr = df['ePR No.'].astype(str).str.strip() != ''
            
            # æ›´æ–°ä¸»ä»»ç°½æ ¸
            director_not_approved = df['ä¸»ä»»ç°½æ ¸'].astype(str).str.strip() != 'V'
            need_update_director = has_epr & director_not_approved
            if need_update_director.any():
                df.loc[need_update_director, 'ä¸»ä»»ç°½æ ¸'] = 'V'
                columns_added = True
            
            # æ›´æ–°å”å”ç°½æ ¸
            uncle_not_approved = df['å”å”ç°½æ ¸'].astype(str).str.strip() != 'V'
            need_update_uncle = has_epr & uncle_not_approved
            if need_update_uncle.any():
                df.loc[need_update_uncle, 'å”å”ç°½æ ¸'] = 'V'
                columns_added = True
            
            if need_update_director.any() or need_update_uncle.any():
                updated_count = max(need_update_director.sum(), need_update_uncle.sum())
                logger.info(f"âœ… è‡ªå‹•æ›´æ–° {updated_count} ç­†å·²é–‹å–®è³‡æ–™ç‚ºå·²ç¢ºèª")
        
        # å¦‚æœæœ‰æ–°å¢æ¬„ä½æˆ–æ›´æ–°ï¼Œå„²å­˜ CSV
        if columns_added:
            df = df.fillna('')
            df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
            logger.info("âœ… å·²æ–°å¢/æ›´æ–°ä¸»ä»»ç°½æ ¸å’Œå”å”ç°½æ ¸æ¬„ä½")
        
        # è½‰æ›ç‚º dict list
        items_list = []
        for _, row in df.iterrows():
            item_dict = {}
            for col in df.columns:
                val = row[col]
                if pd.isna(val):
                    item_dict[col] = ''
                else:
                    item_dict[col] = str(val)
            items_list.append(item_dict)
        
        return jsonify({
            'status': 'success',
            'items': items_list,
            'count': len(items_list)
        })
        
    except Exception as e:
        logger.info(f"å–å¾—è³‡æ–™å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


@app.route('/api/clear-remark-and-approve', methods=['POST'])
def clear_remark_and_approve():
    """æ¸…ç©ºé€€å›åŸå› ï¼ˆä¿ç•™åŸæœ¬å‚™è¨»ï¼‰ä¸¦æ ¹æ“šé€€å›éšæ®µè¨­å®šç°½æ ¸ç‹€æ…‹"""
    try:
        data = request.json
        item_id = data.get('item_id')
        reject_stage = data.get('reject_stage', 'unknown')  # 'director' æˆ– 'uncle'
        
        if not item_id:
            return jsonify({
                'status': 'error',
                'message': 'æœªæä¾›è³‡æ–™ ID'
            }), 400
        
        # è®€å– CSV
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna('')  # é¿å… nan å•é¡Œ
        
        # â­ ç§»é™¤èˆŠçš„ã€Œé•·å®˜ç¢ºèªã€æ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'é•·å®˜ç¢ºèª' in df.columns:
            df = df.drop(columns=['é•·å®˜ç¢ºèª'])
            logger.info("âœ… å·²ç§»é™¤èˆŠçš„ã€Œé•·å®˜ç¢ºèªã€æ¬„ä½")
        
        # ç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½å­˜åœ¨
        if 'å‚™è¨»' not in df.columns:
            df['å‚™è¨»'] = ''
        if 'ä¸»ä»»ç°½æ ¸' not in df.columns:
            df['ä¸»ä»»ç°½æ ¸'] = 'X'
        if 'å”å”ç°½æ ¸' not in df.columns:
            df['å”å”ç°½æ ¸'] = 'X'
        
        # æ‰¾åˆ°å°æ‡‰çš„è³‡æ–™
        mask = df['Id'].astype(str).str.strip() == str(item_id).strip()
        
        if not mask.any():
            return jsonify({
                'status': 'error',
                'message': f'æ‰¾ä¸åˆ° ID ç‚º {item_id} çš„è³‡æ–™'
            }), 404
        
        # â­ åªç§»é™¤ã€ä¸»ä»»é€€å›ã€‘å’Œã€å”å”é€€å›ã€‘çš„éƒ¨åˆ†ï¼Œä¿ç•™åŸæœ¬å‚™è¨»
        current_remark = df.loc[mask, 'å‚™è¨»'].values[0]
        current_remark = '' if (pd.isna(current_remark) or 
                               str(current_remark).strip() == '' or 
                               str(current_remark) == 'nan') else str(current_remark)
        
        # ç§»é™¤é€€å›åŸå› ï¼ˆä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼‰
        new_remark = re.sub(r'ï¼›*ã€ä¸»ä»»é€€å›ã€‘[^ï¼›]*', '', current_remark)
        new_remark = re.sub(r'ï¼›*ã€å”å”é€€å›ã€‘[^ï¼›]*', '', new_remark)
        new_remark = new_remark.strip('ï¼›').strip()
        
        df.loc[mask, 'å‚™è¨»'] = new_remark
        
        # æ ¹æ“šé€€å›éšæ®µè¨­å®šç°½æ ¸ç‹€æ…‹
        if reject_stage == 'director':
            # ä¸»ä»»é€€å› â†’ è™•ç†å®Œæˆå¾Œï¼šä¸»ä»»æ”¹ç‚º Vï¼Œå”å”ç¶­æŒ Xï¼ˆç­‰å¾…å”å”ç°½æ ¸ï¼‰
            df.loc[mask, 'ä¸»ä»»ç°½æ ¸'] = 'V'
            df.loc[mask, 'å”å”ç°½æ ¸'] = 'X'
            message = 'é€€å›åŸå› å·²æ¸…é™¤ï¼Œä¸»ä»»ç°½æ ¸å·²é€šéï¼Œè«‹å”å”ç¹¼çºŒç°½æ ¸'
            logger.info(f"âœ… ID {item_id}: ä¸»ä»»é€€å›è™•ç†å®Œæˆ â†’ ä¸»ä»»V, å”å”X")
        elif reject_stage == 'uncle':
            # å”å”é€€å› â†’ è™•ç†å®Œæˆå¾Œï¼šä¸»ä»»ç¶­æŒ Vï¼Œå”å”æ”¹ç‚º Vï¼ˆç°½æ ¸å®Œæˆï¼‰
            df.loc[mask, 'ä¸»ä»»ç°½æ ¸'] = 'V'
            df.loc[mask, 'å”å”ç°½æ ¸'] = 'V'
            message = 'é€€å›åŸå› å·²æ¸…é™¤ï¼Œç°½æ ¸æµç¨‹å·²å®Œæˆ'
            logger.info(f"âœ… ID {item_id}: å”å”é€€å›è™•ç†å®Œæˆ â†’ ä¸»ä»»V, å”å”V")
        else:
            # æœªçŸ¥ç‹€æ…‹ï¼Œåªæ¸…ç©ºé€€å›åŸå› 
            message = 'é€€å›åŸå› å·²æ¸…é™¤'
            logger.info(f"âœ… ID {item_id}: æœªçŸ¥é€€å›éšæ®µï¼Œåƒ…æ¸…é™¤é€€å›åŸå› ")
        
        # å„²å­˜ CSV
        df = df.fillna('')  # å¯«å…¥å‰ç¢ºä¿æ²’æœ‰ nan
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        
        return jsonify({
            'status': 'success',
            'message': message,
            'item_id': item_id,
            'reject_stage': reject_stage,
            'remaining_remark': new_remark  # å›å‚³ä¿ç•™çš„å‚™è¨»å…§å®¹
        })
        
    except Exception as e:
        logger.info(f"âŒ è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500


if __name__ == "__main__":
    app.run(debug=True)
