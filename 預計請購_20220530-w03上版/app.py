import datetime
import json
import os
import re
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
from ldap3 import Server, Connection, ALL, NTLM # type: ignore
from ldap3.core.exceptions import LDAPException, LDAPBindError # type: ignore
import uuid
from filelock import FileLock
from werkzeug.utils import secure_filename
import logging
import shutil
import numpy as np
import traceback

BACKEND_DATA = r"D:\Data\Backend_Access_Management\Backend_data.json"
VENDER_FILE_PATH = f'static/data/vender.ini'

app = Flask(__name__)
CORS(app)
CSV_FILE = "static/data/Planned_Purchase_Request_List.csv"
JSON_FILE = f"static/data/money.json"
BUYER_FILE = f"static/data/Buyer_detail_updated.csv"

# å‚™ä»½
def backup_files():
    """å‚™ä»½ä¸»è¦ CSV æª”æ¡ˆ"""
    try:
        backup_dir = "static/backup"
        os.makedirs(backup_dir, exist_ok=True)

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S") # type: ignore

        files_to_backup = {
            "Planned_Purchase_Request_List": CSV_FILE,
            "Buyer_detail_updated": BUYER_FILE
        }

        backed_up = []
        for name, path in files_to_backup.items():
            if os.path.exists(path):
                filename = f"{name}_backup_{timestamp}.csv"
                dest_path = os.path.join(backup_dir, filename)
                shutil.copyfile(path, dest_path)
                backed_up.append(dest_path)

        return {"status": "success", "files": backed_up}
    except Exception as e:
        return {"status": "error", "message": str(e)}
    

def read_json_file():
    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå­—å…¸


def clean_value(val):
    try:
        if isinstance(val, str) and val.endswith(".0"):
            return str(int(float(val)))
        return str(val) if val is not None else ""
    except Exception:
        return str(val)


def authenticate_user(username, password):
    try:
        # server = Server('ldap://KHADDC02.kh.asegroup.com', get_info = ALL)
        # ä½¿ç”¨ NTLM
        user = f'kh\\{username}'
        password = f'{password}'

        # print("å¸³è™Ÿ: ", username, " å¯†ç¢¼: ", password)
        # å»ºç«‹é€£æ¥
        # conn = Connection(server, user = user, password = password, authentication = NTLM)

        # å˜—è©¦ç¶å®š
        # if conn.bind():
            # app.logger.info(f"User {username} login successful.")
        return True
        # else:
        #     # app.logger.warning(f"Login failed for user {username}: {conn.last_error}")
        #     return False
    except Exception as e:
        # app.logger.error(f"Error during authentication for user {username}: {e}")
        return False


@app.route("/api/getAllLoginer", methods=["POST"])
def getAllLoginer():
    try:
        data = request.get_json()
        user_id = data.get("username")
        print(user_id)

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            items = json.load(f)

        for item in items:
            if str(item.get("å·¥è™Ÿ", "")).strip() == user_id:
                return jsonify({"name": "Username Find"})
            
        return jsonify({"error": "Item not found"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/login', methods=['POST'])
def login():
    data = request.json
    if not data:
        return jsonify({'message': 'æœªæä¾›ç™»å…¥è³‡è¨Š'}), 400
    
    username = data.get('username', '').strip()
    password = data.get('password', '').strip()
    
    if authenticate_user(username, password):
        return jsonify({'message': 'ç™»å…¥æˆåŠŸ'})
    return jsonify({'message': 'å¸³è™Ÿæˆ–å¯†ç¢¼éŒ¯èª¤'}), 401


# è¨­å®šå„²å­˜ç¯©é¸ç‹€æ…‹çš„è³‡æ–™å¤¾
FILTERS_DIR = 'user_filters'
if not os.path.exists(FILTERS_DIR):
    os.makedirs(FILTERS_DIR)

@app.route('/api/save-filters-json', methods=['POST'])
def save_filters_json():
    try:
        data = request.json
        username = data.get('username') # type: ignore
        
        if not username:
            return jsonify({'status': 'error', 'message': 'ç¼ºå°‘ä½¿ç”¨è€…åç¨±'}), 400
        
        # å„²å­˜åˆ° JSON æª”æ¡ˆ
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # å¯«å…¥æª”æ¡ˆ
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return jsonify({'status': 'success', 'message': 'ç¯©é¸ç‹€æ…‹å·²å„²å­˜'})
        
    except Exception as e:
        print(f"å„²å­˜ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/get-filters-json/<username>', methods=['GET'])
def get_filters_json(username):
    try:
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                filters = json.load(f)
            return jsonify(filters)
        else:
            return jsonify({'message': 'æ‰¾ä¸åˆ°ç¯©é¸è¨­å®š'}), 404
            
    except Exception as e:
        print(f"è¼‰å…¥ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/api/clear-filters-json/<username>', methods=['DELETE'])
def clear_filters_json(username):
    try:
        filename = os.path.join(FILTERS_DIR, f'{username}_filters.json')
        
        if os.path.exists(filename):
            os.remove(filename)
            return jsonify({'status': 'success', 'message': 'ç¯©é¸ç‹€æ…‹å·²æ¸…é™¤'})
        else:
            return jsonify({'status': 'success', 'message': 'æ²’æœ‰éœ€è¦æ¸…é™¤çš„ç¯©é¸ç‹€æ…‹'})
            
    except Exception as e:
        print(f"æ¸…é™¤ç¯©é¸ç‹€æ…‹éŒ¯èª¤: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500


def update_verification_status_and_po_numbers():
    """
    æ›´æ–°é©—æ”¶ç‹€æ…‹å’ŒPO No.æ¬„ä½
    1. æ ¹æ“šBuyer_detail.csvä¸­åŒIDçš„é©—æ”¶ç‹€æ…‹ä¾†æ›´æ–°Planned_Purchase_Request_List.csvçš„é©—æ”¶ç‹€æ…‹
    2. å°‡åŒIDä¸‹çš„PO No.çµ„åˆæˆå­—ä¸²
    3. æª¢æŸ¥Buyer_detail.csvä¸­çš„ePR No.ï¼Œå¦‚æœæœ‰å€¼å‰‡è¨­é–‹å–®ç‹€æ…‹ç‚º'V'
    """
    backup_files()
    try:
        # è®€å–å…©å€‹CSVæª”æ¡ˆ
        main_df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)  # Planned_Purchase_Request_List.csv
        detail_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)  # Buyer_detail.csv
        
        # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
        if 'é©—æ”¶ç‹€æ…‹' not in main_df.columns:
            main_df['é©—æ”¶ç‹€æ…‹'] = 'X'
        if 'PO No.' not in main_df.columns:
            main_df['PO No.'] = ''
        if 'é–‹å–®ç‹€æ…‹' not in detail_df.columns:
            detail_df['é–‹å–®ç‹€æ…‹'] = 'X'
        
        # ===== æ­¥é©Ÿ1ï¼šæ›´æ–° BUYER_FILE ä¸­çš„é–‹å–®ç‹€æ…‹ =====
        for idx, detail_row in detail_df.iterrows():
            epr_no = str(detail_row.get('ePR No.', '')).strip()
            if epr_no and epr_no.isdigit() and len(epr_no) == 10:
                detail_df.at[idx, 'é–‹å–®ç‹€æ…‹'] = 'V'
            else:
                detail_df.at[idx, 'é–‹å–®ç‹€æ…‹'] = 'X'
        
        detail_df.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
        
        # ===== æ­¥é©Ÿ2ï¼šæ›´æ–°ä¸»è¡¨ é©—æ”¶ç‹€æ…‹ å’Œ PO No. =====
        for idx, main_row in main_df.iterrows():
            main_id = str(main_row.get('Id', '')).strip()
            if not main_id:
                continue
                
            detail_records = detail_df[detail_df['Id'].astype(str).str.strip() == main_id]
            
            if detail_records.empty:
                main_df.at[idx, 'é©—æ”¶ç‹€æ…‹'] = 'X'
                main_df.at[idx, 'PO No.'] = ''
                continue
            
            # é©—æ”¶ç‹€æ…‹ï¼šåªæœ‰ç•¶æ‰€æœ‰éƒ½æ˜¯ V æ‰è¨­ V
            statuses = detail_records['é©—æ”¶ç‹€æ…‹'].fillna('X').astype(str).str.strip()
            main_df.at[idx, 'é©—æ”¶ç‹€æ…‹'] = 'V' if all(s == 'V' for s in statuses if s) else 'X'
            
            # PO No. çµ„åˆ
            po_numbers = detail_records['PO No.'].fillna('').astype(str).str.strip()
            unique_po_numbers = []
            for po in po_numbers:
                if po and po not in unique_po_numbers:
                    unique_po_numbers.append(po)
            main_df.at[idx, 'PO No.'] = '<br />'.join(unique_po_numbers) if unique_po_numbers else ''
        
        main_df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        return True
    
    except Exception as e:
        print(f"âŒ æ›´æ–°é©—æ”¶ç‹€æ…‹å’ŒPO No.æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback; traceback.print_exc()
        return False


# ===============================
@app.route("/data")
def get_data():
    update_verification_status_and_po_numbers()
    df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)

    expected_columns = [
        "Id", "é–‹å–®ç‹€æ…‹", "WBS", "è«‹è³¼é †åº", "éœ€æ±‚è€…", "è«‹è³¼é …ç›®", "éœ€æ±‚åŸå› ",
        "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No.", "é€²åº¦è¿½è¹¤è¶…é€£çµ", "å‚™è¨»",
        "Status", "ç°½æ ¸ä¸­é—œå¡", "å ±å‘Šè·¯å¾‘", "é©—æ”¶è·¯å¾‘", "åˆä½œé¡åˆ¥", "åˆä½œå» å•†", 
        "å‰è³¼å–®å–®è™Ÿ", "é©—æ”¶ç‹€æ…‹", "PO No."
    ]
    for col in expected_columns:
        if col not in df.columns:
            df[col] = ""

    if "Id" not in df.columns:
        df.insert(0, "Id", [str(uuid.uuid4()) for _ in range(len(df))])
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
    else:
        df["Id"] = df["Id"].fillna("").astype(str).str.strip()
        missing_ids = df["Id"] == ""
        df.loc[missing_ids, "Id"] = [str(uuid.uuid4()) for _ in range(missing_ids.sum())]
        if missing_ids.any():
            df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

    df["ePR No."] = df["ePR No."].apply(lambda x: "" if pd.isna(x) or x == 0 else str(int(float(x))))
    df["ç¸½é‡‘é¡"] = pd.to_numeric(df["ç¸½é‡‘é¡"], errors="coerce").fillna(0)
    df["ç¸½é‡‘é¡"] = df["ç¸½é‡‘é¡"].apply(lambda x: "" if x == 0 else int(x))

    numeric_columns = ["è«‹è³¼é †åº", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "å‰è³¼å–®å–®è™Ÿ", "é©—æ”¶ç‹€æ…‹"]
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].apply(
                lambda x: "" if pd.isna(x) or x == 0 else str(int(float(x)))
                if str(x).replace('.','').isdigit() else str(x)
            )
    
    df = df.fillna("")
    return jsonify(df.to_dict(orient="records"))


@app.route('/api/unordered-count')
def get_unordered_count():
    
    if not os.path.exists(CSV_FILE) or os.path.getsize(CSV_FILE) == 0:
        return jsonify({
            "count_X": 0,
            "count_V": 0,
            "monthly_expenses": {}
        })

    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df = df.fillna("")

        count_X = df[df["é–‹å–®ç‹€æ…‹"] != "V"].shape[0]
        count_V = df[df["é–‹å–®ç‹€æ…‹"] == "V"].shape[0]

        df_unordered = df[df["é–‹å–®ç‹€æ…‹"] != "V"].copy()

        # âœ… åŠ é€™ä¸€è¡Œï¼Œéæ¿¾æ‰éæ³•çš„éœ€æ±‚æ—¥
        df_unordered["éœ€æ±‚æ—¥"] = df_unordered["éœ€æ±‚æ—¥"].astype(str).str.replace("/", "", regex=False)
        df_unordered = df_unordered[df_unordered["éœ€æ±‚æ—¥"].str.match(r'^\d{8}$')].copy()

        df_unordered["ç¸½é‡‘é¡"] = pd.to_numeric(df_unordered["ç¸½é‡‘é¡"], errors="coerce").fillna(0)
        df_unordered["éœ€æ±‚æ—¥"] = df_unordered["éœ€æ±‚æ—¥"].astype(str)
        df_unordered["æœˆä»½"] = df_unordered["éœ€æ±‚æ—¥"].str.slice(0, 6)

        monthly_sums = df_unordered.groupby("æœˆä»½")["ç¸½é‡‘é¡"].sum().sort_index()
        monthly_expenses = {month: int(round(amount, 2)) for month, amount in monthly_sums.items()}

        print(monthly_expenses)

        return jsonify({
            "count_X": count_X,
            "count_V": count_V,
            "monthly_expenses": monthly_expenses
        })

    except Exception as e:
        traceback.print_exc()  
        return jsonify({
            "count_X": 0,
            "count_V": 0,
            "monthly_expenses": {},
            "error": str(e)
        }), 500


@app.route('/api/getrestofmoney', methods = ['GET'])
def getrestofmoney():
    import datetime, re, math

    budget = read_json_file()
    current_date = datetime.datetime.now()
    current_year = str(current_date.year)
    month_no_pad = str(current_date.month)       # '8'
    month_pad = str(current_date.month).zfill(2) # '08'
    yyyymm = current_year + month_pad

    # å˜—è©¦å…©ç¨® key
    data = budget.get("é ç®—", {}).get(current_year, {}).get(month_no_pad, {})
    if not data:
        data = budget.get("é ç®—", {}).get(current_year, {}).get(month_pad, {})

    current_budget = float(data.get('ç•¶æœˆè«‹è³¼é ç®—', 0))
    additional_budget = float(data.get('ç•¶æœˆè¿½åŠ é ç®—', 0))

    try:
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
    except:
        df = pd.DataFrame()

    total_money = 0.0
    if not df.empty:
        for _, row in df.iterrows():
            status = str(row.get('é–‹å–®ç‹€æ…‹', '')).strip()
            raw_date = str(row.get('å·²é–‹å–®æ—¥æœŸ', '')).strip()
            cleaned_date = ''.join([c for c in raw_date if c.isdigit()])[:8]
            wbs = str(row.get('WBS', '')).strip()

            raw_amount = str(row.get('ç¸½é‡‘é¡', '')).replace(',', '').strip()
            try:
                amount = float(raw_amount) if raw_amount and raw_amount.lower() != 'nan' else 0.0
            except:
                amount = 0.0
            if math.isnan(amount):
                amount = 0.0

            monthKey = cleaned_date[:6]
            matchMonth = len(cleaned_date) == 8 and monthKey == yyyymm

            if status == 'V' and matchMonth and not re.match(r'^\d{2}FT0A\d{4}$', wbs):
                total_money += amount
                # print(f"[DEBUG] åŠ ç¸½ -> ç‹€æ…‹={status}, æ—¥æœŸ={cleaned_date}, æœˆä»½={monthKey}, é‡‘é¡={amount}, WBS={wbs}")

    rest_money = (current_budget + additional_budget) - total_money
    # print(f"[DEBUG] ç•¶æœˆé ç®—={current_budget}, è¿½åŠ ={additional_budget}, å·²é–‹å–®={total_money}, å‰©é¤˜={rest_money}")
    return jsonify({
        'ç•¶æœˆè«‹è³¼é ç®—': int(current_budget),
        'ç•¶æœˆè¿½åŠ é ç®—': int(additional_budget),
        'å·²é–‹å–®ç¸½é¡': int(total_money),
        'å‰©é¤˜é‡‘é¡': int(rest_money)
    })




@app.route('/api/budget_months', methods=['GET'])
def get_budget_months():
    """
    ç²å–æ‰€æœ‰å¯ç”¨çš„é ç®—æœˆä»½é¸é …å’Œå°æ‡‰çš„é ç®—é‡‘é¡
    """
    try:
        budget = read_json_file()
        # print(f"ğŸ“Š è®€å–é ç®—è³‡æ–™: {budget}")
        
        budget_list = []
        
        # å¾é ç®—è³‡æ–™ä¸­æå–æ‰€æœ‰å¹´ä»½å’Œæœˆä»½ï¼Œä¸¦è¨ˆç®—é ç®—
        budget_data = budget.get("é ç®—", {})
        
        for year in budget_data.keys():
            year_data = budget_data[year]
            for month in year_data.keys():
                # æ ¼å¼åŒ–ç‚º YYYYMM æ ¼å¼
                month_padded = str(month).zfill(2)
                year_month = f"{year}{month_padded}"
                
                # ç²å–è©²æœˆä»½çš„é ç®—è³‡æ–™
                month_budget_data = year_data[month]
                current_budget = month_budget_data.get('ç•¶æœˆè«‹è³¼é ç®—', 0)
                additional_budget = month_budget_data.get('ç•¶æœˆè¿½åŠ é ç®—', 0)
                total_budget = current_budget + additional_budget
                
                # å»ºç«‹æœˆä»½å’Œé ç®—çš„å°æ‡‰è³‡æ–™
                budget_info = {
                    'month': year_month,
                    'money': total_budget,
                    'ç•¶æœˆè«‹è³¼é ç®—': current_budget,
                    'ç•¶æœˆè¿½åŠ é ç®—': additional_budget
                }
                
                budget_list.append(budget_info)
                # print(f"ğŸ’° {year_month}: è«‹è³¼={current_budget:,}, è¿½åŠ ={additional_budget:,}, ç¸½è¨ˆ={total_budget:,}")
        
        # æŒ‰æœˆä»½æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        budget_list.sort(key=lambda x: x['month'], reverse=True)
        
        # print(f"ğŸ“… å®Œæ•´é ç®—æ¸…å–®: {budget_list}")
        
        return jsonify({
            'success': True,
            'budget_list': budget_list,
            'count': len(budget_list)
        })
        
    except Exception as e:
        print(f"âŒ ç²å–é ç®—æœˆä»½éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
    

@app.route('/api/uploadMoney', methods=['POST'])
def uploadMoney():
    try:
        data = request.get_json()
        year = data.get('currentYear')
        month = data.get('currentMonth')
        current_budget = data.get('currentBudget')
        additional_budget = data.get('additionalBudget')

        budget_data = read_json_file()  # è®€åŸå§‹ JSON è³‡æ–™
        print("budget_data: ", budget_data)

        # æª¢æŸ¥å¹´åº¦
        if "é ç®—" not in budget_data:
            budget_data["é ç®—"] = {}

        if str(year) not in budget_data["é ç®—"]:
            budget_data["é ç®—"][str(year)] = {}

        # æ›´æ–°è©²æœˆä»½è³‡æ–™
        budget_data["é ç®—"][str(year)][str(month)] = {
            "ç•¶æœˆè«‹è³¼é ç®—": current_budget,
            "ç•¶æœˆè¿½åŠ é ç®—": additional_budget
        }

        # âœ… æ­£ç¢ºå¯«å…¥æ›´æ–°å¾Œçš„è³‡æ–™
        with open(JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(budget_data, f, ensure_ascii=False, indent=4)

        return jsonify({'message': 'è³‡æ–™æäº¤æˆåŠŸ'}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 400


@app.route('/api/requesters', methods=['GET'])
def get_requesters():
    try:
        with open("config.cfg", "r", encoding="utf-8-sig") as f:
            lines = f.readlines()
            names = [line.strip() for line in lines if line.strip()]  
            # print("requesters: ", names)
            return jsonify(names)
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@app.route('/api/admins', methods=['GET'])
def get_admins():
    try:
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            data = json.load(f)
            seen = set()
            names = []
            for entry in data:
                if entry["è«‹è³¼ç¶²é å¾Œå°"] == 'O':
                    name = entry["å·¥è™Ÿ"]
                    if name not in seen:
                        seen.add(name)
                        names.append(name)
            # print("admins: ", names)
            return jsonify(names)
    except Exception as e:
        return jsonify({"error": str(e)}), 500



@app.route("/api/add", methods = ['POST'])
def add_new_item():
    try:
        # å‚™ä»½
        backup_files()
        data = request.get_json()
        main_data = {k: v for k, v in data.items() if k != 'tableRows'}
        table_rows = data.get('tableRows', [])

        print("ä¸»è³‡æ–™ï¼š", main_data)
        
        if not data:
            return jsonify({'status': 'error', 'message': 'ç„¡æ•ˆçš„ JSON è«‹æ±‚'}), 400

        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)

        # ç¢ºä¿æ‰€æœ‰æ¬„ä½ä¸€è‡´
        # new_row = {col: data.get(col, "") for col in df.columns}

        new_row = {col: clean_value(main_data.get(col, "")) for col in df.columns}

        if isinstance(new_row["éœ€æ±‚æ—¥"], str) and "/" in new_row["éœ€æ±‚æ—¥"]:
            new_row['éœ€æ±‚æ—¥'] = new_row["éœ€æ±‚æ—¥"].replace("/", "")

        if isinstance(new_row["å·²é–‹å–®æ—¥æœŸ"], str) and "/" in new_row["å·²é–‹å–®æ—¥æœŸ"]:
            new_row['å·²é–‹å–®æ—¥æœŸ'] = new_row["å·²é–‹å–®æ—¥æœŸ"].replace("/", "")

        def safe_float_to_int64(value):
            try:
                return str(int(float(value)))
            except (ValueError, TypeError, OverflowError):
                return ""

        # âœ¨ è¦è½‰æ›çš„æ¬„ä½ï¼ˆé¿å… int32 æº¢ä½ï¼‰
        numeric_fields = ["è«‹è³¼é †åº", "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No."]
        for field in numeric_fields:
            if field in new_row:
                new_row[field] = safe_float_to_int64(new_row[field])

        new_row["Id"] = str(uuid.uuid4())  # è‡ªå‹•ç”¢ç”Ÿå”¯ä¸€ ID

        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        print(new_row)
        # å„²å­˜å› CSV
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

        # å¯«å…¥å¦ä¸€å¼µè¡¨
        print("è¡¨æ ¼åˆ—ï¼š", table_rows)
        DETAIL_CSV_FILE = "static/data/Buyer_detail_updated.csv"  # å»ºè­°å­˜å¦ä¸€ä»½ CSV æª”

        detail_columns = [
            "Id", "é–‹å–®ç‹€æ…‹", "äº¤è²¨é©—è­‰", "User", "ePR No.", "PO No.",
            "Item", "å“é …", "è¦æ ¼", "æ•¸é‡", "ç¸½æ•¸", "å–®åƒ¹", "ç¸½åƒ¹", "å‚™è¨»", "å­—æ•¸",
            "isEditing", "backup", "_alertedItemLimit", "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ",
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "ç™¼ç¥¨æœˆä»½", "WBS", "éœ€æ±‚æ—¥", "RTé‡‘é¡", "RTç¸½é‡‘é¡","é©—æ”¶ç‹€æ…‹"
        ]

        # è™•ç†å¾Œçš„è³‡æ–™åˆ—ï¼ˆæ¯ç­† row è£œä¸Šä¸»è³‡æ–™ IDï¼Œæ¬„ä½é †åºçµ±ä¸€ï¼‰
        cleaned_rows = []
        for row in table_rows:
            cleaned_row = {"Id": new_row["Id"]}  # ä¸»è¡¨ Id
            for col in detail_columns[1:-2]:  # ä¸å« isEditing, backupï¼Œé€™å…©å€‹æ‰‹å‹•è£œ
                cleaned_row[col] = row.get(col, "")
            cleaned_row["isEditing"] = "False"
            cleaned_row["backup"] = "{}"
            cleaned_rows.append(cleaned_row)
            
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨å°± appendï¼Œå¦å‰‡å»ºç«‹æ–°æª”
        if os.path.exists(DETAIL_CSV_FILE):
            df_detail = pd.read_csv(DETAIL_CSV_FILE, encoding="utf-8-sig", dtype=str)
            df_detail = pd.concat([df_detail, pd.DataFrame(cleaned_rows)], ignore_index=True)
        else:
            df_detail = pd.DataFrame(cleaned_rows, columns=detail_columns)

        # å¯«å…¥ CSV
        df_detail.to_csv(DETAIL_CSV_FILE, index=False, columns=detail_columns, encoding="utf-8-sig")

        return jsonify({'status': 'success', 'message': 'è³‡æ–™æ–°å¢æˆåŠŸ'}), 200

    except Exception as e:
        print("æ–°å¢éŒ¯èª¤ï¼š", e)
        return jsonify({'status': 'error', 'message': str(e)}), 500
    



@app.route('/update', methods=['POST'])
def update_data():
    # å‚™ä»½
    backup_files()
    new_data = request.json
    if not isinstance(new_data, dict):
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    print(new_data)
    main_data = {k: v for k, v in new_data.items() if k != "tableRows"} # type: ignore
    detail_data = new_data.get("tableRows", []) # type: ignore
    
    if main_data is None:
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    
    target_id = str(main_data.get("Id", "")).strip()
    if not target_id:
        return jsonify({"message": "ç¼ºå°‘ Id"}), 400
    
    try:
        df = pd.read_csv(CSV_FILE, dtype=str)
        df.fillna("", inplace=True)

        # æ‰¾åˆ°å°æ‡‰åˆ—
        df["Id"] = df["Id"].astype(str).str.strip()
        match_idx = df.index[df["Id"] == target_id].tolist()
        if not match_idx:
            return jsonify({"message": "æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™"}), 404
        idx = match_idx[0]

        # è™•ç†æ—¥æœŸæ¬„ä½æ ¼å¼
        if isinstance(new_data.get("éœ€æ±‚æ—¥"), str):
            main_data["éœ€æ±‚æ—¥"] = main_data["éœ€æ±‚æ—¥"].replace("/", "")
        if isinstance(new_data.get("å·²é–‹å–®æ—¥æœŸ"), str):
            main_data["å·²é–‹å–®æ—¥æœŸ"] = main_data["å·²é–‹å–®æ—¥æœŸ"].replace("/", "")

        # æ›´æ–°æ¬„ä½
        for col in df.columns:
            df.at[idx, col] = str(main_data.get(col, ""))

            # æ¸…æ´—è³‡æ–™ï¼ˆåŒ…å«å»é™¤ .0 èˆ‡æ–œç·šï¼‰
        for col in df.columns:
            val = main_data.get(col, "")
            val = clean_value(val)
            if col in ["éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ"] and isinstance(val, str):
                val = val.replace("/", "")
            df.at[idx, col] = val

        # è‹¥ ePR No. å­˜åœ¨ â†’ è£œé€£çµæ¬„ä½
        if main_data.get('ePR No.'):
            df.at[idx, "é€²åº¦è¿½è¹¤è¶…é€£çµ"] = f"https://khwfap.kh.asegroup.com/ePR/PRQuery/QueryPR?id={main_data['ePR No.']}"


        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
       
    
        detail_file = "static/data/Buyer_detail_updated.csv"
        if os.path.exists(detail_file):
            df_detail = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
            df_detail = df_detail[df_detail["Id"] != target_id]
        else:
            df_detail = pd.DataFrame()

        # åŠ å…¥æ–°çš„ç´°é …è³‡æ–™
        if detail_data:
            new_rows = pd.DataFrame(detail_data)
            new_rows["Id"] = target_id  # åŠ å…¥å°æ‡‰ä¸»è¡¨ Id
            df_detail = pd.concat([df_detail, new_rows], ignore_index=True)

        # å„²å­˜
        df_detail.to_csv(detail_file, index=False, encoding="utf-8-sig")
        
        return jsonify({"message": "æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        print("éŒ¯èª¤ï¼š", e)
        return jsonify({"message": "è³‡æ–™æ›´æ–°å¤±æ•—"}), 500



@app.route('/api/get-username-info', methods=['POST'])
def get_username_info():
    try:
        data = request.get_json()
        emp_id = data.get("emp_id", "").strip()  # å·¥è™Ÿ

        if not emp_id:
            return jsonify({"error": "ç¼ºå°‘å·¥è™Ÿ"}), 400

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            users = json.load(f)

        for entry in users:
            if entry.get("å·¥è™Ÿ", "").strip() == emp_id:
                return jsonify({
                    "name": entry.get("å§“å", "").strip(),
                    "ç­åˆ¥": entry.get("ç­åˆ¥", "").strip()
                })

        return jsonify({"error": "æŸ¥ç„¡æ­¤å·¥è™Ÿ"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
    
@app.route('/api/checkeEPRno', methods=['POST'])
def check_epr_no():
    try:
        epr_no = request.get_json()
        if not isinstance(epr_no, str):
            return jsonify({'error': 'Invalid ePR No. format'}), 400

        df = pd.read_csv(CSV_FILE, dtype=str)  # è®€å–æ‰€æœ‰æ¬„ä½ç‚ºå­—ä¸²é¿å…è½‰å‹å•é¡Œ
        exists = epr_no in df['ePR No.'].dropna().tolist()

        return jsonify({'exists': exists})
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return jsonify({'error': 'æª¢æŸ¥å¤±æ•—'}), 500


@app.route('/api/check-edit-permission', methods=['POST'])
def check_edit_permission():
    try:
        data = request.get_json()
        current_user = data.get("currentUser", "").strip()
        target_id = data.get("id", "").strip()

        if not current_user or not target_id:
            return jsonify({"allowed": False, "error": "ç¼ºå°‘å¿…è¦åƒæ•¸"}), 400
        
        # è®€å– admin å·¥è™Ÿåˆ—è¡¨
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)
            admin_ids = [
                entry.get("å·¥è™Ÿ", "").strip()
                for entry in backend_data
                if entry.get("è«‹è³¼ç¶²é å¾Œå°") == "O"
            ]

        is_admin = current_user in admin_ids

        # æŸ¥è©¢è©² ID å°æ‡‰çš„éœ€æ±‚è€…
        df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
        df.fillna("", inplace=True)
        df["Id"] = df["Id"].astype(str).str.strip()

        matched_row = df[df["Id"] == target_id]
        if matched_row.empty:
            return jsonify({"allowed": False, "error": "æ‰¾ä¸åˆ°å°æ‡‰è³‡æ–™"}), 404

        row = matched_row.iloc[0]
        requester = row.get("éœ€æ±‚è€…", "").strip()

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)
        
        requester_id = ""
        for entry in backend_data:
            if entry.get("å§“å", "").strip() == requester:
                requester_id = entry.get("å·¥è™Ÿ", "").strip()
                break

        print(f"ä½¿ç”¨è€…: {current_user}, å·¥è™Ÿ: {requester_id}, æ˜¯å¦ç‚º admin: ", is_admin)

        if requester_id == current_user:
            return jsonify({"allowed": True})

        return jsonify({"allowed": is_admin})

    except Exception as e:
        print("æ¬Šé™æª¢æŸ¥å¤±æ•—ï¼š", e)
        return jsonify({"allowed": False, "error": str(e)}), 500




@app.route("/api/get_detail/<id>", methods=["GET"])
def get_detail_by_id(id):
    try:
        detail_file = "static/data/Buyer_detail_updated.csv"
        if not os.path.exists(detail_file):
            return jsonify([])  # æ²’æœ‰è³‡æ–™å›å‚³ç©ºé™£åˆ—

        df = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
        df.fillna("", inplace=True)

        # ç¯©é¸å‡ºæŒ‡å®š Id çš„ç´°é …è³‡æ–™
        detail_rows = df[df["Id"] == id].to_dict(orient="records")
        return jsonify(detail_rows)
    
    except Exception as e:
        print("è®€å–ç´°é …è³‡æ–™éŒ¯èª¤ï¼š", e)
        return jsonify({"error": str(e)}), 500



@app.route("/getItemName", methods=["POST"])
def get_item_name():
    try:
        data = request.get_json()
        user_id = data.get("username")
        user_name = data.get("NeedPerson")
        print(user_id, user_name)

        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            items = json.load(f)

        for item in items:
            if str(item.get("å·¥è™Ÿ", "")).strip() == user_id:
                return jsonify({"name": item.get("å§“å", "")})
            
        return jsonify({"error": "Item not found"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/delete', methods=['POST'])
def delete_entry():
    # å‚™ä»½
    backup_files()
    data = request.json
    if data is None:
        return jsonify({"message": "è«‹æ±‚å…§å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON"}), 400
    
    target_id = str(data.get("Id", "")).strip()
    if not target_id:
        return jsonify({"status": "error", "message": "ç¼ºå°‘ Id æ¬„ä½"}), 400

    df = pd.read_csv(CSV_FILE, encoding="utf-8-sig", dtype=str)
    # å–æ¶ˆ eprnoçš„æ¬„ä½

    new_df = df[df['Id'] != target_id]

    if len(new_df) == len(df):
        return jsonify({"status": "error", "message": "æ‰¾ä¸åˆ°ç¬¦åˆæ¢ä»¶çš„è³‡æ–™"})

    new_df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

    detail_file = "static/data/Buyer_detail_updated.csv" 
    if os.path.exists(detail_file):
        df_detail = pd.read_csv(detail_file, encoding="utf-8-sig", dtype=str)
        df_detail = df_detail[df_detail["Id"] != target_id]
        df_detail.to_csv(detail_file, index=False, encoding="utf-8-sig")

    return jsonify({"status": "success", "message": "å·²æˆåŠŸåˆªé™¤"})

# é è¨ˆè«‹è³¼ æ›´æ–°ç›®å‰ç‹€æ…‹
@app.route("/api/Status-upload", methods=["POST"])
def upload():
    # å‚™ä»½
    backup_files()
    if 'file' not in request.files:
        return jsonify({'status': 'fail', 'msg': 'No file uploaded'}), 400

    file = request.files['file']
    filename = file.filename
    if filename != "download.csv":
        return jsonify({'status': 'Fail', 'filename': filename}), 400
    filename = f"status_{uuid.uuid4().hex}.csv"
    filepath = os.path.join('uploads', str(filename))
    file.save(filepath)

    lock = FileLock(f"{CSV_FILE}.lock")

    def clean_nan_value(val):
        if pd.isna(val) or str(val).strip().lower() in ['nan', 'none']:
            return ""
        return str(val).strip()
    
    with lock:
        try:

            df = pd.read_csv(filepath)
            updates = []
            for _, row in df.iterrows():
                epr_no = str(row.get('E-PRè™Ÿç¢¼           ', '')).strip()
                status = clean_nan_value(row.get('ç‹€æ…‹', ''))
                stage = clean_nan_value(row.get('ç°½æ ¸ä¸­é—œå¡', ''))

                if epr_no and epr_no != 'nan':
                    updates.append({
                        'epr_no': epr_no,
                        'ç‹€æ…‹': status,
                        'ç°½æ ¸ä¸­é—œå¡': stage
                    })
                    
            Planned_Purchase_Request_List_df = pd.read_csv(CSV_FILE)
            updated_count = 0
            no_change_count = 0

            # non_null_count = Planned_Purchase_Request_List_df['ePR No.'].notna().sum()


            for update in updates:
                epr = update['epr_no']
                new_status = update['ç‹€æ…‹']
                new_stage = update['ç°½æ ¸ä¸­é—œå¡']
            
                match = Planned_Purchase_Request_List_df['ePR No.'] == int(epr)
                if match.any():
                    idx = Planned_Purchase_Request_List_df[match].index[0]
                    
                    # å–å¾—ç›®å‰å€¼
                    old_status = clean_nan_value(Planned_Purchase_Request_List_df.at[idx, 'Status'])
                    old_stage = clean_nan_value(Planned_Purchase_Request_List_df.at[idx, 'ç°½æ ¸ä¸­é—œå¡'])
                    
                    # åªæœ‰åœ¨çœŸçš„æœ‰è®Šæ›´æ™‚æ‰æ›´æ–°
                    if old_status != new_status or old_stage != new_stage:
                        Planned_Purchase_Request_List_df.at[idx, 'Status'] = new_status
                        Planned_Purchase_Request_List_df.at[idx, 'ç°½æ ¸ä¸­é—œå¡'] = new_stage
                        updated_count += 1
                    else:
                        no_change_count += 1

                    
            def safe_float_to_int64(val):
                try:
                    if pd.isna(val):
                        return ""
                    return str(int(float(val)))
                except:
                    return str(val).strip()
                
            numeric_fields = ["è«‹è³¼é †åº", "ç¸½é‡‘é¡", "éœ€æ±‚æ—¥", "å·²é–‹å–®æ—¥æœŸ", "ePR No."]
            for field in numeric_fields:
                if field in Planned_Purchase_Request_List_df.columns:
                    Planned_Purchase_Request_List_df[field] = Planned_Purchase_Request_List_df[field].apply(safe_float_to_int64)

            # æ¸…ç†æ‰€æœ‰æ¬„ä½
            for col in Planned_Purchase_Request_List_df.columns:
                Planned_Purchase_Request_List_df[col] = Planned_Purchase_Request_List_df[col].apply(clean_nan_value)

            Planned_Purchase_Request_List_df.to_csv(CSV_FILE, index=False, encoding='utf-8-sig', na_rep="")
        except Exception as e:
            print("Exception: ", e)


    with lock:
        try:
            df_update = pd.read_csv(filepath, dtype=str)

            # æå–æ›´æ–°è³‡æ–™
            po_updates = []
            for _, row in df_update.iterrows():
                epr_no = str(row.get('E-PRè™Ÿç¢¼           ', '')).strip()
                sap_po  = str(row.get('SAP PO', '')).strip() 
                if epr_no and sap_po:
                    po_updates.append({
                        'epr_no': epr_no,
                        'po_no': sap_po
                    })
            df_detail = pd.read_csv(BUYER_FILE, dtype=str)

            if "PO No." not in df_detail.columns:
                df_detail["PO No."] = ""


            updated = 0
            nochange = 0

            for upd in po_updates:
                epr = upd['epr_no']
                new_po = upd['po_no']

                # ePR No. è½‰æˆ int â†’ str é¿å…æ ¼å¼ä¸åŒ
                try:
                    epr_str = str(int(float(epr)))
                except:
                    epr_str = str(epr).strip()

                match_idx = df_detail.index[df_detail['ePR No.'] == epr_str].tolist()

                if match_idx:
                    for idx in match_idx:
                        old_po = str(df_detail.at[idx, 'PO No.']).strip()

                        if old_po != new_po:
                            df_detail.at[idx, 'PO No.'] = new_po
                            updated += 1
                        else:
                            nochange += 1

            # å¯«å› Buyer_detail.csv
            df_detail.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
            return jsonify({'status': 'success', 'updated_count': updated_count})

        except Exception as e:
            return jsonify({'status': 'Fail', 'reason': e})




# è·¯å¾‘(å ±å‘Šè·¯å¾‘)
@app.route('/upload_report', methods=['POST'])
def upload_report():
    file = request.files.get('file')
    folder = request.form.get('folder')
    username = request.form.get('work_username')

    if not file:
        return jsonify(success=False, message='ç„¡æª”æ¡ˆ')
    if not folder:
        return jsonify(success=False, message='è³‡æ–™å¤¾åç¨±ç‚ºç©º')
    if not username:
        return jsonify(success=False, message='ç¼ºå°‘ username')

    filename = file.filename or ''
    if filename == '':
        return jsonify(success=False, message='æª”æ¡ˆåç¨±ç‚ºç©º')

    try:
        save_path = os.path.join(
            r'\\cim300\FT01_CIM\FT01_4000\11.RRç­äººå“¡-ePRè«‹è³¼ç®¡ç†',
            username,
            folder
        )
        os.makedirs(save_path, exist_ok=True)
        file.save(os.path.join(save_path, filename))
        return jsonify(success=True, message='å·²ä¸Šå‚³')
    except Exception as e:
        print('âŒ å„²å­˜éŒ¯èª¤ï¼š', e)
        return jsonify(success=False, message=str(e))



# è·¯å¾‘(å ±å‘Šè·¯å¾‘)
@app.route('/upload_acceptancereport', methods=['POST'])
def upload_acceptancereport():
    file = request.files.get('file')
    folder = request.form.get('folder')
    username = request.form.get('work_username')

    if not file:
        return jsonify(success=False, message='ç„¡æª”æ¡ˆ')
    if not folder:
        return jsonify(success=False, message='è³‡æ–™å¤¾åç¨±ç‚ºç©º')
    if not username:
        return jsonify(success=False, message='ç¼ºå°‘ username')

    filename = file.filename or ''
    if filename == '':
        return jsonify(success=False, message='æª”æ¡ˆåç¨±ç‚ºç©º')

    try:
        save_path = os.path.join(
            r'\\cim300\FT01_CIM\FT01_4000\11.RRç­äººå“¡-ePRè«‹è³¼ç®¡ç†',
            username,
            r"=å·²çµå–®=",
            folder
        )
        os.makedirs(save_path, exist_ok=True)
        file.save(os.path.join(save_path, filename))
        return jsonify(success=True, message='å·²ä¸Šå‚³')
    except Exception as e:
        print('âŒ å„²å­˜éŒ¯èª¤ï¼š', e)
        return jsonify(success=False, message=str(e))

# å–å¾—è‹±æ–‡åå­—
@app.route('/api/getUsername', methods=['POST'])
def get_username():
    try:
        data = request.get_json()
        username = data.get("username")
        print("æ”¶åˆ°éœ€æ±‚è€…åå­—ï¼š", username)


        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)

        matched = next((entry for entry in backend_data if entry["å§“å"] == username), None)
        
        if matched and "Notes_ID" in matched:
            try:
                name = matched["Notes_ID"].split("_")[0]
            except:
                name = matched["Notes_ID"].split("@")[0]

            return jsonify({"name": name})
        else:
            return jsonify({"name": "æŸ¥ç„¡ä½¿ç”¨è€…"}), 404
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/api/sendmail', methods=['POST']) 
def sendmail():
    try:
        data = request.get_json()
        with open("Backend_data.json", "r", encoding="utf-8-sig") as f:
            backend_data = json.load(f)

        mail_data = data.get("data", {})
        recipient_clean = mail_data.get("éœ€æ±‚è€…", "").strip()  
        mail_name = ''
        matched = next(
            (entry for entry in backend_data if entry.get("å§“å", "").strip() == recipient_clean),
            None
        )

        if matched and "Notes_ID" in matched:
            mail_name = matched["Notes_ID"]

        raw_cc = data.get("cc", "")  

        cc_names = [name.strip() for name in raw_cc.split(",") if name.strip()]

        cc_list = [name.replace(" ", "_") + "@aseglobal.com" for name in cc_names]

        cc_string = ",".join(cc_list)

        if data["data"]["è«‹è³¼é †åº"] == "1":
            print("æ­£åœ¨å°å…¥è¶…æ€¥ä»¶æ¨¡çµ„...")
            from MailFunction.urgent import send_mail
            print("é–‹å§‹åŸ·è¡Œè¶…æ€¥ä»¶éƒµä»¶ç™¼é€...")
            try:
                print(data["data"], '\n', data["recipient"], '\n', mail_name, '\n', cc_string)
                send_mail(data["data"], data["recipient"], mail_name, cc_string)
                print("è¶…æ€¥ä»¶éƒµä»¶ç™¼é€å®Œæˆ")
            except Exception as urgent_error:
                print(f"âŒ è¶…æ€¥ä»¶éƒµä»¶ç™¼é€éŒ¯èª¤: {str(urgent_error)}")
              
                print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                traceback.print_exc()
                raise urgent_error
        else:
            print("æ­£åœ¨å°å…¥ä¸€èˆ¬éƒµä»¶æ¨¡çµ„...")
            from MailFunction.normail import send_mail
            print("é–‹å§‹åŸ·è¡Œä¸€èˆ¬éƒµä»¶ç™¼é€...")
            try:
                print(data["data"], '\n', data["recipient"], '\n', mail_name, '\n', cc_string)
                send_mail(data["data"], data["recipient"], mail_name, cc_string)
                print("ä¸€èˆ¬éƒµä»¶ç™¼é€å®Œæˆ")
            except Exception as normal_error:
                print(f"âŒ ä¸€èˆ¬éƒµä»¶ç™¼é€éŒ¯èª¤: {str(normal_error)}")
        
                print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
                traceback.print_exc()
                raise normal_error

        print("æº–å‚™è¿”å›æˆåŠŸéŸ¿æ‡‰")
        return jsonify({"success": True, "message": "éƒµä»¶ç™¼é€æˆåŠŸ"}), 200
        
    except Exception as e:
        print(f"âŒ ä¸»å‡½æ•¸éŒ¯èª¤: {str(e)}")

        print("å®Œæ•´éŒ¯èª¤è¿½è¹¤:")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500
    
@app.route('/update_for_mail', methods=['POST'])
def update_for_mail():
    # å‚™ä»½
    backup_files()
    new_data = request.json
    print("æ”¶åˆ°çš„è³‡æ–™:", new_data)
    
    if new_data is None:
        return jsonify({"message": "æ²’æœ‰æ”¶åˆ°è³‡æ–™"}), 400
    
    try:
        df = pd.read_csv(CSV_FILE, dtype=str)
        df.fillna("", inplace=True)

        # æ‰¾åˆ°å°æ‡‰åˆ—
        df["Id"] = df["Id"].astype(str).str.strip()
        target_id = str(new_data['Id']).strip()
        
        print(f"å°‹æ‰¾ Id: {target_id}")
        
        # æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„åˆ—
        mask = df["Id"] == target_id
        
        if not mask.any():
            print(f"æ‰¾ä¸åˆ° Id ç‚º {target_id} çš„è³‡æ–™")
            return jsonify({"message": "æ‰¾ä¸åˆ°å°æ‡‰çš„è³‡æ–™"}), 404
        
        print(f"æ‰¾åˆ°å°æ‡‰è³‡æ–™ï¼Œé–‹å§‹æ›´æ–°...")
        
        # æ›´æ–°è³‡æ–™
        for key, value in new_data.items():
            if key in df.columns:
                df.loc[mask, key] = str(value) if value is not None else ""
                print(f"æ›´æ–° {key}: {value}")
        
        # å„²å­˜åˆ° CSV
        df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")
        print("è³‡æ–™å·²å„²å­˜")
        
        return jsonify({"message": "æ›´æ–°æˆåŠŸ"}), 200

    except Exception as e:
        print("éŒ¯èª¤ï¼š", e)
        
        traceback.print_exc()
        return jsonify({"message": "è³‡æ–™æ›´æ–°å¤±æ•—"}), 500


# ç½é ­è¨Šæ¯   - ã€‚(å‰è³¼å–®ï¼šï¼Œè©³å¦‚é™„ä»¶)éœ€æ±‚å·¥ç¨‹å¸«ï¼š(CT4:)ï¼Œåˆä½œé–‹ç™¼ï¼š
@app.route('/api/get_phone', methods=['POST'])
def get_phone():
    data = request.get_json()
    name = data.get("name", "")
    with open('static/data/phone.json', 'r', encoding='utf-8-sig') as f:
        phone_data = json.load(f)

    phone_number = phone_data.get(name, "æœªçŸ¥")

    return jsonify({"phone": phone_number})


# å» å•†
@app.route('/api/venders', methods=['GET'])
def get_venders():
    """è®€å– vender.ini ä¸¦è¿”å›ä¾›æ‡‰å•†åˆ—è¡¨"""
    try:
        if os.path.exists(VENDER_FILE_PATH):
            with open(VENDER_FILE_PATH, 'r', encoding='utf-8') as f:
                venders = [line.strip() for line in f.readlines() if line.strip()]
            return jsonify(venders)
        else:
            return jsonify([])
    except Exception as e:
        print(e)
        return jsonify({'error': str(e)}), 500


@app.route('/api/venders', methods=['POST'])
def add_vender():
    """æ–°å¢ä¾›æ‡‰å•†åˆ° vender.ini"""
    try:
        data = request.get_json()
        new_vender = data.get('vender', '').strip()

        if not new_vender:
            return jsonify({'error': 'ä¾›æ‡‰å•†åç¨±ä¸èƒ½æ˜¯ç©ºçš„'}), 400

        # è®€å–ç¾æœ‰æ¸…å–®ï¼Œé¿å…é‡è¤‡
        existing = []
        if os.path.exists(VENDER_FILE_PATH):
            with open(VENDER_FILE_PATH, 'r', encoding='utf-8') as f:
                existing = [line.strip() for line in f.readlines() if line.strip()]

        if new_vender in existing:
            return jsonify({'message': 'ä¾›æ‡‰å•†å·²å­˜åœ¨'}), 200

        # è¿½åŠ å¯«å…¥
        with open(VENDER_FILE_PATH, 'a', encoding='utf-8') as f:
            f.write(new_vender + '\n')

        return jsonify({'message': 'æ–°å¢æˆåŠŸ', 'vender': new_vender}), 201

    except Exception as e:
        print(e)
        return jsonify({'error': str(e)}), 500


# eHub è™•ç†
@app.route("/api/save_csv", methods=["POST", "OPTIONS"])
def save_csv():
    # å‚™ä»½
    backup_files()
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200

    data = request.get_json(silent=True) or {}
    content = data.get("content", "")

    from io import StringIO
    df_all = pd.read_csv(StringIO(content), dtype=str).fillna("")

    # å…ˆè™•ç† Item â†’ ä¸€å¾‹è£œæˆ4ä½æ•¸
    if "PO Item æ¡è³¼å–®é …æ¬¡" in df_all.columns:
        df_all["PO Item æ¡è³¼å–®é …æ¬¡"] = (
            df_all["PO Item æ¡è³¼å–®é …æ¬¡"]
            .str.strip()
            .apply(lambda x: str(int(float(x))) if x.replace('.', '', 1).isdigit() else x)
            .apply(lambda x: x.zfill(4) if x.isdigit() else x)
        )

    # ç¢ºèªæœ‰å¿…è¦æ¬„ä½
    if "PO NO æ¡è³¼å–®è™Ÿç¢¼" not in df_all.columns:
        return jsonify({"status": "error", "msg": "âŒ ç¼ºå°‘ PO NO æ¡è³¼å–®è™Ÿç¢¼ æ¬„ä½ï¼"}), 400

    # æŠ“å‡ºæ‰€æœ‰ä¸åŒçš„ PO NO
    unique_po_nos = df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"].dropna().unique()

    # âœ… ç¬¬ä¸€æ­¥ï¼šåˆ†ç¾¤å„²å­˜ uploads/{po_no}.csv
    saved_files = []
    for po_no in unique_po_nos:
        po_no_clean = str(po_no).strip()
        group_df = df_all[df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"] == po_no_clean]

        upload_path = f"uploads/{po_no_clean}.csv"
        group_df.to_csv(upload_path, index=False, encoding="utf-8-sig")

        saved_files.append({
            "po_no": po_no_clean,
            "rows": len(group_df),
            "file": upload_path
        })
        print(f"âœ… å·²å„²å­˜ {upload_path} ({len(group_df)} ç­†è³‡æ–™)")

    # âœ… ç¬¬äºŒæ­¥ï¼šè¼‰å…¥ Buyer_detail.csv (æ¯”å°ç”¨)
    df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
    df_buyer["PO No."] = df_buyer["PO No."].str.strip()
    df_buyer["Item"] = (
        df_buyer["Item"]
        .str.replace(r"\.0$", "", regex=True)
        .str.strip()
        .apply(lambda x: x.zfill(4) if x.isdigit() else x)
    )

    def clean_name(x: str) -> str:
        return str(x).replace("\n", "").replace("\r", "").replace("<br>", "").strip()

    df_buyer["å“é …_clean"] = df_buyer["å“é …"].apply(clean_name)
    buyer_id_lookup = {
        (row["PO No."], row["Item"]): row.get("Id", "")
        for _, row in df_buyer.iterrows()
    }
    # âœ… æœ€å¾Œè¦å›å‚³çš„çµæœï¼Œä¾ç…§ po_no åˆ†çµ„
    all_group_results = []

    # âœ… ç¬¬ä¸‰æ­¥ï¼šé€ä¸€æ‰“é–‹ uploads/{po_no}.csv å»æ¯”å°
    for file_info in saved_files:
        po_no = file_info["po_no"]
        csv_path = file_info["file"]

        # æ‰“é–‹å‰›å‰›å„²å­˜çš„é€™å€‹ po_no CSV
        df_po = pd.read_csv(csv_path, encoding="utf-8-sig", dtype=str).fillna("")
        df_po["PO Item æ¡è³¼å–®é …æ¬¡"] = df_po["PO Item æ¡è³¼å–®é …æ¬¡"].str.zfill(4)

        # é€™å€‹ po_no ç›¸é—œçš„ Buyer_detail è³‡æ–™
        buyer_related = df_buyer[df_buyer["PO No."].str.contains(po_no, regex=False, na=False)].copy()

        matched_list = []
        conflict_list = []

        # é€ç­†æ¯”å°
        for _, row in df_po.iterrows():
            item = row["PO Item æ¡è³¼å–®é …æ¬¡"]
            desc = clean_name(row.get("Description å“å", ""))
            delivery = row["Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"]
            qty = row["SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"]

            # å…ˆç”¨å“é …æ¯”å°
            same_name_match = buyer_related[buyer_related["å“é …_clean"] == desc]

            if not same_name_match.empty:
                buyer_row = same_name_match.iloc[0]
                # âœ… å“é …ç›¸åŒ â†’ æª¢æŸ¥ Item æ˜¯å¦ä¸€è‡´
                buyer_item = same_name_match.iloc[0]["Item"]
                buyer_desc = clean_name(same_name_match.iloc[0]["å“é …"])
                buyer_id = buyer_row.get("Id", "")

                # æ›´æ–°äº¤æœŸ & æ•¸é‡
                df_buyer.loc[same_name_match.index, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = delivery
                df_buyer.loc[same_name_match.index, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = qty

                if buyer_item == item:
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    # âœ… å“é … & Item éƒ½ä¸€è‡´
                    matched_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc or buyer_desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âœ… ç›¸åŒ",
                        "diff_type": "none"
                    })
                else:
                    # âš ï¸ å“é …ç›¸åŒï¼Œä½† Item ä¸åŒ
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    conflict_list.append({
                        "id": '',
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc or buyer_desc,
                        "old_item": buyer_item,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": f"âš ï¸ Item ä¸ç›¸åŒ (èˆŠ {buyer_item} â†’ æ–° {item})",
                        "diff_type": "item"
                    })

            else:
                # æ”¹ç”¨ Item æ¯”å°
                buyer_match_by_item = buyer_related[buyer_related["Item"] == item]

                if not buyer_match_by_item.empty:
                    # âœ… Item ç›¸åŒä½†å“é …ä¸åŒ â†’ å“åä¸ç›¸åŒ
                    buyer_desc = clean_name(buyer_match_by_item.iloc[0]["å“é …"])
                    df_buyer.loc[buyer_match_by_item.index, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = delivery
                    df_buyer.loc[buyer_match_by_item.index, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = qty
                    buyer_id = buyer_id_lookup.get((po_no, item), "")
                    conflict_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âš ï¸ å“åä¸ç›¸åŒ",
                        "diff_type": "desc"
                    })
                else:
                    # âŒ å®Œå…¨æ²’æ‰¾åˆ°
                    buyer_id = ''
                    conflict_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": None,
                        "po_description": desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âš ï¸ Buyer_detail æ²’æœ‰é€™ç­†è³‡æ–™",
                        "diff_type": "missing"
                    })

        # æŠŠé€™å€‹ po_no çš„çµæœå­˜é€²ç¸½çµæœ
        all_group_results.append({
            "po_no": po_no,
            "matched": matched_list,
            "conflict": conflict_list
        })
    print(all_group_results)
    # âœ… æœ€å¾Œä¸€æ¬¡æ€§å›å‚³ï¼Œå‰ç«¯å¯ä»¥ä¾ç…§ po_no åˆ†çµ„é¡¯ç¤º
    return jsonify({
        "status": "ok",
        "groups": all_group_results,
        "saved_files": saved_files
    })


from difflib import SequenceMatcher

def is_po_in_record(row_po_str, target_po):
    """æª¢æŸ¥ PO æ˜¯å¦åœ¨è¨˜éŒ„ä¸­ï¼ˆæ”¯æ´ <br /> åˆ†éš”çš„å¤šå€‹ POï¼‰"""
    po_list = re.split(r"<br\s*/?>", str(row_po_str))
    po_list = [po.strip() for po in po_list if po.strip()]
    return target_po.strip() in po_list

def fuzzy_in(text, keyword):
    """æ¨¡ç³Šæ¯”å°é—œéµå­—æ˜¯å¦åœ¨æ–‡å­—ä¸­"""
    return keyword.strip() in str(text).strip()

@app.route("/api/save_override_all", methods=["POST"])
def save_override_all():
    """
    Version 31 - å“åå„ªå…ˆæ¯”å°é‚è¼¯
    æ”¹é€²ï¼šå„ªå…ˆä»¥å“åç›¸ä¼¼åº¦ç‚ºä¸»è¦æ¯”å°ä¾æ“šï¼ŒItem ä½œç‚ºæ¬¡è¦åƒè€ƒ
    å„ªå…ˆé †åºï¼šåŒ PO å…§çš„å“åé«˜åº¦ç›¸ä¼¼ > Item ç›¸åŒ > å“åä¸­åº¦ç›¸ä¼¼ > æ–°å¢
    """
    # å‚™ä»½
    backup_files()
    data = request.get_json()
    rows = data.get("rows", [])
    confirm_override = data.get("confirm_override", False)  # æ˜¯å¦å·²ç¢ºèªè¦†è“‹

    if not rows:
        return jsonify({"status": "error", "msg": "âŒ æ²’æœ‰æ”¶åˆ°ä»»ä½•è³‡æ–™"}), 400

    def clean_text(x):
        """æ¸…ç†æ–‡å­—ï¼šç§»é™¤æ›è¡Œå’Œç©ºç™½"""
        return str(x).replace("\n", "").replace("\r", "").strip()
    
    def calculate_similarity(text1, text2):
        """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-100)"""
        text1_clean = clean_text(text1).lower()
        text2_clean = clean_text(text2).lower()
        return SequenceMatcher(None, text1_clean, text2_clean).ratio() * 100
    
    # è™•ç† pandas int64 è½‰æ›å•é¡Œ
    def convert_to_json_serializable(obj):
        """å°‡ pandas çš„ç‰¹æ®Šé¡å‹è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„é¡å‹"""
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_json_serializable(item) for item in obj]
        elif hasattr(obj, 'item'):  # numpy/pandas æ•¸å€¼é¡å‹
            return obj.item()
        elif pd.isna(obj):  # NaN å€¼
            return None
        else:
            return obj

    # è®€å–ä¸¦è™•ç†è³‡æ–™
    df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
    df_buyer["PO No."] = df_buyer["PO No."].str.strip()
    df_buyer["Item"] = (
        df_buyer["Item"]
        .str.replace(r"\.0$", "", regex=True)
        .str.strip()
        .apply(lambda x: x.zfill(4) if x.isdigit() else x)
    )
    
    # ğŸ”´ é‡è¦ï¼šå»ºç«‹ä¸€å€‹åªåŒ…å«ç‹€æ…‹ç‚º V çš„è³‡æ–™ç´¢å¼•
    df_active = df_buyer[df_buyer["é–‹å–®ç‹€æ…‹"] == "V"].copy()
    print(f"ç¸½è³‡æ–™ç­†æ•¸: {len(df_buyer)}, æœ‰æ•ˆè³‡æ–™(ç‹€æ…‹=V): {len(df_active)}")

    updated_count = 0
    inserted_count = 0
    failed = []
    need_confirm_items = []  # éœ€è¦ç¢ºèªçš„é …ç›®
    matching_output = []  # æ¯”å°çµæœè¼¸å‡º

    new_item = ''
    epr_no = 0
    po_no_new = ''
    
    # è¼¸å‡ºé–‹å§‹è¨Šæ¯
    print("\n" + "="*80)
    print("é–‹å§‹è™•ç†è³‡æ–™æ¯”å° (Version 31 - å“åå„ªå…ˆ)")
    print("="*80)

    for row_num, row in enumerate(rows, 1):
        id_ = row.get("id", "").strip()
        po_no_new = row.get("po_no", "").strip()
        item_new = row.get("item", "").strip()
        
        # ç¢ºä¿ item æ ¼å¼ä¸€è‡´ï¼ˆ4ä½æ•¸ï¼‰
        if item_new.isdigit():
            item_new = item_new.zfill(4)
            
        new_delivery = row.get("delivery_date", "").strip()
        new_qty = row.get("sod_qty", "").strip()
        new_desc = row.get("po_description", "").strip()
        new_desc_clean = clean_text(new_desc)

        target_idx = None
        match_reason = ""
        
        # è¼¸å‡ºç•¶å‰è™•ç†é …ç›®
        print(f"\n[ç¬¬ {row_num} ç­†]")
        print(f"  æ–°è³‡æ–™ => PO: {po_no_new}, Item: {item_new}")
        print(f"  å“å: {new_desc[:50]}{'...' if len(new_desc) > 50 else ''}")
        
        # ğŸ” Version 31 æ ¸å¿ƒæ”¹è®Šï¼šå…ˆæ‰¾å“åç›¸ä¼¼åº¦ï¼Œå†è€ƒæ…® Item
        # æ­¥é©Ÿ1ï¼šå…ˆåœ¨åŒ PO å…§æ‰¾è³‡æ–™ï¼ˆåªæ‰¾ç‹€æ…‹ç‚º V çš„ï¼‰
        po_group = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        
        if not po_group.empty:
            print(f"     åœ¨ PO {po_no_new} æ‰¾åˆ° {len(po_group)} ç­†è³‡æ–™")
            
            # ğŸ”¥ Version 31ï¼šè¨ˆç®—æ‰€æœ‰é …ç›®çš„å“åç›¸ä¼¼åº¦
            similarity_scores = []
            for idx, row_data in po_group.iterrows():
                existing_desc = row_data["å“é …"]
                existing_item = row_data["Item"]
                similarity = calculate_similarity(new_desc, existing_desc)
                similarity_scores.append({
                    'index': idx,
                    'item': existing_item,
                    'desc': existing_desc,
                    'similarity': similarity,
                    'item_match': (existing_item == item_new),  # è¨˜éŒ„ Item æ˜¯å¦ç›¸åŒ
                    'delivery_date': row_data.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")  # åŠ å…¥äº¤æœŸè³‡è¨Š
                })
            
            # ğŸ”´ Version 31 æ”¹é€²ï¼šè™•ç†ç›¸åŒ Item çš„å¤šç­†è³‡æ–™
            # å¦‚æœæœ‰å¤šç­†å®Œå…¨ç›¸åŒçš„ Itemï¼Œé¸æ“‡äº¤æœŸæœ€æ–°çš„
            item_matches = [s for s in similarity_scores if s['item_match']]
            if len(item_matches) > 1:
                print(f"     ç™¼ç¾ {len(item_matches)} ç­†ç›¸åŒçš„ Item {item_new}")
                
                # å°‡äº¤æœŸè½‰æ›ç‚ºå¯æ¯”è¼ƒçš„æ ¼å¼ä¸¦æ’åº
                for match in item_matches:
                    try:
                        # å˜—è©¦è§£ææ—¥æœŸï¼ˆæ”¯æ´ YYYY/MM/DD æ ¼å¼ï¼‰
                        date_str = match['delivery_date'].strip()
                        if date_str:
                            # ç§»é™¤å¯èƒ½çš„æ™‚é–“éƒ¨åˆ†ï¼Œåªä¿ç•™æ—¥æœŸ
                            date_str = date_str.split(' ')[0]
                            # è½‰æ›æ–œç·šç‚ºç ´æŠ˜è™Ÿä»¥ä¾¿è§£æ
                            date_str = date_str.replace('/', '-')
                            match['parsed_date'] = date_str
                        else:
                            match['parsed_date'] = '1900-01-01'  # ç©ºæ—¥æœŸè¨­ç‚ºæœ€æ—©
                    except:
                        match['parsed_date'] = '1900-01-01'
                    
                    print(f"       - Index {match['index']}: äº¤æœŸ {match['delivery_date']}")
                
                # æŒ‰æ—¥æœŸæ’åºï¼Œé¸æ“‡æœ€æ–°çš„
                item_matches.sort(key=lambda x: x.get('parsed_date', '1900-01-01'), reverse=True)
                newest_match = item_matches[0]
                print(f"     => é¸æ“‡æœ€æ–°äº¤æœŸçš„è³‡æ–™: Index {newest_match['index']} (äº¤æœŸ: {newest_match['delivery_date']})")
                
                # å°‡æœ€æ–°çš„è³‡æ–™ç§»åˆ° similarity_scores çš„æœ€å‰é¢
                similarity_scores = [s for s in similarity_scores if not s['item_match']]
                similarity_scores.insert(0, newest_match)
            else:
                # æ’åºï¼šå…ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œç›¸ä¼¼åº¦ç›¸åŒæ™‚ Item ç›¸åŒçš„å„ªå…ˆ
                similarity_scores.sort(key=lambda x: (x['similarity'], x['item_match']), reverse=True)
            
            # è¼¸å‡ºç›¸ä¼¼åº¦æ’åï¼ˆé™¤éŒ¯ç”¨ï¼‰
            print(f"     å“åç›¸ä¼¼åº¦æ’åï¼š")
            for i, score in enumerate(similarity_scores[:3], 1):  # é¡¯ç¤ºå‰3å
                item_marker = " [Itemç›¸åŒ]" if score['item_match'] else ""
                print(f"       {i}. Item {score['item']}: {score['similarity']:.1f}%{item_marker} - {score['desc'][:30]}...")
            
            # å–å¾—æœ€é«˜ç›¸ä¼¼åº¦çš„é …ç›®
            best_match = similarity_scores[0]
            best_similarity = best_match['similarity']
            best_idx = best_match['index']
            best_item = best_match['item']
            best_desc = best_match['desc']
            
            # ğŸ¯ æ ¹æ“šç›¸ä¼¼åº¦å’Œ Item æ˜¯å¦ç›¸åŒä¾†æ±ºå®šè™•ç†æ–¹å¼
            if best_similarity >= 95:  # å“åå¹¾ä¹å®Œå…¨ç›¸åŒ
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = "å“åå®Œå…¨ç›¸åŒ+Itemç›¸åŒ"
                    print(f"  âœ… å“åå®Œå…¨ç›¸åŒä¸” Item ç›¸åŒï¼ˆç›¸ä¼¼åº¦ {best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                else:
                    match_reason = f"å“åå®Œå…¨ç›¸åŒ(Item:{best_item}â†’{item_new})"
                    print(f"  âš ï¸ å“åå®Œå…¨ç›¸åŒä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    print(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    
                    if not confirm_override:
                        print(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–° Item")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åç›¸åŒä½†Itemä¸åŒ",
                            "action_type": "update_item_change"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸åŒä½†Itemä¸åŒ"
                        })
                        continue
                    else:
                        print(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–° Item")
                        
            elif best_similarity >= 80:  # å“åé«˜åº¦ç›¸ä¼¼
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼+Itemç›¸åŒ({best_similarity:.0f}%)"
                    print(f"  âœ… å“åé«˜åº¦ç›¸ä¼¼ä¸” Item ç›¸åŒï¼ˆ{best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                else:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼(Item:{best_item}â†’{item_new})"
                    print(f"  âš ï¸ å“åé«˜åº¦ç›¸ä¼¼ä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    print(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    print(f"     åŸå“å: {best_desc[:40]}...")
                    print(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        print(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åé«˜åº¦ç›¸ä¼¼ä½†Itemä¸åŒ",
                            "action_type": "update_high_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸ä¼¼{best_similarity:.0f}%"
                        })
                        continue
                    else:
                        print(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            elif best_similarity >= 60:  # å“åä¸­åº¦ç›¸ä¼¼
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item ç›¸åŒçš„é …ç›®
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match and item_match['similarity'] >= 40:
                    # å¦‚æœæœ‰ Item ç›¸åŒä¸”ç›¸ä¼¼åº¦ä¸æ˜¯å¤ªä½ï¼Œå„ªå…ˆé¸æ“‡ Item ç›¸åŒçš„
                    target_idx = item_match['index']
                    match_reason = f"Itemç›¸åŒ+å“åç›¸ä¼¼({item_match['similarity']:.0f}%)"
                    print(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒçš„é …ç›®ï¼Œå“åç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%")
                    print(f"     åŸå“å: {item_match['desc'][:40]}...")
                    print(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        print(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå·®ç•°è¼ƒå¤§",
                            "action_type": "update_medium_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åå·®ç•°{item_match['similarity']:.0f}%"
                        })
                        continue
                    else:
                        print(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                else:
                    # å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒ
                    target_idx = best_idx
                    match_reason = f"å“åä¸­åº¦ç›¸ä¼¼({best_similarity:.0f}%)"
                    print(f"  âš ï¸ å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    print(f"     ç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    print(f"     åŸå“å: {best_desc[:40]}...")
                    print(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        print(f"     éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åä¸­åº¦ç›¸ä¼¼",
                            "action_type": "update_medium_similarity",
                            "warning": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åä¸­åº¦ç›¸ä¼¼"
                        })
                        continue
                    else:
                        print(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            else:  # ç›¸ä¼¼åº¦ < 60%
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item å®Œå…¨ç›¸åŒçš„
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match:
                    # Item ç›¸åŒä½†å“åç›¸ä¼¼åº¦ä½
                    print(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒä½†å“åå·®ç•°å¾ˆå¤§ï¼ˆç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%ï¼‰")
                    print(f"     åŸå“å: {item_match['desc'][:40]}...")
                    print(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        print(f"     âš ï¸âš ï¸ å“åå·®ç•°å¾ˆå¤§ï¼éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå®Œå…¨ä¸åŒ",
                            "action_type": "update_low_similarity",
                            "warning": True,
                            "critical": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": f"Itemç›¸åŒ(ç›¸ä¼¼åº¦{item_match['similarity']:.0f}%)",
                            "action": "å¾…ç¢ºèª",
                            "note": f"âŒå“åå·®ç•°æ¥µå¤§"
                        })
                        continue
                    else:
                        target_idx = item_match['index']
                        match_reason = f"Itemç›¸åŒ(å“åå·®ç•°å¤§)"
                        print(f"     => å·²ç¢ºèªï¼Œå°‡å¼·åˆ¶æ›´æ–°")
                else:
                    # æ²’æœ‰ä»»ä½•åŒ¹é…ï¼Œå»ºè­°æ–°å¢
                    target_idx = None
        
        # æ­¥é©Ÿ2ï¼šå¦‚æœåœ¨åŒ PO å…§æ‰¾ä¸åˆ°åŒ¹é…ï¼Œè©¢å•æ˜¯å¦æ–°å¢
        if target_idx is None and not po_group.empty:
            print(f"  âš ï¸  åœ¨ PO {po_no_new} å…§æ‰¾ä¸åˆ°ç›¸ä¼¼çš„å“åæˆ–ç›¸åŒçš„ Item")
            print(f"     æ–°Item: {item_new}, æ–°å“å: {new_desc[:40]}...")
            
            if not confirm_override:
                print(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ–°å¢ç‚ºæ–°é …ç›®")
                
                # åˆ—å‡ºç¾æœ‰çš„é …ç›®ä¾›åƒè€ƒ
                existing_items = []
                for score in similarity_scores[:5]:  # é¡¯ç¤ºå‰5å€‹ç›¸ä¼¼åº¦æœ€é«˜çš„
                    existing_items.append({
                        "item": score['item'],
                        "desc": score['desc'][:50],
                        "similarity": round(score['similarity'], 1)
                    })
                
                need_confirm_items.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "new_item": item_new,
                    "new_desc": new_desc,
                    "existing_items": existing_items,
                    "new_delivery": new_delivery,
                    "new_qty": new_qty,
                    "reason": "ç„¡ç›¸ä¼¼é …ç›®",
                    "action_type": "add_new"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡åŒ¹é…",
                    "action": "å¾…ç¢ºèª",
                    "note": "å»ºè­°æ–°å¢"
                })
                continue
            else:
                print(f"     => å·²ç¢ºèªï¼Œå°‡æ–°å¢ç‚ºæ–°é …ç›®")
        
        # æ­¥é©Ÿ3ï¼šå…¶ä»–æ¯”å°æ–¹å¼ï¼ˆIDã€æ¨¡ç³Šæ¯”å°ç­‰ï¼‰- åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None and id_:
            candidates = df_active[df_active["Id"] == id_].copy()
            
            if len(candidates) > 1:
                candidates["å“é …_clean"] = candidates["å“é …"].apply(clean_text)
                exact_match = candidates[candidates["å“é …_clean"] == new_desc_clean]
                if len(exact_match) == 1:
                    target_idx = exact_match.index[0]
                    match_reason = "ID+å“é …åŒ¹é…"
                    print(f"  âœ… ID+å“é …åŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")
            elif len(candidates) == 1:
                target_idx = candidates.index[0]
                match_reason = "IDåŒ¹é…"
                print(f"  âœ… IDåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # æ­¥é©Ÿ4ï¼šPO + å“é …æ¨¡ç³Šæ¯”å° - åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None:
            po_match = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
            po_match = po_match[po_match["å“é …"].apply(lambda x: fuzzy_in(x, new_desc_clean))]

            if not po_match.empty:
                target_idx = po_match.index[0]
                match_reason = "PO+å“é …æ¨¡ç³ŠåŒ¹é…"
                print(f"  âœ… PO+å“é …æ¨¡ç³ŠåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # ğŸ“ å¦‚æœæ‰¾åˆ°åŒ¹é…é …ç›®ï¼ŒåŸ·è¡Œæ›´æ–°
        if target_idx is not None:
            # è¨˜éŒ„åŸå§‹å€¼ï¼ˆç”¨æ–¼è¼¸å‡ºï¼‰
            old_values = {
                "po": df_buyer.at[target_idx, "PO No."],
                "item": df_buyer.at[target_idx, "Item"],
                "delivery": df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"],
                "qty": df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"],
                "desc": df_buyer.at[target_idx, "å“é …"]
            }
            
            # åŸ·è¡Œæ›´æ–°
            df_buyer.at[target_idx, "PO No."] = po_no_new
            df_buyer.at[target_idx, "Item"] = item_new
            df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = new_delivery
            df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = new_qty
            if new_desc:
                df_buyer.at[target_idx, "å“é …"] = new_desc
            updated_count += 1
            
            # è¼¸å‡ºè®Šæ›´è©³æƒ…
            if old_values["po"] != po_no_new:
                print(f"     POè®Šæ›´: {old_values['po']} â†’ {po_no_new}")
            if old_values["item"] != item_new:
                print(f"     Itemè®Šæ›´: {old_values['item']} â†’ {item_new}")
            if old_values["delivery"] != new_delivery:
                print(f"     äº¤æœŸè®Šæ›´: {old_values['delivery']} â†’ {new_delivery}")
            if old_values["qty"] != new_qty:
                print(f"     æ•¸é‡è®Šæ›´: {old_values['qty']} â†’ {new_qty}")
            
            matching_output.append({
                "row": row_num,
                "po": po_no_new,
                "item": item_new if old_values["item"] == item_new else f"{old_values['item']}â†’{item_new}",
                "match": match_reason,
                "action": "å·²æ›´æ–°",
                "note": "å“åå·²è®Šæ›´" if old_values["desc"] != new_desc else ""
            })
            continue

        # ğŸ†• å¦‚æœéƒ½æ‰¾ä¸åˆ° â†’ æ–°å¢è³‡æ–™ï¼ˆä½†è¦æª¢æŸ¥ PO æ˜¯å¦å­˜åœ¨æ–¼ç‹€æ…‹ V çš„è³‡æ–™ä¸­ï¼‰
        po_matches = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        if po_matches.empty:
            # å†æª¢æŸ¥æ˜¯å¦æœ‰ç‹€æ…‹ç‚º X çš„ç›¸åŒ PO
            po_cancelled = df_buyer[
                (df_buyer["é–‹å–®ç‹€æ…‹"] == "X") & 
                (df_buyer["PO No."].apply(lambda x: is_po_in_record(x, po_no_new)))
            ]
            
            if not po_cancelled.empty:
                print(f"  âš ï¸  æ‰¾åˆ° PO {po_no_new} ä½†ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰ï¼Œç„¡æ³•æ›´æ–°")
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": f"PO {po_no_new} ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "POå·²å–æ¶ˆ",
                    "action": "å¤±æ•—",
                    "note": "ç‹€æ…‹ç‚ºX"
                })
                continue
            else:
                # ğŸ”´ Version 31ï¼šPO å®Œå…¨ä¸å­˜åœ¨çš„æƒ…æ³
                print(f"  âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®ï¼šPO {po_no_new}")
                
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡PO",
                    "action": "å¤±æ•—",
                    "note": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                continue
        
        # ğŸ†” æ¨æ¸¬ Idï¼ˆå–é¦–ç­†ï¼‰
        possible_ids = po_matches["Id"].dropna().unique().tolist()
        id_ = possible_ids[0] if possible_ids else row.get("id") or row.get("Id", "")
        id_ = str(id_).strip()

        # ğŸ‘¤ å–åŒçµ„ç¬¬ä¸€ç­†çš„è³‡è¨Š
        user = po_matches["User"].iloc[0] if not po_matches.empty else ""
        epr_no = po_matches["ePR No."].iloc[0] if not po_matches.empty else ""
        wbs_no = po_matches["WBS"].iloc[0] if not po_matches.empty else ""
        need_day_no = po_matches["éœ€æ±‚æ—¥"].iloc[0] if not po_matches.empty else ""

        print(f"  ğŸ†• æ‰¾ä¸åˆ°åŒ¹é…é …ç›® => æ–°å¢è³‡æ–™")
        print(f"     æ–°å¢åˆ° ePR No.: {epr_no}")

        # â• æ–°å¢æ–°çš„ä¸€ç­†è³‡æ–™
        new_row = {
            "Id": id_,
            "é–‹å–®ç‹€æ…‹": "V",
            "äº¤è²¨é©—è­‰": "",
            "User": user,
            "ePR No.": epr_no,
            "PO No.": po_no_new,
            "Item": item_new,
            "å“é …": new_desc,
            "è¦æ ¼": "",
            "æ•¸é‡": new_qty,
            "ç¸½æ•¸": new_qty,
            "å–®åƒ¹": "",
            "ç¸½åƒ¹": "",
            "å‚™è¨»": "",
            "å­—æ•¸": "",
            "isEditing": "False",
            "backup": "{}",
            "_alertedItemLimit": "",
            "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": new_delivery,
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": new_qty,
            "é©—æ”¶æ•¸é‡": "",
            "æ‹’æ”¶æ•¸é‡": "",
            "ç™¼ç¥¨æœˆä»½": "",
            "WBS": wbs_no,
            "éœ€æ±‚æ—¥": need_day_no,
            "RTé‡‘é¡": '',
            "RTç¸½é‡‘é¡": '',
            "é©—æ”¶ç‹€æ…‹": "X" 
        }

        # ğŸ“Œ æ‰¾é€™å€‹ id çš„æœ€å¾Œä¸€ç­†ä½ç½®
        same_id_idx = df_buyer[df_buyer["Id"] == id_].index
        insert_pos = same_id_idx[-1] + 1 if len(same_id_idx) > 0 else len(df_buyer)

        # âœ¨ æ’å…¥åˆ°åŸ df_buyer ä¸­æŒ‡å®šä½ç½®
        df_buyer = pd.concat([
            df_buyer.iloc[:insert_pos],
            pd.DataFrame([new_row]),
            df_buyer.iloc[insert_pos:]
        ], ignore_index=True)
        
        new_item = 'æ–°å¢ç‰©ä»¶'
        inserted_count += 1
        
        matching_output.append({
            "row": row_num,
            "po": po_no_new,
            "item": item_new,
            "match": "ç„¡åŒ¹é…",
            "action": "å·²æ–°å¢",
            "note": f"ePR:{epr_no}"
        })

    # è¼¸å‡ºæ¯”å°çµæœæ‘˜è¦
    print("\n" + "="*80)
    print("æ¯”å°çµæœæ‘˜è¦ (Version 31 - å“åå„ªå…ˆ)")
    print("="*80)
    print(f"ç¸½è™•ç†ç­†æ•¸: {len(rows)}")
    print(f"æ›´æ–°ç­†æ•¸: {updated_count}")
    print(f"æ–°å¢ç­†æ•¸: {inserted_count}")
    print(f"å¤±æ•—ç­†æ•¸: {len(failed)}")
    print(f"å¾…ç¢ºèªç­†æ•¸: {len(need_confirm_items)}")
    
    # è¼¸å‡ºè©³ç´°æ¯”å°è¡¨æ ¼
    if matching_output:
        print("\nè©³ç´°æ¯”å°çµæœ:")
        print("-"*80)
        print(f"{'ç­†æ•¸':<5} {'PO No.':<15} {'Item':<10} {'æ¯”å°æ–¹å¼':<20} {'è™•ç†':<8} {'å‚™è¨»'}")
        print("-"*80)
        for item in matching_output:
            print(f"{item['row']:<5} {item['po']:<15} {item['item']:<10} {item['match']:<20} {item['action']:<8} {item['note']}")
    
    print("="*80 + "\n")

    # å¦‚æœæœ‰éœ€è¦ç¢ºèªçš„é …ç›®ï¼Œå›å‚³çµ¦å‰ç«¯
    if need_confirm_items and not confirm_override:
        # æª¢æŸ¥æ˜¯å¦æœ‰é—œéµç¢ºèªé …ï¼ˆå“åå®Œå…¨ä¸åŒï¼‰
        critical_items = [item for item in need_confirm_items if item.get("critical", False)]
        warning_items = [item for item in need_confirm_items if item.get("warning", False)]
        
        # æ ¹æ“šä¸åŒçš„ç¢ºèªåŸå› å’Œå‹•ä½œé¡å‹ï¼Œç”¢ç”Ÿä¸åŒçš„è¨Šæ¯
        action_types = set(item.get("action_type", "") for item in need_confirm_items)
        
        if critical_items:
            msg = f"âš ï¸ ç™¼ç¾ {len(critical_items)} å€‹å“åå®Œå…¨ä¸åŒçš„é …ç›®éœ€è¦ç‰¹åˆ¥ç¢ºèª"
        elif "update_low_similarity" in action_types:
            msg = f"âŒ ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸ä¼¼åº¦æ¥µä½çš„é …ç›®éœ€è¦ç¢ºèª"
        elif "update_item_change" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸åŒä½†Itemä¸åŒçš„é …ç›®éœ€è¦ç¢ºèª"
        elif "add_new" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹é …ç›®å¯èƒ½éœ€è¦æ–°å¢"
        else:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹éœ€è¦ç¢ºèªçš„é …ç›®"
        
        return jsonify(convert_to_json_serializable({
            "status": "confirm_needed",
            "msg": msg,
            "items": need_confirm_items,
            "updated": updated_count,
            "inserted": inserted_count,
            "matching_output": matching_output,
            "has_critical": len(critical_items) > 0,
            "has_warning": len(warning_items) > 0
        }))

    # å„²å­˜å›æª”æ¡ˆ
    df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
    
    # åˆªé™¤æš«å­˜æª”æ¡ˆ
    if po_no_new:
        temp_file = f"uploads/{po_no_new}.csv"
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    # ğŸ”´ æª¢æŸ¥æ˜¯å¦æ‰€æœ‰é …ç›®éƒ½å¤±æ•—ä¸”åŸå› éƒ½æ˜¯ã€Œ360è¡¨å–®ç„¡æ­¤é …ç›®ã€
    if failed and len(failed) == len(rows):
        all_not_found = all(f.get("reason") == "360è¡¨å–®ç„¡æ­¤é …ç›®" for f in failed)
        if all_not_found:
            return jsonify(convert_to_json_serializable({
                "status": "not_found",
                "msg": "âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®",
                "failed": failed,
                "matching_output": matching_output
            }))
    
    # å›å‚³çµæœ
    if new_item == 'æ–°å¢ç‰©ä»¶':
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"æœ‰æ–°å¢çš„ç‰©ä»¶(item)éœ€è¦ç¶­è­·ï¼ŒePR No. å–®è™Ÿç‚º {epr_no}ã€‚æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output
        }))
    else:
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"âœ… æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output
        }))
    
    

# eRT é©—æ”¶è¡¨å–®
# === è¨­å®š logger ï¼Œé‡å° eRT é©—æ”¶è¡¨å–®===
log_file_path = "buyer_detail_update_log.log"
logging.basicConfig(
    filename=log_file_path,
    filemode='a',
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
    encoding='utf-8'
)
logger = logging.getLogger("BuyerDetailUpdater")


@app.route("/api/update_delivery_receipt", methods=["POST"])
def upload_buyer_detail():
    # å‚™ä»½
    backup_files()
    file = request.files.get('file')
    if not file or not file.filename:
        print("âŒ æ²’æœ‰æ”¶åˆ°æª”æ¡ˆ")
        return jsonify({"status": "fail", "message": "No file uploaded"}), 400

    filename = file.filename
    ext = os.path.splitext(filename)[-1].lower()
    print(f"ğŸ“ æ”¶åˆ°æª”æ¡ˆï¼š{filename}ï¼ˆå‰¯æª”åï¼š{ext}ï¼‰")

    if ext not in ['.xlsx', '.xls']:
        return jsonify({"status": "fail", "message": "Invalid file type"}), 400

    try:
        engine = 'openpyxl' if ext == '.xlsx' else 'xlrd'
        df = pd.read_excel(file, engine=engine)  # é€™è¡Œæœ€å®¹æ˜“å ±éŒ¯
        df.to_csv("static/data/delivery_receipt.csv", index=False, encoding="utf-8-sig")
        print("ğŸ’¾ å·²å„²å­˜ç‚º static/data/delivery_receipt.csvï¼Œé–‹å§‹é€²è¡Œå° Buyer detail è©²è¡¨æ•¸æ“šæ›´æ–°")


        output_df = pd.read_csv("static/data/delivery_receipt.csv", encoding="utf-8-sig", dtype=str)
        buyer_df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)

        # === æ¬„ä½æ¨™æº–åŒ– ===
        output_df.columns = output_df.columns.str.replace("\ufeff", "").str.strip()
        buyer_df.columns = buyer_df.columns.str.strip()


        # === ç¢ºèªä¸¦è™•ç†å¿…è¦çš„æ¬„ä½ ===
        required_buyer_cols = ["PO No.", "å“é …", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "ç™¼ç¥¨æœˆä»½"]
        required_output_cols = ["PONO", "å“å", "é©—æ”¶æ•¸é‡", "æ‹’æ”¶æ•¸é‡", "æ”¶æ–™æ—¥æœŸ"]

        # æª¢æŸ¥ buyer_df æ˜¯å¦æœ‰å¿…è¦çš„æ¬„ä½
        if not all(col in buyer_df.columns for col in required_buyer_cols):
            missing_cols = [col for col in required_buyer_cols if col not in buyer_df.columns]
            print(f"éŒ¯èª¤: {BUYER_FILE} ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            logger.error(f"{BUYER_FILE} ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            exit()

        # æª¢æŸ¥ output_df æ˜¯å¦æœ‰å¿…è¦çš„æ¬„ä½
        if not all(col in output_df.columns for col in required_output_cols):
            missing_cols = [col for col in required_output_cols if col not in output_df.columns]
            print(f"éŒ¯èª¤: delivery_receipt.csv ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            logger.error(f"delivery_receipt.csv ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {', '.join(missing_cols)}")
            exit()

        # === è³‡æ–™æ¸…ç†èˆ‡é è™•ç† ===
        # æ¸…ç† output_df çš„ PONO å’Œ å“å
        output_df["PONO_clean"] = output_df["PONO"].astype(str).str.strip().str.upper()
        output_df["å“å_clean"] = output_df["å“å"].astype(str).str.strip()

        # æ ¹æ“šä½ çš„è¦æ±‚ï¼Œç›´æ¥è¤‡è£½æ”¶æ–™æ—¥æœŸåˆ°æ–°çš„ç™¼ç¥¨æœˆä»½æ¬„ä½ï¼Œä¸é€²è¡Œä»»ä½•æ ¼å¼æ›´æ”¹ã€‚
        output_df["ç™¼ç¥¨æœˆä»½_from_output"] = output_df["æ”¶æ–™æ—¥æœŸ"].astype(str).str.strip()

        output_df.rename(columns={
            "é©—æ”¶æ•¸é‡": "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡": "æ‹’æ”¶æ•¸é‡_from_output"
        }, inplace=True)

        # æ¸…ç† buyer_df çš„ PO No. å’Œ å“é …
        buyer_df["PO_clean"] = buyer_df["PO No."].astype(str).str.strip().str.upper()
        buyer_df["å“é …_clean"] = buyer_df["å“é …"].astype(str).str.strip()

        # é¸æ“‡ output_df éœ€è¦çš„æ¬„ä½é€²è¡Œåˆä½µ
        output_cols_to_merge = [
            "PONO_clean",
            "å“å_clean",
            "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡_from_output",
            "ç™¼ç¥¨æœˆä»½_from_output"
        ]
        output_subset = output_df[output_cols_to_merge].copy()

        # === åˆä½µè³‡æ–™ ===
        merged = pd.merge(
            buyer_df,
            output_subset,
            how='left',
            left_on=["PO_clean", "å“é …_clean"],
            right_on=["PONO_clean", "å“å_clean"],
            suffixes=("", "_from_output")
        )

        # === æ›´æ–°æ¬„ä½ ===
        columns_to_update = {
            "é©—æ”¶æ•¸é‡": "é©—æ”¶æ•¸é‡_from_output",
            "æ‹’æ”¶æ•¸é‡": "æ‹’æ”¶æ•¸é‡_from_output",
            "ç™¼ç¥¨æœˆä»½": "ç™¼ç¥¨æœˆä»½_from_output"
        }

        update_count = 0
        for idx, row in merged.iterrows():
            updated = False
            log_info = {}
            
            po = row.get("PO No.", "N/A")
            item = row.get("Item", "N/A") # ä½¿ç”¨ 'Item' æ¬„ä½
            
            # é€™è£¡ç¢ºä¿ 'å“é …' æ¬„ä½æœ‰å€¼ï¼Œå¦å‰‡ä»¥ 'Item' æ›¿ä»£
            item_for_log = row.get("å“é …", "N/A")
            if item_for_log == 'N/A' or pd.isna(item_for_log):
                item_for_log = item

            for target_col, source_col in columns_to_update.items():
                new_val = row.get(source_col, "")
                old_val = row.get(target_col, "")

                if pd.isna(new_val): new_val = ""
                if pd.isna(old_val): old_val = ""

                clean_new_val = str(new_val).strip()
                clean_old_val = str(old_val).strip()

                if clean_new_val != "" and clean_new_val != clean_old_val:
                    merged.at[idx, target_col] = clean_new_val
                    updated = True
                    log_info[target_col] = {"old": old_val, "new": clean_new_val}

            if updated:
                update_count += 1
                # ä½¿ç”¨ä½ è¦æ±‚çš„æ—¥èªŒæ ¼å¼
                logger.info(
                    f"{{'{po}'}} æ›´æ”¹ -> item: {item_for_log}, "
                    f"é©—æ”¶æ•¸é‡: {log_info.get('é©—æ”¶æ•¸é‡', {}).get('new', row.get('é©—æ”¶æ•¸é‡', ''))}, "
                    f"æ‹’æ”¶æ•¸é‡: {log_info.get('æ‹’æ”¶æ•¸é‡', {}).get('new', row.get('æ‹’æ”¶æ•¸é‡', ''))}, "
                    f"ç™¼ç¥¨æœˆä»½: {log_info.get('ç™¼ç¥¨æœˆä»½', {}).get('new', row.get('ç™¼ç¥¨æœˆä»½', ''))}"
                )

        # === ç§»é™¤ä¸­ç¹¼æ¬„ä½ ===
        merged.drop(columns=[c for c in merged.columns if c.endswith("_clean") or c.endswith("_from_output")], inplace=True)

        # === æœ€çµ‚è¼¸å‡ºæ¬„ä½æ¸…å–® ===
        final_columns = ['Id', 'é–‹å–®ç‹€æ…‹', 'äº¤è²¨é©—è­‰', 'User', 'ePR No.', 'PO No.', 'Item', 'å“é …', 'è¦æ ¼', 'æ•¸é‡', 'ç¸½æ•¸', 'å–®åƒ¹', 'ç¸½åƒ¹', 'å‚™è¨»', 'å­—æ•¸', 'isEditing', 'backup', '_alertedItemLimit', 
                         'Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ', 'SOD Qty å» å•†æ‰¿è«¾æ•¸é‡', 'é©—æ”¶æ•¸é‡', 'æ‹’æ”¶æ•¸é‡', 'ç™¼ç¥¨æœˆä»½', "WBS", "éœ€æ±‚æ—¥", "RTé‡‘é¡", "RTç¸½é‡‘é¡", "é©—æ”¶ç‹€æ…‹"]

        # ç¢ºä¿åªä¿ç•™éœ€è¦çš„æ¬„ä½
        final_df = merged[final_columns].copy()

        # === å„²å­˜æ›´æ–°å¾Œçš„æª”æ¡ˆ ===
        final_df.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")

        print(f"ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚")
        print(f"æª”æ¡ˆå·²æ›´æ–°ã€‚ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚")
        os.remove("static/data/delivery_receipt.csv")

        return jsonify({"status": "success", "msg": f"æª”æ¡ˆå·²æ›´æ–°ã€‚ç¸½å…±æ›´æ–°äº† {update_count} ç­†è³‡æ–™ã€‚"})
    except Exception as e:
        traceback.print_exc()  # å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š
        return jsonify({"status": "error", "message": str(e)}), 500
    

@app.route("/api/buyer_detail", methods=["GET"])
def get_buyer_details():
    """
    è®€å– Buyer_detail.csv æª”æ¡ˆä¸¦ä»¥ JSON æ ¼å¼å›å‚³ã€‚
    """
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(BUYER_FILE):
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {BUYER_FILE}")
        return jsonify({"error": f"æ‰¾ä¸åˆ°æª”æ¡ˆ {BUYER_FILE}"}), 500
        
    try:
        # ä½¿ç”¨ pandas è®€å– CSV æª”æ¡ˆ
        df = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str)
        
        # ç¢ºä¿æ¬„ä½åç¨±æ²’æœ‰å¤šé¤˜çš„ç©ºç™½
        df.columns = df.columns.str.strip()

        # === ä¿®æ­£ï¼šå°‡ DataFrame ä¸­çš„ NaN å€¼æ›¿æ›ç‚º None ===
        # è®€å– CSV æ™‚ dtype=str æœƒå°‡ NaN è®€æˆå­—ä¸² 'nan'ã€‚
        df = df.replace({np.nan: None, 'nan': None})
        
        # å°‡ DataFrame è½‰æ›æˆ JSON æ ¼å¼çš„åˆ—è¡¨
        data_json = df.to_dict('records')
        
        return jsonify(data_json)
    except Exception as e:
        print(f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return jsonify({"error": f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}), 500

# # åœ¨ app.py ä¸­æ–°å¢ä»¥ä¸‹ API ç«¯é»

# ACCOUNTING_SUMMARY_FILE = "static/data/accounting_summary.json"
# MONTHLY_ACTUAL_ACCOUNTING_FILE = "static/data/monthly_actual_accounting.json"

# @app.route('/api/accounting-summary', methods=['GET'])
# def get_accounting_summary():
#     """
#     ç²å–å°šæœªå…¥å¸³çš„æœˆä»½é¸é …å’Œé‡‘é¡
#     """
#     try:
#         if os.path.exists(ACCOUNTING_SUMMARY_FILE):
#             with open(ACCOUNTING_SUMMARY_FILE, 'r', encoding='utf-8') as f:
#                 data = json.load(f)
#             return jsonify(data)
#         else:
#             return jsonify({})
#     except Exception as e:
#         print(f"âŒ è®€å– accounting_summary.json éŒ¯èª¤: {e}")
#         return jsonify({'error': str(e)}), 500

# @app.route('/api/monthly-actual-accounting', methods=['GET'])
# def get_monthly_actual_accounting():
#     """
#     ç²å–æ¯æœˆå¯¦éš›å…¥å¸³é‡‘é¡
#     """
#     try:
#         if os.path.exists(MONTHLY_ACTUAL_ACCOUNTING_FILE):
#             with open(MONTHLY_ACTUAL_ACCOUNTING_FILE, 'r', encoding='utf-8') as f:
#                 data = json.load(f)
#             return jsonify(data)
#         else:
#             return jsonify({})
#     except Exception as e:
#         print(f"âŒ è®€å– monthly_actual_accounting.json éŒ¯èª¤: {e}")
#         return jsonify({'error': str(e)}), 500






# ç‰©æ–™æ”¶è²¨å–®
from datetime import datetime
import hashlib
from bs4 import BeautifulSoup
import email
from email import policy
from email.parser import BytesParser
import logging
# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Buyer CSV è·¯å¾‘è¨­å®šï¼ˆèˆ‡ app.py åŒå±¤ç´šï¼‰
BUYER_CSV_PATH = 'static/data/Buyer_detail_updated.csv'

# è¨­å®š
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB æª”æ¡ˆå¤§å°é™åˆ¶
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['PROCESSED_FOLDER'] = 'processed'
app.config['ALLOWED_EXTENSIONS'] = {'mhtml'}  

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
os.makedirs(app.config['PROCESSED_FOLDER'], exist_ok=True)

def allowed_file(filename):
    """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºå…è¨±çš„æ ¼å¼"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def generate_unique_filename(original_name, new_name):
    """ç”Ÿæˆå”¯ä¸€çš„æª”å"""
    # å–å¾—å‰¯æª”å
    ext = original_name.rsplit('.', 1)[1].lower()
    
    # å¦‚æœæ–°æª”åæ²’æœ‰å‰¯æª”åï¼ŒåŠ ä¸ŠåŸå§‹å‰¯æª”å
    if not new_name.endswith(f'.{ext}'):
        new_name = f"{new_name}.{ext}"
    
    # ç”Ÿæˆå”¯ä¸€è­˜åˆ¥ç¢¼
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    unique_id = hashlib.md5(f"{new_name}{timestamp}".encode()).hexdigest()[:8]
    
    # çµ„åˆæœ€çµ‚æª”å
    name_without_ext = new_name.rsplit('.', 1)[0]
    final_name = f"{name_without_ext}_{unique_id}.{ext}"
    
    return secure_filename(final_name)

# åŒ¯å…¥ MHTML è§£æå™¨é¡åˆ¥
from Mhtml_parser.mhtml_parser import MHTMLParser

@app.route('/api/upload-mhtml', methods=['POST'])
def upload_mhtml():
    """è™•ç† MHTML æª”æ¡ˆä¸Šå‚³ä¸¦è‡ªå‹•èˆ‡ Buyer CSV æ¯”å°"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰æª”æ¡ˆ
        if 'file' not in request.files:
            logger.error("No file in request")
            return jsonify({'success': False, 'error': 'æ²’æœ‰æª”æ¡ˆè¢«ä¸Šå‚³'}), 400
        
        file = request.files['file']
        new_name = request.form.get('newName', '')
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦ç‚ºç©º
        if file.filename == '':
            logger.error("Empty filename")
            return jsonify({'success': False, 'error': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'}), 400
        
        # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
        if not allowed_file(file.filename):
            logger.error(f"Invalid file type: {file.filename}")
            return jsonify({'success': False, 'error': 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼'}), 400
        
        # ç”Ÿæˆæ–°çš„å”¯ä¸€æª”å
        if new_name:
            filename = generate_unique_filename(file.filename, new_name)
        else:
            filename = generate_unique_filename(file.filename, file.filename)
        
        logger.info(f"Processing file: {filename}")
        
        # å„²å­˜æª”æ¡ˆåˆ°ä¸Šå‚³è³‡æ–™å¤¾
        upload_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(upload_path)
        
        # ç§»å‹•ä¸¦é‡æ–°å‘½åæª”æ¡ˆåˆ°è™•ç†è³‡æ–™å¤¾
        processed_path = os.path.join(app.config['PROCESSED_FOLDER'], filename)
        
        # å¦‚æœç›®æ¨™æª”æ¡ˆå·²å­˜åœ¨ï¼Œå…ˆåˆªé™¤
        if os.path.exists(processed_path):
            os.remove(processed_path)
        
        os.rename(upload_path, processed_path)
        
        # è§£æ MHTML æª”æ¡ˆ
        parser = MHTMLParser(processed_path)
        extracted_info = parser.parse()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è§£æéŒ¯èª¤
        if 'error' in extracted_info:
            logger.error(f"Parser error: {extracted_info['error']}")
            # å³ä½¿æœ‰éŒ¯èª¤ï¼Œé‚„æ˜¯è¿”å›éƒ¨åˆ†è³‡è¨Š
        
        # åŠ å…¥æª”æ¡ˆè³‡è¨Š
        file_stats = os.stat(processed_path)
        extracted_info.update({
            'original_filename': file.filename,
            'saved_filename': filename,
            'file_size': file_stats.st_size,
            'upload_time': datetime.now().isoformat(),
            'file_path': processed_path
        })
        
        # æ ¼å¼åŒ–æª”æ¡ˆå¤§å°
        size = file_stats.st_size
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                extracted_info['file_size_formatted'] = f"{size:.2f} {unit}"
                break
            size /= 1024.0
        
        # å¦‚æœæœ‰ GridView è³‡æ–™ï¼Œå¦å¤–å„²å­˜ç‚º JSON
        if 'gridview_data' in extracted_info and extracted_info['gridview_data']:
            json_filename = filename.rsplit('.', 1)[0] + '_gridview.json'
            json_path = os.path.join(app.config['PROCESSED_FOLDER'], json_filename)
            
            try:
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(extracted_info['gridview_data'], f, ensure_ascii=False, indent=2)
                
                extracted_info['gridview_json_file'] = json_filename
                logger.info(f"GridView data saved to: {json_filename}")
                
                # è‡ªå‹•èˆ‡ Buyer_detail.csv æ¯”å°
                if os.path.exists(BUYER_CSV_PATH):
                    logger.info("é–‹å§‹èˆ‡ Buyer_detail.csv æ¯”å°...")
                    comparison_result = compare_with_buyer_csv(extracted_info['gridview_data'])
                    extracted_info['comparison_result'] = comparison_result
                else:
                    logger.warning(f"æ‰¾ä¸åˆ° {BUYER_CSV_PATH}")
                    extracted_info['comparison_result'] = {
                        'error': f'æ‰¾ä¸åˆ° {BUYER_CSV_PATH} æª”æ¡ˆ',
                        'needs_buyer_csv': True
                    }
                    
            except Exception as e:
                logger.error(f"Failed to save GridView JSON or compare: {str(e)}")
        
        return jsonify({
            'success': True,
            'message': 'æª”æ¡ˆä¸Šå‚³ä¸¦è§£ææˆåŠŸ',
            'data': extracted_info
        })
        
    except Exception as e:
        logger.error(f"ä¸Šå‚³éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'
        }), 500

def compare_with_buyer_csv(gridview_data):
    """èˆ‡ Buyer CSV æ¯”å° PO No. å’Œå“å"""
    try:
        # è®€å– Buyer CSVï¼ˆå˜—è©¦ä¸åŒç·¨ç¢¼ï¼‰
        try:
            buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig')
        except:
            try:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8')
            except:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='big5')
        
        logger.info(f"Buyer CSV columns: {list(buyer_df.columns)}")
        logger.info(f"Buyer CSV shape: {buyer_df.shape}")
        
        # è§£æ GridView è³‡æ–™
        headers = gridview_data.get('headers', [])
        rows = gridview_data.get('rows', [])
        
        logger.info(f"GridView headers: {headers}")
        logger.info(f"GridView rows count: {len(rows)}")
        
        # æ‰¾å‡ºé—œéµæ¬„ä½çš„ç´¢å¼•
        po_index = -1
        desc_index = -1
        qty_index = -1
        accept_qty_index = -1
        amount_index = -1
        
        for i, header in enumerate(headers):
            header_text = header.get('en', '').lower() or header.get('zh', '').lower() or header.get('full', '').lower()
            # PO No. æ¬„ä½ï¼ˆé¿å…åŒ¹é…åˆ° Demo POè™Ÿï¼‰
            if ('po' in header_text and 'no' in header_text) or 'poè™Ÿç¢¼' in header_text:
                if 'demo' not in header_text.lower():  # æ’é™¤ Demo POè™Ÿ
                    po_index = i
                    logger.info(f"Found PO index at {i}: {header}")
            elif 'description' in header_text or 'å“å' in header_text:
                desc_index = i
                logger.info(f"Found Description index at {i}: {header}")
            elif 'accept' in header_text and 'qty' in header_text or 'é©—æ”¶æ•¸é‡' in header_text:
                accept_qty_index = i
            elif 'qty' in header_text or 'æ•¸é‡' in header_text or 'æ”¶è²¨æ•¸é‡' in header_text:
                if accept_qty_index == -1:
                    qty_index = i
            elif 'amount' in header_text or 'ç¸½é‡‘é¡' in header_text:
                amount_index = i
        
        # å…ˆæå–æ‰€æœ‰ PO è™Ÿç¢¼ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å¤šå€‹ä¸åŒçš„ PO
        po_numbers = []
        for row_idx, row in enumerate(rows):
            raw_data = row.get('raw_data', [])
            
            if po_index >= 0 and po_index < len(raw_data):
                po_no = str(raw_data[po_index].get('value', '')).strip()
                logger.info(f"Row {row_idx} - Extracted PO: '{po_no}'")
                if po_no:  # åªè¨˜éŒ„éç©ºçš„ PO è™Ÿç¢¼
                    po_numbers.append(po_no)
            else:
                logger.warning(f"Row {row_idx} - PO index {po_index} out of range")
        
        # æª¢æŸ¥ PO è™Ÿç¢¼çš„å”¯ä¸€æ€§
        unique_pos = list(set(po_numbers))
        logger.info(f"ç™¼ç¾çš„å”¯ä¸€ PO è™Ÿç¢¼: {unique_pos}")
        
        if len(unique_pos) > 1:
            # æœ‰å¤šå€‹ä¸åŒçš„ PO è™Ÿç¢¼ï¼Œè¨ˆç®—æ¯å€‹ PO çš„å‡ºç¾æ¬¡æ•¸
            po_counts = {}
            for po in po_numbers:
                po_counts[po] = po_counts.get(po, 0) + 1
            
            # æŒ‰å‡ºç¾æ¬¡æ•¸æ’åºï¼Œæ‰¾å‡ºæœ€å¤šçš„ PO
            sorted_pos = sorted(po_counts.items(), key=lambda x: x[1], reverse=True)
            most_common_po = sorted_pos[0][0]
            most_common_count = sorted_pos[0][1]
            
            logger.warning(f"ç™¼ç¾å¤šå€‹ä¸åŒçš„ PO è™Ÿç¢¼: {po_counts}")
            logger.warning(f"æœ€å¸¸å‡ºç¾çš„ PO: {most_common_po} (å‡ºç¾ {most_common_count} æ¬¡)")
            
            # åˆ¤æ–·æ˜¯å¦æ‡‰è©²å ±éŒ¯ - å¦‚æœæ²’æœ‰æ˜é¡¯çš„å¤šæ•¸ï¼Œå°±å ±éŒ¯
            total_count = len(po_numbers)
            majority_threshold = total_count / 2
            
            if most_common_count <= majority_threshold:
                # æ²’æœ‰æ˜é¡¯å¤šæ•¸ï¼Œå ±éŒ¯
                error_msg = f"ç™¼ç¾å¤šå€‹ä¸åŒçš„ PO è™Ÿç¢¼ï¼Œç„¡æ³•ç¢ºå®šæ­£ç¢ºçš„ PO: {dict(po_counts)}"
                logger.error(error_msg)
                return {
                    'error': error_msg,
                    'error_type': 'multiple_po_no_majority',
                    'po_counts': po_counts,
                    'items': [],
                    'summary': {}
                }
            else:
                # æœ‰æ˜é¡¯å¤šæ•¸ï¼Œç™¼å‡ºè­¦å‘Šä½†ç¹¼çºŒè™•ç†
                logger.warning(f"ä½¿ç”¨å¤šæ•¸æ±º PO è™Ÿç¢¼: {most_common_po}")
        
        # å»ºç«‹æ¯”å°çµæœ
        comparison_items = []
        total_amount = 0
        
        # åˆ—å‡ºç¬¬ä¸€ç­†è³‡æ–™çš„åŸå§‹å…§å®¹ä»¥é™¤éŒ¯
        if len(rows) > 0:
            first_row = rows[0].get('raw_data', [])
            logger.info(f"First row raw data length: {len(first_row)}")
            for i, cell in enumerate(first_row[:10]):  # åªåˆ—å‰10å€‹æ¬„ä½
                logger.info(f"Cell {i}: {cell.get('value', 'NO VALUE')}")
        
        for row_idx, row in enumerate(rows):
            item = {}
            raw_data = row.get('raw_data', [])
            
            # æå–è³‡æ–™
            if po_index >= 0 and po_index < len(raw_data):
                item['po_no'] = str(raw_data[po_index].get('value', '')).strip()
                logger.info(f"Row {row_idx} - Extracted PO from index {po_index}: '{item['po_no']}'")
            else:
                logger.warning(f"Row {row_idx} - PO index {po_index} out of range (row has {len(raw_data)} cells)")
                
            if desc_index >= 0 and desc_index < len(raw_data):
                item['description'] = str(raw_data[desc_index].get('value', '')).strip()
                logger.info(f"Row {row_idx} - Extracted Description from index {desc_index}: '{item['description']}'")
                
            if qty_index >= 0 and qty_index < len(raw_data):
                item['qty'] = raw_data[qty_index].get('value', '')
                
            if accept_qty_index >= 0 and accept_qty_index < len(raw_data):
                item['accept_qty'] = raw_data[accept_qty_index].get('value', '')
            elif qty_index >= 0:
                item['accept_qty'] = item.get('qty', '0')
                
            if amount_index >= 0 and amount_index < len(raw_data):
                item['amount'] = raw_data[amount_index].get('value', '')
            
            # è¨ˆç®— RT é‡‘é¡å’Œ RT ç¸½é‡‘é¡
            try:
                amount_str = str(item.get('amount', '0'))
                amount_str = amount_str.replace(',', '').replace('$', '').replace('TWD', '').strip()
                amount_value = float(amount_str) if amount_str and amount_str != '-' else 0
                
                accept_qty_str = str(item.get('accept_qty', '0'))
                accept_qty_str = accept_qty_str.replace(',', '').strip()
                accept_qty_value = float(accept_qty_str) if accept_qty_str and accept_qty_str != '0' and accept_qty_str != '-' else 1
                
                item['rt_amount'] = amount_value / accept_qty_value if accept_qty_value > 0 else 0
                item['rt_total_amount'] = amount_value
                
                total_amount += amount_value
            except Exception as e:
                logger.error(f"è¨ˆç®— RT é‡‘é¡éŒ¯èª¤: {str(e)}")
                item['rt_amount'] = 0
                item['rt_total_amount'] = 0
            
            # åœ¨ Buyer CSV ä¸­å°‹æ‰¾åŒ¹é…
            matched_in_buyer = False
            buyer_row_index = -1
            
            if item.get('po_no'):
                # åœ¨ Buyer CSV çš„ PO No. æ¬„ä½ä¸­å°‹æ‰¾
                for idx, buyer_row in buyer_df.iterrows():
                    buyer_po = str(buyer_row.get('PO No.', '')).strip()
                    
                    if buyer_po == item['po_no']:
                        matched_in_buyer = True
                        buyer_row_index = idx
                        logger.info(f"âœ“ Matched PO {item['po_no']} at Buyer CSV row {idx}")
                        logger.debug(f"æ‰€æœ‰æ¬„ä½: {buyer_row.to_dict()}")
                        # æª¢æŸ¥å“åæ˜¯å¦ä¹ŸåŒ¹é…
                        col_item = next((col for col in buyer_row.index if col.strip().replace(' ', '') in ['Item']), 'Item')
                        col_desc = next((col for col in buyer_row.index if col.strip().replace(' ', '') in ['å“é …', 'å“å']), 'å“é …')

                        buyer_item = str(buyer_row.get(col_item, '')).strip()
                        buyer_desc = str(buyer_row.get(col_desc, '')).strip()
                        
                        if item.get('description'):
                            if buyer_item != item['description'] and buyer_desc != item['description']:
                                logger.warning(f"PO matched but description different: Buyer='{buyer_item}' or '{buyer_desc}', GridView='{item['description']}'")
                        break
                
                if not matched_in_buyer:
                    logger.warning(f"âœ— PO {item['po_no']} not found in Buyer CSV")
            else:
                logger.warning(f"Row {row_idx} has no PO number")
            
            item['matched_in_buyer'] = matched_in_buyer
            item['buyer_row_index'] = buyer_row_index
            comparison_items.append(item)
        
        # çµ±è¨ˆçµæœ
        matched_count = sum(1 for item in comparison_items if item['matched_in_buyer'])
        unmatched_count = len(comparison_items) - matched_count
        
        logger.info(f"æ¯”å°çµæœ: ç¸½é …ç›®={len(comparison_items)}, åŒ¹é…={matched_count}, æœªåŒ¹é…={unmatched_count}")
        
        return {
            'items': comparison_items,
            'summary': {
                'total_items': len(comparison_items),
                'matched_count': matched_count,
                'unmatched_count': unmatched_count,
                'total_amount': total_amount,
                'buyer_csv_rows': len(buyer_df),
                'buyer_csv_columns': list(buyer_df.columns)
            }
        }
        
    except Exception as e:
        logger.error(f"æ¯”å°éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'error': str(e),
            'items': [],
            'summary': {}
        }



# åœ¨ app.py ä¸­æ–°å¢ä»¥ä¸‹è·¯ç”±å’ŒåŠŸèƒ½

import shutil

@app.route('/api/cleanup-processed', methods=['POST'])
def cleanup_processed():
    """æ¸…ç† processed è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    try:
        processed_folder = app.config['PROCESSED_FOLDER']
        upload_folder = app.config['UPLOAD_FOLDER']
        
        total_files_removed = 0
        
        # æ¸…ç† processed è³‡æ–™å¤¾
        if os.path.exists(processed_folder):
            files_before = os.listdir(processed_folder)
            if files_before:
                for filename in files_before:
                    file_path = os.path.join(processed_folder, filename)
                    try:
                        if os.path.isfile(file_path) or os.path.islink(file_path):
                            os.unlink(file_path)
                            total_files_removed += 1
                            logger.info(f"å·²åˆªé™¤ processed æ–‡ä»¶: {filename}")
                        elif os.path.isdir(file_path):
                            shutil.rmtree(file_path)
                            total_files_removed += 1
                            logger.info(f"å·²åˆªé™¤ processed ç›®éŒ„: {filename}")
                    except Exception as e:
                        logger.error(f"ç„¡æ³•åˆªé™¤ processed æ–‡ä»¶ {file_path}: {str(e)}")
        
        # åŒæ™‚æ¸…ç† upload è³‡æ–™å¤¾ï¼ˆé é˜²æªæ–½ï¼‰
        if os.path.exists(upload_folder):
            upload_files = os.listdir(upload_folder)
            if upload_files:
                for filename in upload_files:
                    file_path = os.path.join(upload_folder, filename)
                    try:
                        if os.path.isfile(file_path) or os.path.islink(file_path):
                            os.unlink(file_path)
                            total_files_removed += 1
                            logger.info(f"å·²åˆªé™¤ upload æ–‡ä»¶: {filename}")
                        elif os.path.isdir(file_path):
                            shutil.rmtree(file_path)
                            total_files_removed += 1
                            logger.info(f"å·²åˆªé™¤ upload ç›®éŒ„: {filename}")
                    except Exception as e:
                        logger.error(f"ç„¡æ³•åˆªé™¤ upload æ–‡ä»¶ {file_path}: {str(e)}")
        
        # é©—è­‰æ¸…ç†çµæœ
        processed_files_after = os.listdir(processed_folder) if os.path.exists(processed_folder) else []
        upload_files_after = os.listdir(upload_folder) if os.path.exists(upload_folder) else []
        
        logger.info(f"æ¸…ç†å®Œæˆ: åˆªé™¤äº† {total_files_removed} å€‹æ–‡ä»¶")
        
        return jsonify({
            'success': True,
            'message': f'æˆåŠŸæ¸…ç† {total_files_removed} å€‹æ–‡ä»¶',
            'files_removed': total_files_removed,
            'remaining_processed_files': len(processed_files_after),
            'remaining_upload_files': len(upload_files_after)
        })
        
    except Exception as e:
        logger.error(f"æ¸…ç†è³‡æ–™å¤¾éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'æ¸…ç†å¤±æ•—: {str(e)}'
        }), 500

# ä¿®æ”¹ç¾æœ‰çš„ update_buyer_csv å‡½æ•¸ï¼Œåœ¨æˆåŠŸæ›´æ–°å¾Œè‡ªå‹•è§¸ç™¼æ¸…ç†
@app.route('/api/update-buyer-csv', methods=['POST'])
def update_buyer_csv():
    # å‚™ä»½
    backup_files()
    """æ›´æ–° Buyer CSV æª”æ¡ˆä¸­çš„ RT é‡‘é¡"""
    try:
        data = request.json
        items_to_update = data.get('items', []) # type: ignore
        
        if not items_to_update:
            return jsonify({'success': False, 'error': 'æ²’æœ‰è¦æ›´æ–°çš„é …ç›®'}), 400
        
        logger.info(f"æº–å‚™æ›´æ–° {len(items_to_update)} ç­†è³‡æ–™")
        
        # è®€å– Buyer CSV
        try:
            buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype=str)
        except:
            try:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8', dtype=str)
            except:
                buyer_df = pd.read_csv(BUYER_CSV_PATH, encoding='big5', dtype=str)
        
        logger.info(f"Buyer CSV åŸå§‹æ¬„ä½: {list(buyer_df.columns)}")
        
        # ç¢ºä¿æœ‰ RT ç›¸é—œæ¬„ä½
        if 'RTé‡‘é¡' not in buyer_df.columns:
            buyer_df['RTé‡‘é¡'] = ''
            logger.info("æ–°å¢ RTé‡‘é¡ æ¬„ä½")
        if 'RTç¸½é‡‘é¡' not in buyer_df.columns:
            buyer_df['RTç¸½é‡‘é¡'] = ''
            logger.info("æ–°å¢ RTç¸½é‡‘é¡ æ¬„ä½")
        
        updated_count = 0
        
        # æ›´æ–°æ¯å€‹é …ç›®
        for item in items_to_update:
            logger.info(items_to_update)
            po_no = str(item.get('po_no', '')).strip()
            description = str(item.get('description', '')).strip()
            
            # å°‡é‡‘é¡è½‰æ›ç‚ºæ•´æ•¸å­—ä¸²ï¼ˆç§»é™¤å°æ•¸é»ï¼‰
            rt_amount = item.get('rt_amount', 0)
            rt_total_amount = item.get('rt_total_amount', 0)
            
            try:
                rt_amount_str = str(int(round(float(rt_amount))))
                rt_total_amount_str = str(int(round(float(rt_total_amount))))
            except:
                rt_amount_str = '0'
                rt_total_amount_str = '0'
            
            logger.info(f"å˜—è©¦æ›´æ–°: PO={po_no}, å“å={description}, RTé‡‘é¡={rt_amount_str}, RTç¸½é‡‘é¡={rt_total_amount_str}")
            
            if po_no:
                mask_po = buyer_df['PO No.'].astype(str).str.strip() == po_no

                if 'å“é …' in buyer_df.columns:
                    mask_desc = buyer_df['å“é …'].astype(str).str.strip() == description
                    mask_both = mask_po & mask_desc
                else:
                    mask_desc = pd.Series([False] * len(buyer_df))
                    mask_both = mask_po  # fallback

                if mask_both.sum() > 0:
                    buyer_df.loc[mask_both, 'RTé‡‘é¡'] = rt_amount_str
                    buyer_df.loc[mask_both, 'RTç¸½é‡‘é¡'] = rt_total_amount_str
                    updated_count += mask_both.sum()
                    logger.info(f"âœ“ å®Œæ•´åŒ¹é… PO={po_no}, å“é …={description} â†’ æˆåŠŸæ›´æ–°")
                elif mask_po.sum() > 0:
                    buyer_df.loc[mask_po, 'RTé‡‘é¡'] = rt_amount_str
                    buyer_df.loc[mask_po, 'RTç¸½é‡‘é¡'] = rt_total_amount_str
                    updated_count += mask_po.sum()
                    logger.warning(f"âš ï¸ PO={po_no} åœ¨ Buyer CSV ä¸­æœ‰ {mask_po.sum()} ç­†ï¼Œä½†å“é …ä¸åŒ (GridView: {description}, Buyer: {buyer_df.loc[mask_po, 'å“é …'].unique().tolist()})ï¼Œä»å¼·åˆ¶æ›´æ–° RT é‡‘é¡")
                else:
                    logger.warning(f"âœ— åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ° PO {po_no}")
        
        # å„²å­˜æ›´æ–°å¾Œçš„ CSV
        if updated_count > 0:
            try:
                buyer_df.to_csv(BUYER_CSV_PATH, index=False, encoding='utf-8-sig', na_rep='')
                logger.info(f"æˆåŠŸå„²å­˜æ›´æ–°å¾Œçš„ Buyer CSVï¼Œå…±æ›´æ–° {updated_count} ç­†è³‡æ–™")
                
                # é©—è­‰æ›´æ–°æ˜¯å¦æˆåŠŸ
                verify_df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype=str)
                logger.info(f"é©—è­‰: æ›´æ–°å¾Œçš„ CSV æœ‰ {len(verify_df)} ç­†è³‡æ–™")
                
                # æª¢æŸ¥ç‰¹å®š PO çš„æ›´æ–°çµæœ
                for item in items_to_update[:3]:
                    po_no = str(item.get('po_no', '')).strip()
                    if po_no:
                        verify_rows = verify_df[verify_df['PO No.'].astype(str).str.strip() == po_no]
                        if len(verify_rows) > 0:
                            logger.info(f"é©—è­‰ PO {po_no}ï¼šå…±æœ‰ {len(verify_rows)} ç­†è³‡æ–™")
                            for idx, row in verify_rows.iterrows():
                                logger.info(f"- å“é …={row.get('å“é …', '')}, RTé‡‘é¡={row.get('RTé‡‘é¡', '')}, RTç¸½é‡‘é¡={row.get('RTç¸½é‡‘é¡', '')}")
                                
                return jsonify({
                    'success': True,
                    'message': f'æˆåŠŸæ›´æ–° {int(updated_count)} ç­†è³‡æ–™',
                    'updated_count': int(updated_count),
                    'total_items': int(len(items_to_update)),
                    'should_cleanup': True,  # ç¢ºä¿æ·»åŠ æ¨™è¨˜å‘Šè¨´å‰ç«¯éœ€è¦æ¸…ç†
                    'cleanup_required': True  # é¡å¤–çš„æ¸…ç†æ¨™è¨˜
                })
                
            except Exception as e:
                logger.error(f"å„²å­˜ CSV å¤±æ•—: {str(e)}")
                return jsonify({
                    'success': False,
                    'error': f'å„²å­˜æª”æ¡ˆå¤±æ•—: {str(e)}'
                }), 500
        else:
            # æä¾›æ›´è©³ç´°çš„éŒ¯èª¤è³‡è¨Š
            logger.warning("æ²’æœ‰ä»»ä½•è³‡æ–™è¢«æ›´æ–°")
            
            # çµ±è¨ˆå„ç¨®æƒ…æ³
            empty_po_count = sum(1 for item in items_to_update if not str(item.get('po_no', '')).strip())
            valid_po_count = len(items_to_update) - empty_po_count
            
            error_details = {
                'total_items': len(items_to_update),
                'valid_po_count': valid_po_count,
                'empty_po_count': empty_po_count,
                'updated_count': 0
            }
            
            if empty_po_count > 0:
                error_message = f'å…± {len(items_to_update)} ç­†è³‡æ–™ä¸­æœ‰ {empty_po_count} ç­†ç¼ºå°‘ PO è™Ÿç¢¼ï¼Œ{valid_po_count} ç­†åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ°åŒ¹é…é …ç›®'
            else:
                error_message = f'åœ¨ Buyer CSV ä¸­æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…çš„ PO è™Ÿç¢¼ (å…±æª¢æŸ¥ {len(items_to_update)} ç­†)'
            
            return jsonify({
                'success': False,
                'message': error_message,
                'error': 'å¾Œå°æŸ¥ç„¡æ­¤è³‡æ–™ï¼Œè«‹æª¢æŸ¥ PO è™Ÿç¢¼æ˜¯å¦æ­£ç¢º',
                'details': error_details,
                'updated_count': 0
            })
        
    except Exception as e:
        logger.error(f"æ›´æ–° Buyer CSV éŒ¯èª¤: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': str(e)}), 500





from flask import Flask, send_file, jsonify, Response
from io import BytesIO
import tempfile

@app.route('/api/download_buyer_detail_xlsx', methods=['GET'])
def download_buyer_detail_xlsx():
    try:
        # æ–¹æ³•1ï¼šç›´æ¥å¾è³‡æ–™åº«æˆ– JSON ç²å–è³‡æ–™ï¼ˆæ¨è–¦ï¼‰
        # é€™æ¨£å¯ä»¥é¿å… CSV æª”æ¡ˆä¸­çš„æ ¼å¼å•é¡Œ
        
        # å¦‚æœä½ æœ‰è³‡æ–™åº«é€£æ¥ï¼Œä½¿ç”¨é€™å€‹æ–¹æ³•ï¼š
        # df = pd.read_sql("SELECT * FROM buyer_detail", connection)
        
        # å¦‚æœä½ æƒ³å¾ç¾æœ‰çš„ API ç«¯é»ç²å–è³‡æ–™ï¼š
        try:
            # å¾ä½ ç¾æœ‰çš„ API ç«¯é»ç²å– JSON è³‡æ–™
            import requests
            response = requests.get('http://127.0.0.1:5000/api/buyer_detail')
            if response.status_code == 200:
                data = response.json()
                df = pd.DataFrame(data)
            else:
                raise Exception("ç„¡æ³•å¾ API ç²å–è³‡æ–™")
        except:
            # å‚™ç”¨æ–¹æ¡ˆï¼šè®€å– CSV æª”æ¡ˆ
            csv_file_path = './data/BUYER_DETAIL_RT_Table.csv'
            
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼æ–¹å¼
            encodings = ['utf-8', 'big5', 'gbk', 'cp950']
            df = None
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(csv_file_path, encoding=encoding)
                    print(f"æˆåŠŸä½¿ç”¨ {encoding} ç·¨ç¢¼è®€å–æª”æ¡ˆ")
                    break
                except UnicodeDecodeError:
                    continue
            
            if df is None:
                return jsonify({'error': 'ç„¡æ³•è®€å–è³‡æ–™æª”æ¡ˆï¼Œç·¨ç¢¼å•é¡Œ'}), 500
        
        # å®šç¾©éœ€è¦çš„æ¬„ä½ï¼ˆæŒ‰é †åºï¼‰
        required_columns = [
            "äº¤è²¨é©—è­‰",
            "é©—æ”¶ç‹€æ…‹", 
            "ePR No.",
            "PO No.",
            "Item",
            "å“é …",
            "è¦æ ¼",
            "æ•¸é‡",
            "ç¸½æ•¸",
            "å–®åƒ¹",
            "ç¸½åƒ¹",
            "RTé‡‘é¡",
            "RTç¸½é‡‘é¡",
            "å‚™è¨»",
            "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ",
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡",
            "é©—æ”¶æ•¸é‡",
            "æ‹’æ”¶æ•¸é‡",
            "ç™¼ç¥¨æœˆä»½",
            "WBS",
            "éœ€æ±‚æ—¥"
        ]
        
        # åªé¸æ“‡éœ€è¦çš„æ¬„ä½ï¼Œä¸¦æŒ‰æŒ‡å®šé †åºæ’åˆ—
        available_columns = [col for col in required_columns if col in df.columns]
        df_filtered = df[available_columns].copy()
        
        # æ¸…ç†è³‡æ–™
        def clean_cell_value(value):
            if pd.isna(value) or value is None:
                return ''
            
            # è½‰æ›ç‚ºå­—ç¬¦ä¸²
            str_value = str(value)
            
            # ç§»é™¤ Excel ä¸æ”¯æ´çš„æ§åˆ¶å­—ç¬¦
            # Excel ä¸æ”¯æ´ ASCII 0-31 çš„æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº† 9=tab, 10=LF, 13=CRï¼‰
            cleaned = ''.join(char for char in str_value 
                            if ord(char) >= 32 or char in ['\t', '\n', '\r'])
            
            # æ›¿æ›å¯èƒ½é€ æˆå•é¡Œçš„å­—ç¬¦
            cleaned = cleaned.replace('\n', ' ').replace('\r', ' ')
            
            # é™åˆ¶å–®å…ƒæ ¼å…§å®¹é•·åº¦ï¼ˆExcel é™åˆ¶ç‚º 32,767 å­—ç¬¦ï¼‰
            if len(cleaned) > 32000:
                cleaned = cleaned[:32000] + '...'
            
            return cleaned.strip()
        
        # æ‡‰ç”¨æ¸…ç†å‡½æ•¸åˆ°ç¯©é¸å¾Œçš„è³‡æ–™
        for column in df_filtered.columns:
            df_filtered[column] = df_filtered[column].apply(clean_cell_value)
        
        # ä½¿ç”¨ç¯©é¸å¾Œçš„è³‡æ–™
        df = df_filtered
        
        # å‰µå»º Excel æª”æ¡ˆ
        excel_buffer = BytesIO()
        
        try:
            # ä½¿ç”¨ xlsxwriter å¼•æ“ï¼ˆæ›´ç©©å®šï¼‰
            with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
                # å¯«å…¥è³‡æ–™
                df.to_excel(writer, sheet_name='eRTé©—æ”¶æ˜ç´°', index=False)
                
                # ç²å–å·¥ä½œè¡¨å’Œå·¥ä½œç°¿ç‰©ä»¶
                workbook = writer.book
                worksheet = writer.sheets['eRTé©—æ”¶æ˜ç´°']
                
                # è¨­å®šæ¨™é¡Œæ ¼å¼
                header_format = workbook.add_format({ # type: ignore
                    'bold': True,
                    'bg_color': '#D7E4BC',
                    'border': 1,
                    'align': 'center',
                    'valign': 'vcenter'
                })
                
                # è¨­å®šè³‡æ–™æ ¼å¼
                cell_format = workbook.add_format({ # type: ignore
                    'border': 1,
                    'align': 'left',
                    'valign': 'top',
                    'text_wrap': True
                })
                
                # æ‡‰ç”¨æ ¼å¼åˆ°æ¨™é¡Œè¡Œ
                for col_num, column_name in enumerate(df.columns):
                    worksheet.write(0, col_num, column_name, header_format)
                
                # è‡ªå‹•èª¿æ•´æ¬„ä½å¯¬åº¦
                for i, column in enumerate(df.columns):
                    max_length = max(
                        df[column].astype(str).apply(len).max(),
                        len(str(column))
                    )
                    # è¨­å®šåˆç†çš„æ¬„ä½å¯¬åº¦
                    width = min(max_length + 2, 50)
                    width = max(width, 10)
                    worksheet.set_column(i, i, width)
                
                # å‡çµé¦–è¡Œ
                worksheet.freeze_panes(1, 0)
        
        except ImportError:
            # å¦‚æœæ²’æœ‰ xlsxwriterï¼Œå›é€€åˆ° openpyxl
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='eRTé©—æ”¶æ˜ç´°', index=False)
        
        excel_buffer.seek(0)
        
        # å‰µå»ºè‡¨æ™‚æª”æ¡ˆ
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx')
        temp_file.write(excel_buffer.getvalue())
        temp_file.close()
        
        print(f"âœ… æˆåŠŸå‰µå»º Excel æª”æ¡ˆï¼ŒåŒ…å« {len(df)} è¡Œè³‡æ–™ï¼Œ{len(df.columns)} å€‹æ¬„ä½")
        print(f"ğŸ“‹ åŒ…å«æ¬„ä½: {', '.join(df.columns)}")
        
        # è¿”å›æª”æ¡ˆ
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name='eRTé©—æ”¶ç´°é …è³‡æ–™.xlsx',
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰éŒ¯èª¤: {str(e)}")
        return jsonify({'error': f'æª”æ¡ˆè™•ç†å¤±æ•—: {str(e)}'}), 500
    
    finally:
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆï¼ˆå»¶é²æ¸…ç†ï¼‰
        try:
            if 'temp_file' in locals():
                # å»¶é² 10 ç§’å¾Œæ¸…ç†ï¼ˆçµ¦ä¸‹è¼‰æ™‚é–“ï¼‰
                import threading
                def delayed_cleanup():
                    import time
                    time.sleep(10)
                    try:
                        os.unlink(temp_file.name)
                    except:
                        pass
                
                threading.Thread(target=delayed_cleanup).start()
        except:
            pass



# eRT åŠŸèƒ½
@app.route('/api/get_unaccounted_amount', methods=['GET'])
def get_unaccounted_amount():
    """
    çµ±è¨ˆå°šæœªå…¥å¸³çš„é‡‘é¡ (ä»¥ä»Šå¤©æ—¥æœŸç‚ºåŸºæº–)
    æ¢ä»¶ï¼š
    1. æœ‰æ‰¿è«¾äº¤æœŸ
    2. æ‰¿è«¾äº¤æœŸ <= ä»Šå¤©
    3. ç™¼ç¥¨æœˆä»½ç‚ºç©º
    """
    import datetime
    file_path = BUYER_CSV_PATH
    if not os.path.exists(file_path):
        return {"file": file_path, "unaccounted_amount": 0, "rows": []}

    try:
        df = pd.read_csv(file_path, encoding="utf-8-sig", dtype=str).fillna("")

        # æ—¥æœŸè™•ç†
        today = datetime.datetime.now().strftime("%Y%m%d")

        def clean_date(val):
            val = str(val).strip().replace("/", "").replace("-", "")
            return val if val.isdigit() and len(val) == 8 else ""

        df["äº¤æœŸ_clean"] = df["Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"].apply(clean_date)
        df["ç™¼ç¥¨æœˆä»½"] = df["ç™¼ç¥¨æœˆä»½"].astype(str).str.strip()

        # éæ¿¾æ¢ä»¶
        mask = (
            (df["äº¤æœŸ_clean"] != "") &
            (df["äº¤æœŸ_clean"] <= today) &
            (df["ç™¼ç¥¨æœˆä»½"] == "")
        )
        filtered = df[mask].copy()

        # è™•ç†é‡‘é¡ï¼šå„ªå…ˆç”¨ã€Œç¸½åƒ¹ã€ï¼Œè‹¥æ²’æœ‰å°±ç”¨ æ•¸é‡*å–®åƒ¹
        def calc_amount(row):
            try:
                if row.get("ç¸½åƒ¹") and str(row["ç¸½åƒ¹"]).strip():
                    return float(str(row["ç¸½åƒ¹"]).replace(",", "").strip())
                qty = float(str(row.get("æ•¸é‡", "0")).replace(",", "").strip() or 0)
                price = float(str(row.get("å–®åƒ¹", "0")).replace(",", "").strip() or 0)
                return qty * price
            except:
                return 0.0

        filtered["ç¸½åƒ¹"] = filtered.apply(calc_amount, axis=1)

        total_amount = round(filtered["ç¸½åƒ¹"].sum(), 2)

        rows = filtered[[
            "PO No.", "Item", "å“é …", "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", 
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "ç¸½åƒ¹"
        ]].to_dict(orient="records")

        # print(f"{'PO No.':<12}{'Item':<8}{'å“é …':<30}{'äº¤æœŸ':<12}{'SOD Qty':<10}{'ç¸½åƒ¹':<12}")
        # # å°å‡ºæ¯ä¸€è¡Œ
        # for r in rows:
        #     print(f"{r['PO No.']:<12}{r['Item']:<8}{r['å“é …']:<30}{r['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ']:<12}{r['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']:<10}{r['ç¸½åƒ¹']:<12,.0f}")

        return jsonify({
            "file": file_path,
            "unaccounted_amount": int(total_amount),
            "rows": rows
        })

    except Exception as e:
        return {"file": file_path, "error": str(e)}
    

# 3.py
@app.route("/api/accounting_summary", methods=["GET"])
def get_accounting_summary():
    # è®€å–CSVæª”æ¡ˆ
    df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype='str')

    # ç¯©é¸æ¢ä»¶ï¼šePR Noã€PO Noã€éœ€æ±‚æ—¥éƒ½ä¸ç‚ºç©ºå€¼
    filtered_df = df[
        (df['ePR No.'].notna()) & 
        (df['PO No.'].notna()) & 
        (df['éœ€æ±‚æ—¥'].notna()) &
        (df['ePR No.'] != '') & 
        (df['PO No.'] != '') & 
        (df['éœ€æ±‚æ—¥'] != '')
    ]

    # ==== æ–°å¢éœ€æ±‚ï¼šå…¥å¸³æ¢ä»¶ç¯©é¸ ====
    def filter_for_accounting(df, target_month, target_year=2025):
        """
        ç¯©é¸æœªå…¥å¸³è³‡æ–™ - æ­£ç¢ºé‚è¼¯
        å¦‚æœç™¼ç¥¨æœˆä»½å’Œæ‰¿è«¾äº¤æœŸåœ¨åŒå€‹æœˆä»½ â†’ å·²å…¥å¸³ï¼Œç›´æ¥å¿½ç•¥
        """
        # å»ºç«‹ç›®æ¨™æœˆä»½çš„æœ€å¾Œä¸€å¤©
        if target_month == 12:
            target_date_end = datetime(target_year + 1, 1, 1)
        else:
            target_date_end = datetime(target_year, target_month + 1, 1)
        
        # è¤‡è£½è³‡æ–™é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        accounting_df = df.copy()
        
        # è§£ææ—¥æœŸ
        accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] = pd.to_datetime(accounting_df['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ'], errors='coerce', format='mixed')
        accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] = pd.to_datetime(accounting_df['ç™¼ç¥¨æœˆä»½'], errors='coerce', format='mixed')
        
        # è™•ç†æ•¸å­—æ ¼å¼çš„éœ€æ±‚æ—¥ (å¦‚ 20250523)
        def parse_numeric_date(val):
            if pd.isna(val):
                return pd.NaT
            try:
                val_str = str(int(float(val)))
                if len(val_str) == 8:
                    year = int(val_str[:4])
                    month = int(val_str[4:6])
                    day = int(val_str[6:8])
                    return pd.Timestamp(year, month, day)
            except:
                pass
            return pd.NaT
        
        accounting_df['éœ€æ±‚æ—¥_æ—¥æœŸ'] = accounting_df['éœ€æ±‚æ—¥'].apply(parse_numeric_date)
        
        # 0. æ™‚é–“å›æ¨é‚è¼¯ï¼šæ’é™¤åœ¨ç›®æ¨™æœˆä»½æˆ–ä¹‹å‰å·²é–‹ç™¼ç¥¨çš„è³‡æ–™
        # ä¾‹å¦‚ï¼š7æœˆå ±è¡¨è¦æ’é™¤ç™¼ç¥¨åœ¨7æœˆæˆ–ä¹‹å‰çš„è³‡æ–™ï¼Œä¿ç•™ç™¼ç¥¨åœ¨8æœˆä¹‹å¾Œçš„ï¼ˆ7æœˆæ™‚æœªå…¥å¸³ï¼‰
        target_date_start = datetime(target_year, target_month, 1)
        if target_month == 12:
            target_date_end = datetime(target_year + 1, 1, 1)
        else:
            target_date_end = datetime(target_year, target_month + 1, 1)
        
        # æ’é™¤å·²å…¥å¸³çš„è³‡æ–™ï¼šç™¼ç¥¨æ—¥æœŸåœ¨ç›®æ¨™æœˆä»½æˆ–ä¹‹å‰çš„
        already_paid_mask = (
            accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].notna() & 
            (accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] < target_date_end)
        )
        
        # print(f"èª¿è©¦ - æ‰¾åˆ° {already_paid_mask.sum()} ç­†åœ¨{target_year}å¹´{target_month}æœˆæˆ–ä¹‹å‰å·²é–‹ç™¼ç¥¨çš„è³‡æ–™å°‡è¢«æ’é™¤")
        
        # æ’é™¤å·²å…¥å¸³çš„è³‡æ–™
        accounting_df = accounting_df[~already_paid_mask]
        
        # 1. EPR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        condition1 = (accounting_df['ePR No.'].notna()) & (accounting_df['PO No.'].notna()) & \
                    (accounting_df['ePR No.'] != '') & (accounting_df['PO No.'] != '')
        
        # 2. æ‰£é™¤ WBS æ¬„ä½æœ‰å€¼çš„è³‡æ–™ï¼ˆWBS å¿…é ˆç‚ºç©ºï¼‰
        condition2 = accounting_df['WBS'].isna() | (accounting_df['WBS'] == '')
        
        # 3. æ‰¿è«¾äº¤æœŸå¿…é ˆæœ‰å€¼ä¸”åœ¨ç•¶æœˆæˆ–ä¹‹å‰
        condition3 = accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'].notna() & (accounting_df['æ‰¿è«¾äº¤æœŸ_æ—¥æœŸ'] < target_date_end)
        
        # 4. éœ€æ±‚æ—¥å¿…é ˆåœ¨ç•¶æœˆä¹‹å‰  
        target_date_start = datetime(target_year, target_month, 1)
        condition4 = accounting_df['éœ€æ±‚æ—¥_æ—¥æœŸ'] < target_date_start
        
        # å¥—ç”¨æ‰€æœ‰æ¢ä»¶
        final_condition = condition1 & condition2 & condition3 & condition4
        result_df = accounting_df[final_condition]
        
        # é‡‘é¡è¨ˆç®—
        result_df = result_df.copy()
        result_df['ç¸½åƒ¹_æ•¸å€¼'] = pd.to_numeric(
            result_df['ç¸½åƒ¹'].astype(str).str.replace(',', '').str.replace("$", ''), 
            errors='coerce'
        )
        total_amount = result_df['ç¸½åƒ¹_æ•¸å€¼'].sum()
        
        return result_df, int(total_amount) if not pd.isna(total_amount) else 0

    # ç²å–è³‡æ–™ä¸­æ‰€æœ‰å¯èƒ½çš„å¹´æœˆä»½ (åªåˆ°ç•¶å‰æœˆä»½)
    def get_all_year_months(df):
        current_date = datetime.now()
        current_year = current_date.year
        current_month = current_date.month
        
        all_dates = []
        all_dates.extend(pd.to_datetime(df['Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ'], errors='coerce').dropna())
        all_dates.extend(pd.to_datetime(df['éœ€æ±‚æ—¥'], errors='coerce').dropna())
        all_dates.extend(pd.to_datetime(df['ç™¼ç¥¨æœˆä»½'], errors='coerce').dropna())
        
        year_months = set()
        for date in all_dates:
            year, month = date.year, date.month
            if (year < current_year) or (year == current_year and month <= current_month):
                year_months.add((year, month))
        return sorted(list(year_months))

    # ==== è¨ˆç®—æ‰€æœ‰æœˆä»½ ====
    all_year_months = get_all_year_months(filtered_df)
    json_summary = {}

    for year, month in all_year_months:
        result_df, total_amount = filter_for_accounting(df, month, year)
        year_month_key = f"{year}å¹´{month}æœˆ"
        json_summary[year_month_key] = total_amount
    
    # âœ… æœ€å¾Œå›å‚³ JSON çµ¦å‰ç«¯
    return jsonify(json_summary)



# 2_4.py
@app.route("/api/monthly_actual_accounting", methods=["GET"])
def get_monthly_actual_accounting():

    # è®€å–CSVæª”æ¡ˆ
    df = pd.read_csv(BUYER_CSV_PATH, encoding='utf-8-sig', dtype='str')

    # ç¯©é¸æ¢ä»¶ï¼šePR Noã€PO Noã€éœ€æ±‚æ—¥éƒ½ä¸ç‚ºç©ºå€¼
    filtered_df = df[
        (df['ePR No.'].notna()) & 
        (df['PO No.'].notna()) & 
        (df['éœ€æ±‚æ—¥'].notna()) &
        (df['ePR No.'] != '') & 
        (df['PO No.'] != '') & 
        (df['éœ€æ±‚æ—¥'] != '')
    ]

    def calculate_monthly_actual_accounting(df):
        """
        è¨ˆç®—æ¯æœˆå¯¦éš›å…¥å¸³é‡‘é¡
        æ¢ä»¶ï¼š
        1. ePR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        2. ä¸åŒ…å«WBS (WBSå¿…é ˆç‚ºç©º)
        3. ç™¼ç¥¨æœˆä»½ä¸ç‚ºç©º (æœ‰ç™¼ç¥¨æ‰ç®—å…¥å¸³)
        """
        # è¤‡è£½è³‡æ–™é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        accounting_df = df.copy()
        
        # 1. EPR No. å’Œ PO No. å¿…é ˆæœ‰å€¼
        condition1 = (accounting_df['ePR No.'].notna()) & (accounting_df['PO No.'].notna()) & \
                    (accounting_df['ePR No.'] != '') & (accounting_df['PO No.'] != '')
        
        # 2. ä¸å«WBS (WBS å¿…é ˆç‚ºç©º)
        condition2 = accounting_df['WBS'].isna() | (accounting_df['WBS'] == '')
        
        # 3. ç™¼ç¥¨æœˆä»½ä¸ç‚ºç©º (æœ‰ç™¼ç¥¨æ‰ç®—å…¥å¸³)
        accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'] = pd.to_datetime(accounting_df['ç™¼ç¥¨æœˆä»½'], errors='coerce', format='mixed')
        condition3 = accounting_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].notna()
        
        # å¥—ç”¨æ‰€æœ‰æ¢ä»¶
        final_condition = condition1 & condition2 & condition3
        result_df = accounting_df[final_condition]
        
        # åŠ å…¥ç™¼ç¥¨å¹´æœˆæ¬„ä½
        result_df = result_df.copy()
        result_df['ç™¼ç¥¨å¹´æœˆ'] = result_df['ç™¼ç¥¨æœˆä»½_æ—¥æœŸ'].dt.to_period('M')
        
        # print(f"ç¬¦åˆå…¥å¸³æ¢ä»¶çš„è³‡æ–™ç­†æ•¸: {len(result_df)}")
        # print(f"1. EPR No. å’Œ PO No. æœ‰å€¼: {condition1.sum()} ç­†")
        # print(f"2. WBS æ¬„ä½ç‚ºç©º: {condition2.sum()} ç­†")
        # print(f"3. ç™¼ç¥¨æœˆä»½ä¸ç‚ºç©º: {condition3.sum()} ç­†")
        
        return result_df
    
    def get_monthly_summary(df_with_period):
        df = df_with_period.copy()
        df['RTç¸½é‡‘é¡_æ•¸å€¼'] = pd.to_numeric(
            df['RTç¸½é‡‘é¡'].astype(str).str.replace(',', '').str.replace('$', ''), 
            errors='coerce'
        )

        monthly_summary = df.groupby('ç™¼ç¥¨å¹´æœˆ')['RTç¸½é‡‘é¡_æ•¸å€¼'].sum().reset_index()

        monthly_dict = {}
        for _, row in monthly_summary.iterrows():
            if pd.notna(row['ç™¼ç¥¨å¹´æœˆ']):
                year = row['ç™¼ç¥¨å¹´æœˆ'].year
                month = row['ç™¼ç¥¨å¹´æœˆ'].month
                key = f"{year}å¹´{month}æœˆ"
                monthly_dict[key] = int(row['RTç¸½é‡‘é¡_æ•¸å€¼']) if not pd.isna(row['RTç¸½é‡‘é¡_æ•¸å€¼']) else 0
        return monthly_dict

    # ==== è¨ˆç®— ====
    actual_accounting_df = calculate_monthly_actual_accounting(filtered_df)
    if len(actual_accounting_df) == 0:
        return jsonify({"æœ¬æœˆ": 0, "ä¸Šæœˆ": 0})

    monthly_dict = get_monthly_summary(actual_accounting_df)

    # ==== æ‰¾æœ¬æœˆèˆ‡ä¸Šæœˆ ====
    now = datetime.now()
    this_key = f"{now.year}å¹´{now.month}æœˆ"
    last_month = now.month - 1 if now.month > 1 else 12
    last_year = now.year if now.month > 1 else now.year - 1
    last_key = f"{last_year}å¹´{last_month}æœˆ"

    this_amount = monthly_dict.get(this_key, 0)
    last_amount = monthly_dict.get(last_key, 0)

    return jsonify({
        "æœ¬æœˆ": this_amount,
        "ä¸Šæœˆ": last_amount
    })




if __name__ == "__main__":
    app.run(debug=True)


    # 165