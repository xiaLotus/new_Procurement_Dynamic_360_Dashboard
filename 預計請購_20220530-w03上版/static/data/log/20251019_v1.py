import datetime
import json
import os
import re
from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
from ldap3 import Server, Connection, ALL, NTLM # type: ignore
from ldap3.core.exceptions import LDAPException, LDAPBindError # type: ignore
import uuid
from filelock import FileLock, Timeout
from werkzeug.utils import secure_filename
import logging
import shutil
import numpy as np
import traceback
from loguru import logger

BACKEND_DATA = r"D:\Data\Backend_Access_Management\Backend_data.json"
VENDER_FILE_PATH = f'static/data/vender.ini'

app = Flask(__name__)
CORS(app)
CSV_FILE = "static/data/Planned_Purchase_Request_List.csv"
JSON_FILE = f"static/data/money.json"
BUYER_FILE = f"static/data/Buyer_detail.csv"
BUYER_FILE_LOCK = f"static/data/Buyer_detail.csv.lock"  # ğŸ”’ é–æª”æ¡ˆè·¯å¾‘

from difflib import SequenceMatcher
buyer_file_lock = FileLock(BUYER_FILE_LOCK, timeout=10)

# eHub è™•ç†

def is_po_in_record(row_po_str, target_po):
    """æª¢æŸ¥ PO æ˜¯å¦åœ¨è¨˜éŒ„ä¸­ï¼ˆæ”¯æ´ <br /> åˆ†éš”çš„å¤šå€‹ POï¼‰"""
    po_list = re.split(r"<br\s*/?>", str(row_po_str))
    po_list = [po.strip() for po in po_list if po.strip()]
    return target_po.strip() in po_list

def fuzzy_in(text, keyword):
    """æ¨¡ç³Šæ¯”å°é—œéµå­—æ˜¯å¦åœ¨æ–‡å­—ä¸­"""
    return keyword.strip() in str(text).strip()

def cleanup_temp_csv_files(po_no=None):
    """æ¸…ç†æš«å­˜çš„ CSV æª”æ¡ˆ"""
    try:
        uploads_dir = "uploads"
        if not os.path.exists(uploads_dir):
            return
        
        if po_no:
            # åªåˆªé™¤ç‰¹å®š PO çš„æª”æ¡ˆ
            temp_file = os.path.join(uploads_dir, f"{po_no}.csv")
            if os.path.exists(temp_file):
                os.remove(temp_file)
                logger.info(f"âœ… å·²åˆªé™¤: {temp_file}")
        else:
            # åˆªé™¤æ‰€æœ‰æš«å­˜ CSV æª”æ¡ˆ
            for filename in os.listdir(uploads_dir):
                if filename.endswith('.csv'):
                    file_path = os.path.join(uploads_dir, filename)
                    os.remove(file_path)
                    logger.info(f"âœ… å·²åˆªé™¤: {file_path}")
    except Exception as e:
        logger.error(f"æ¸…ç†æš«å­˜æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


@app.route("/api/save_csv", methods=["POST", "OPTIONS"])
def save_csv():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200

    data = request.get_json(silent=True) or {}
    content = data.get("content", "")

    from io import StringIO
    df_all = pd.read_csv(StringIO(content), dtype=str).fillna("")

    # å…ˆè™•ç† Item â†’ ä¸€å¾‹è£œæˆ4ä½æ•¸
    if "PO Item æ¡è³¼å–®é …æ¬¡" in df_all.columns:
        df_all["PO Item æ¡è³¼å–®é …æ¬¡"] = (
            df_all["PO Item æ¡è³¼å–®é …æ¬¡"]
            .str.strip()
            .apply(lambda x: str(int(float(x))) if x.replace('.', '', 1).isdigit() else x)
            .apply(lambda x: x.zfill(4) if x.isdigit() else x)
        )

    # ç¢ºèªæœ‰å¿…è¦æ¬„ä½
    if "PO NO æ¡è³¼å–®è™Ÿç¢¼" not in df_all.columns:
        return jsonify({"status": "error", "msg": "âŒ ç¼ºå°‘ PO NO æ¡è³¼å–®è™Ÿç¢¼ æ¬„ä½!"}), 400

    # æŠ“å‡ºæ‰€æœ‰ä¸åŒçš„ PO NO
    unique_po_nos = df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"].dropna().unique()

    # âœ… ç¬¬ä¸€æ­¥:åˆ†ç¾¤å„²å­˜ uploads/{po_no}.csv
    saved_files = []
    for po_no in unique_po_nos:
        po_no_clean = str(po_no).strip()
        group_df = df_all[df_all["PO NO æ¡è³¼å–®è™Ÿç¢¼"] == po_no_clean]

        upload_path = f"uploads/{po_no_clean}.csv"
        group_df.to_csv(upload_path, index=False, encoding="utf-8-sig")

        saved_files.append({
            "po_no": po_no_clean,
            "rows": len(group_df),
            "file": upload_path
        })
        logger.info(f"âœ… å·²å„²å­˜ {upload_path} ({len(group_df)} ç­†è³‡æ–™)")

    try:
        with buyer_file_lock:
            # âœ… ç¬¬äºŒæ­¥:è¼‰å…¥ Buyer_detail.csv (æ¯”å°ç”¨)
            df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
            df_buyer["PO No."] = df_buyer["PO No."].str.strip()
            df_buyer["Item"] = (
                df_buyer["Item"]
                .str.replace(r"\.0$", "", regex=True)
                .str.strip()
                .apply(lambda x: x.zfill(4) if x.isdigit() else x)
            )

            # ğŸ”´ å»ºç«‹ä¸€å€‹åªåŒ…å«ç‹€æ…‹ç‚º V çš„è³‡æ–™ç´¢å¼•
            df_active = df_buyer[df_buyer["é–‹å–®ç‹€æ…‹"] == "V"].copy()
            logger.info(f"ç¸½è³‡æ–™ç­†æ•¸: {len(df_buyer)}, æœ‰æ•ˆè³‡æ–™(ç‹€æ…‹=V): {len(df_active)}")

    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ è®€å– Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"è®€å–æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500

    # ğŸ†• åˆå§‹åŒ–
    all_group_results = []
    quantity_mismatch_items = []
    merged_items = []  # ğŸ†• è¨˜éŒ„åˆä½µçš„é …ç›®
    
    # âœ…âœ…âœ… æ–°å¢ï¼šè¨˜éŒ„å“ªäº› PO æœ‰å•é¡Œï¼ˆåˆ†æ‰¹æˆ–åˆä½µï¼‰
    mismatch_po_set = set()  # æœ‰åˆ†æ‰¹å•é¡Œçš„ PO
    merge_po_set = set()      # æœ‰åˆä½µå•é¡Œçš„ PO

    def clean_name(x: str) -> str:
        return str(x).replace("\n", "").replace("\r", "").replace("<br>", "").strip()

    df_buyer["å“é …_clean"] = df_buyer["å“é …"].apply(clean_name)
    buyer_id_lookup = {
        (row["PO No."], row["Item"]): row.get("Id", "")
        for _, row in df_buyer.iterrows()
    }

    # ğŸ”´ ç¬¬ä¸€è¼ª:æª¢æ¸¬æ‰€æœ‰ PO æ˜¯å¦æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œ
    logger.info("\n" + "="*80)
    logger.info("ç¬¬ä¸€è¼ªæƒæï¼šæª¢æ¸¬åˆ†æ‰¹å’Œåˆä½µå•é¡Œ")
    logger.info("="*80)
    
    for file_info in saved_files:
        po_no = file_info["po_no"]
        csv_path = file_info["file"]
        
        df_po = pd.read_csv(csv_path, encoding="utf-8-sig", dtype=str).fillna("")
        df_po["PO Item æ¡è³¼å–®é …æ¬¡"] = df_po["PO Item æ¡è³¼å–®é …æ¬¡"].str.zfill(4)
        
        buyer_related = df_active[df_active["PO No."].str.contains(po_no, regex=False, na=False)].copy()
        
        xls_item_groups = df_po.groupby("PO Item æ¡è³¼å–®é …æ¬¡")
        
        for item, xls_group in xls_item_groups:
            xls_count = len(xls_group)
            csv_items = buyer_related[buyer_related["Item"] == item]
            csv_count = len(csv_items)
            
            # âœ… æ–°å¢:å¦‚æœ XLS ç­†æ•¸ < CSV ç­†æ•¸,ä»£è¡¨ã€Œåˆ†æ‰¹è®Šå›åˆä½µã€
            if csv_count > 0 and xls_count < csv_count:
                logger.info(f"ğŸ”„ åµæ¸¬åˆ°åˆä½µ:PO={po_no}, Item={item}, XLS={xls_count}ç­† < CSV={csv_count}ç­†")
                merge_po_set.add(po_no)  # âœ… æ¨™è¨˜æ­¤ PO æœ‰åˆä½µå•é¡Œ
                
                xls_rows = []
                for _, row in xls_group.iterrows():
                    xls_rows.append({
                        "description": str(row.get("Description å“å", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0"))
                    })
                
                csv_rows = []
                batch_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["å‚™è¨»"].str.contains("åˆ†æ‰¹", na=False)) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                csv_items = df_buyer[batch_mask]
                
                for _, row in csv_items.iterrows():
                    csv_rows.append({
                        "id": str(row.get("Id", "")),
                        "description": str(row.get("å“é …", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0")),
                        "note": str(row.get("å‚™è¨»", ""))
                    })
                
                # ğŸ†• è¨˜éŒ„éœ€è¦åˆä½µçš„é …ç›®
                merged_items.append({
                    "po_no": po_no,
                    "item": item,
                    "xls_count": int(xls_count),
                    "csv_count": int(csv_count),
                    "xls_data": xls_rows,
                    "csv_data": csv_rows,
                    "action_type": "merge"
                })
                
                logger.info(f"   ğŸ“ å·²è¨˜éŒ„éœ€è¦åˆä½µçš„é …ç›®")

            # ğŸš¨ ç­†æ•¸ä¸ç¬¦ï¼ˆåˆ†æ‰¹ï¼‰
            elif csv_count > 0 and xls_count > csv_count:
                mismatch_po_set.add(po_no)  # âœ… æ¨™è¨˜æ­¤ PO æœ‰åˆ†æ‰¹å•é¡Œ
                
                try:
                    total_sod = csv_items["SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"].astype(float).sum()
                except:
                    total_sod = 0.0
                
                xls_rows = []
                for _, row in xls_group.iterrows():
                    xls_rows.append({
                        "description": str(row.get("Description å“å", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0"))
                    })
                
                csv_rows = []
                for _, row in csv_items.iterrows():
                    csv_rows.append({
                        "id": str(row.get("Id", "")),
                        "description": str(row.get("å“é …", "")),
                        "delivery": str(row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")),
                        "sod_qty": str(row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "0")),
                        "total_qty": str(row.get("ç¸½æ•¸", "0"))
                    })
                
                quantity_mismatch_items.append({
                    "po_no": po_no,
                    "item": item,
                    "xls_count": int(xls_count),
                    "csv_count": int(csv_count),
                    "total_sod": float(total_sod),
                    "xls_data": xls_rows,
                    "csv_data": csv_rows
                })
                
                logger.info(f"âš ï¸ ç­†æ•¸ä¸ç¬¦: PO={po_no}, Item={item}, XLS={xls_count}ç­†, CSV={csv_count}ç­†")

    logger.info(f"\nğŸ“Š ç¬¬ä¸€è¼ªæƒæçµæœ:")
    logger.info(f"   åˆ†æ‰¹å•é¡Œ PO: {len(mismatch_po_set)} å€‹ - {list(mismatch_po_set)}")
    logger.info(f"   åˆä½µå•é¡Œ PO: {len(merge_po_set)} å€‹ - {list(merge_po_set)}")
    logger.info("="*80 + "\n")

    # ğŸ”´ ç¬¬äºŒè¼ª:è™•ç†æ²’æœ‰å•é¡Œçš„ PO (æ­£å¸¸æ¯”å°)
    logger.info("\n" + "="*80)
    logger.info("ç¬¬äºŒè¼ªè™•ç†ï¼šæ­£å¸¸æ¯”å°ï¼ˆæ’é™¤æœ‰å•é¡Œçš„ POï¼‰")
    logger.info("="*80)
    
    for file_info in saved_files:
        po_no = file_info["po_no"]
        
        # âœ…âœ…âœ… é—œéµä¿®æ”¹ï¼šå¦‚æœé€™å€‹ PO æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œï¼Œè·³éæ­£å¸¸æ¯”å°
        if po_no in mismatch_po_set:
            logger.info(f"â­•ï¸ è·³é PO {po_no} çš„æ­£å¸¸æ¯”å° (æœ‰åˆ†æ‰¹å•é¡Œ)")
            continue
        
        if po_no in merge_po_set:
            logger.info(f"â­•ï¸ è·³é PO {po_no} çš„æ­£å¸¸æ¯”å° (æœ‰åˆä½µå•é¡Œ)")
            continue
        
        csv_path = file_info["file"]
        df_po = pd.read_csv(csv_path, encoding="utf-8-sig", dtype=str).fillna("")
        df_po["PO Item æ¡è³¼å–®é …æ¬¡"] = df_po["PO Item æ¡è³¼å–®é …æ¬¡"].str.zfill(4)

        buyer_related = df_buyer[df_buyer["PO No."].str.contains(po_no, regex=False, na=False)].copy()

        matched_list = []
        conflict_list = []

        for row_idx, row in df_po.iterrows():
            item = row["PO Item æ¡è³¼å–®é …æ¬¡"]
            desc = clean_name(row.get("Description å“å", ""))
            delivery = row["Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"]
            qty = row["SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"]

            logger.info(f"è™•ç† PO {po_no} çš„ Item {item}")

            item_match = buyer_related[buyer_related["Item"] == item]
            
            if not item_match.empty:
                buyer_row = item_match.iloc[0]
                buyer_item = buyer_row["Item"]
                buyer_desc = clean_name(buyer_row["å“é …"])
                buyer_id = buyer_row.get("Id", "")

                logger.info(f"  æ‰¾åˆ° Buyer Item {buyer_item}, å“å: {buyer_desc}")

                df_buyer.loc[item_match.index, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = delivery
                df_buyer.loc[item_match.index, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = qty

                if buyer_desc == desc:
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    matched_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc or buyer_desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âœ… ç›¸åŒ",
                        "diff_type": "none"
                    })
                    logger.info(f"  âœ… å®Œå…¨ç›¸åŒ")
                else:
                    buyer_id = buyer_id_lookup.get((po_no, buyer_item), "")
                    conflict_list.append({
                        "id": buyer_id,
                        "po_no": po_no,
                        "item": item,
                        "buyer_description": buyer_desc,
                        "po_description": desc,
                        "delivery_date": delivery,
                        "sod_qty": qty,
                        "status": "âš ï¸ å“åä¸ç›¸åŒ",
                        "diff_type": "desc"
                    })
                    logger.info(f"  âš ï¸ å“åä¸åŒ")
            else:
                logger.info(f"  âŒ Buyer ä¸­æ‰¾ä¸åˆ° Item {item}")
                buyer_id = ''
                conflict_list.append({
                    "id": buyer_id,
                    "po_no": po_no,
                    "item": item,
                    "buyer_description": None,
                    "po_description": desc,
                    "delivery_date": delivery,
                    "sod_qty": qty,
                    "status": "âš ï¸ Buyer_detail æ²’æœ‰é€™ç­†è³‡æ–™",
                    "diff_type": "missing"
                })

        all_group_results.append({
            "po_no": po_no,
            "matched": matched_list,
            "conflict": conflict_list
        })
        
        logger.info(f"ğŸ“Š PO {po_no} çµ±è¨ˆ: ç›¸åŒ={len(matched_list)}, è¡çª={len(conflict_list)}")

    logger.info("="*80 + "\n")

    # ğŸ¯ æ ¹æ“šæƒ…æ³è¿”å›ä¸åŒçš„çµæœ
    
    # âœ… å„ªå…ˆè¿”å›åˆä½µç¢ºèª
    if merged_items:
        logger.info(f"ğŸ”„ ç™¼ç¾ {len(merged_items)} å€‹é …ç›®éœ€è¦åˆä½µ")
        
        # è½‰æ›è³‡æ–™
        merged_items_clean = []
        for item in merged_items:
            merged_items_clean.append({
                "po_no": str(item["po_no"]),
                "item": str(item["item"]),
                "xls_count": int(item["xls_count"]),
                "csv_count": int(item["csv_count"]),
                "xls_data": item["xls_data"],
                "csv_data": item["csv_data"],
                "action_type": "merge"
            })
        
        return jsonify({
            "status": "merge_confirmation_needed",
            "message": f"ç™¼ç¾ {len(merged_items)} å€‹é …ç›®å¾åˆ†æ‰¹è®Šå›å–®ç­†,éœ€è¦ç¢ºèª",
            "merge_items": merged_items_clean,
            "total_merge_items": len(merged_items),
            "groups": all_group_results,  # âœ… åªåŒ…å«æ²’å•é¡Œçš„ PO
            "has_normal_items": len(all_group_results) > 0
        })
    
    # âœ… å…¶æ¬¡è¿”å›åˆ†æ‰¹èª¿æ•´
    if quantity_mismatch_items:
        logger.info(f"ğŸš¨ ç™¼ç¾ {len(quantity_mismatch_items)} å€‹é …ç›®ç­†æ•¸ä¸ç¬¦")
        logger.info(f"âœ… åŒæ™‚è™•ç†äº† {len(all_group_results)} å€‹æ­£å¸¸ PO")
        
        return jsonify({
            "status": "quantity_mismatch",
            "message": f"ç™¼ç¾ {len(quantity_mismatch_items)} å€‹é …ç›®çš„ç­†æ•¸ä¸ç¬¦,éœ€è¦æ‰‹å‹•èª¿æ•´",
            "mismatches": quantity_mismatch_items,
            "total_mismatches": len(quantity_mismatch_items),
            "require_manual_adjustment": True,
            "groups": all_group_results,  # âœ… åªåŒ…å«æ²’å•é¡Œçš„ PO
            "has_normal_items": len(all_group_results) > 0
        })

    # æ²’æœ‰åˆ†æ‰¹æˆ–åˆä½µå•é¡Œ,æ­£å¸¸è¿”å›
    logger.info(f"\nâœ… ç¸½å…±è™•ç† {len(all_group_results)} å€‹ PO")
    
    return jsonify({
        "status": "ok",
        "groups": all_group_results,
        "saved_files": saved_files,
        "has_mismatch": False
    })















# èµ°åˆ†æ‰¹
@app.route("/api/confirm_quantity_update", methods=["POST"])
def confirm_quantity_update():
    """è™•ç†ç­†æ•¸ä¸ç¬¦æ™‚ä½¿ç”¨è€…ç¢ºèªçš„æ›´æ–°(ä¿ç•™æ‰€æœ‰åŸå§‹æ¬„ä½)"""
    
    logger.info("=" * 80)
    logger.info("ğŸš€ confirm_quantity_update å‡½æ•¸è¢«èª¿ç”¨")
    logger.info("=" * 80)
    
    try:
        data = request.get_json()
        logger.info(f"ğŸ“¥ æ”¶åˆ°çš„å®Œæ•´è³‡æ–™: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        po_no = data.get("po_no")
        item = data.get("item")
        new_rows = data.get("rows")
        expected_total = data.get("expected_total")
        
        logger.info(f"ğŸ“¦ è§£æå¾Œçš„åƒæ•¸:")
        logger.info(f"   PO No: {po_no}")
        logger.info(f"   Item: {item}")
        logger.info(f"   æ–°ç­†æ•¸: {len(new_rows) if new_rows else 0}")
        logger.info(f"   é æœŸç¸½å’Œ: {expected_total}")
        
        if not po_no or not item or not new_rows:
            error_msg = f"ç¼ºå°‘å¿…è¦åƒæ•¸: po_no={po_no}, item={item}, new_rows={len(new_rows) if new_rows else 'None'}"
            logger.error(f"âŒ {error_msg}")
            return jsonify({
                "status": "error",
                "message": error_msg
            }), 400
        
        def smart_number_format(value):
            """æ™ºèƒ½æ ¼å¼åŒ–æ•¸å­—"""
            try:
                num = float(value)
                if num.is_integer():
                    return str(int(num))
                else:
                    return str(num)
            except (ValueError, TypeError):
                return str(value)
        
        try:
            with buyer_file_lock:
                # ğŸ“Š è¼‰å…¥ Buyer_detail.csv
                logger.info(f"ğŸ“‚ é–‹å§‹è¼‰å…¥ {BUYER_FILE}")
                
                if not os.path.exists(BUYER_FILE):
                    logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {BUYER_FILE}")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {BUYER_FILE}"
                    }), 500
                
                df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str).fillna("")
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ Buyer_detail.csv,å…± {len(df_buyer)} ç­†è³‡æ–™")
                
                # âš ï¸ è¨ˆç®—å¯¦éš› SOD ç¸½å’Œ
                try:
                    actual_total = sum(float(row.get("sod_qty", 0)) for row in new_rows)
                    sod_diff = actual_total - expected_total
                    
                    logger.info(f"ğŸ”¢ SOD ç¸½å’Œè¨ˆç®—:")
                    logger.info(f"   é æœŸ: {expected_total}")
                    logger.info(f"   å¯¦éš›: {actual_total}")
                    logger.info(f"   å·®ç•°: {sod_diff}")
                    
                    if abs(sod_diff) > 0.01:
                        logger.warning(f"âš ï¸ SOD ç¸½å’Œä¸ç¬¦!é æœŸ={expected_total}, å¯¦éš›={actual_total}, å·®ç•°={sod_diff}")
                        logger.warning(f"   ä½¿ç”¨è€…é¸æ“‡å¼·åˆ¶å„²å­˜,å°‡ç¹¼çºŒè™•ç†...")
                    else:
                        logger.info(f"âœ… SOD ç¸½å’Œæ­£ç¢º:{actual_total}")
                except Exception as e:
                    logger.error(f"âŒ è¨ˆç®— SOD ç¸½å’Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    logger.error(traceback.format_exc())
                
                # ğŸ—‘ï¸ åˆªé™¤èˆŠçš„è³‡æ–™
                logger.info(f"ğŸ” é–‹å§‹å°‹æ‰¾è¦åˆªé™¤çš„è³‡æ–™ (PO={po_no}, Item={item}, ç‹€æ…‹=V)")
                
                old_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                
                deleted_count = old_mask.sum()
                logger.info(f"ğŸ“Š æ‰¾åˆ° {deleted_count} ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
                
                if deleted_count == 0:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™")
                    logger.error(f"   æŸ¥è©¢æ¢ä»¶: PO No.={po_no}, Item={item}, é–‹å–®ç‹€æ…‹=V")
                    
                    po_exists = df_buyer[df_buyer["PO No."].str.strip() == po_no]
                    logger.error(f"   è©² PO å…±æœ‰ {len(po_exists)} ç­†è³‡æ–™")
                    
                    if len(po_exists) > 0:
                        items = po_exists["Item"].str.strip().unique()
                        logger.error(f"   è©² PO çš„ Items: {list(items)}")
                        statuses = po_exists["é–‹å–®ç‹€æ…‹"].unique()
                        logger.error(f"   è©² PO çš„ç‹€æ…‹: {list(statuses)}")
                    else:
                        logger.error(f"   âŒ è©² PO å®Œå…¨ä¸å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­")
                    
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™ (PO {po_no} - Item {item}, ç‹€æ…‹=V)"
                    }), 404
                
                # ğŸ¯ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬
                logger.info(f"ğŸ“‹ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬")
                first_old_row = df_buyer[old_mask].iloc[0]
                
                # ğŸ“ è¨˜éŒ„è¢«åˆªé™¤çš„è³‡æ–™
                deleted_data = []
                for idx, row in df_buyer[old_mask].iterrows():
                    deleted_data.append({
                        "å“é …": row.get("å“é …", ""),
                        "è¦æ ¼": row.get("è¦æ ¼", ""),
                        "å–®åƒ¹": row.get("å–®åƒ¹", ""),
                        "ç¸½åƒ¹": row.get("ç¸½åƒ¹", ""),
                        "äº¤æœŸ": row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", ""),
                        "SOD": row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", "")
                    })
                
                logger.info(f"ğŸ—‘ï¸ å°‡åˆªé™¤çš„è³‡æ–™:")
                for i, d in enumerate(deleted_data, 1):
                    logger.info(f"   ç¬¬ {i} ç­†: {d}")
                
                # åˆªé™¤èˆŠè³‡æ–™
                logger.info(f"ğŸ—‘ï¸ åŸ·è¡Œåˆªé™¤æ“ä½œ...")
                df_buyer = df_buyer[~old_mask]
                logger.info(f"âœ… åˆªé™¤å®Œæˆ,å‰©é¤˜ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’° è¨ˆç®—å–®åƒ¹
                try:
                    unit_price_str = str(first_old_row.get("å–®åƒ¹", "0")).replace(",", "").strip()
                    unit_price = float(unit_price_str) if unit_price_str else 0.0
                    logger.info(f"ğŸ’° åŸå§‹å–®åƒ¹: {unit_price}")
                except Exception as e:
                    logger.error(f"âŒ è§£æå–®åƒ¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    unit_price = 0.0
                
                # â• æº–å‚™æ–°è³‡æ–™
                logger.info(f"â• é–‹å§‹æº–å‚™æ–°è³‡æ–™...")
                new_data_rows = []
                
                for idx, row_data in enumerate(new_rows):
                    logger.info(f"   è™•ç†ç¬¬ {idx+1}/{len(new_rows)} ç­†æ–°è³‡æ–™...")
                    
                    try:
                        sod_qty = float(row_data.get("sod_qty", 0))
                    except:
                        sod_qty = 0.0
                    
                    sod_qty_formatted = smart_number_format(sod_qty)
                    total_price = unit_price * sod_qty
                    total_price_formatted = smart_number_format(total_price)
                    unit_price_formatted = smart_number_format(unit_price)
                    
                    new_row = {
                        "Id": first_old_row.get("Id", ""),
                        "é–‹å–®ç‹€æ…‹": "V",
                        "äº¤è²¨é©—è­‰": first_old_row.get("äº¤è²¨é©—è­‰", ""),
                        "User": first_old_row.get("User", ""),
                        "ePR No.": first_old_row.get("ePR No.", ""),
                        "PO No.": po_no,
                        "Item": item,
                        "å“é …": row_data.get("description", first_old_row.get("å“é …", "")),
                        "è¦æ ¼": first_old_row.get("è¦æ ¼", ""),
                        "æ•¸é‡": sod_qty_formatted,
                        "ç¸½æ•¸": sod_qty_formatted,
                        "å–®åƒ¹": unit_price_formatted,
                        "ç¸½åƒ¹": total_price_formatted,
                        "å‚™è¨»": f"åˆ†æ‰¹{idx+1}/{len(new_rows)}" if len(new_rows) > 1 else first_old_row.get("å‚™è¨»", ""),
                        "å­—æ•¸": first_old_row.get("å­—æ•¸", ""),
                        "isEditing": "False",
                        "backup": "{}",
                        "_alertedItemLimit": "",
                        "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": row_data.get("delivery", ""),
                        "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": sod_qty_formatted,
                        "é©—æ”¶æ•¸é‡": "",
                        "æ‹’æ”¶æ•¸é‡": "",
                        "ç™¼ç¥¨æœˆä»½": "",
                        "WBS": first_old_row.get("WBS", ""),
                        "éœ€æ±‚æ—¥": first_old_row.get("éœ€æ±‚æ—¥", ""),
                        "RTé‡‘é¡": "",
                        "RTç¸½é‡‘é¡": "",
                        "é©—æ”¶ç‹€æ…‹": "X"
                    }
                    new_data_rows.append(new_row)
                    logger.info(f"      å“é …: {new_row['å“é …']}, SOD: {new_row['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']}, ç¸½åƒ¹: {new_row['ç¸½åƒ¹']}")
                
                logger.info(f"âœ… æº–å‚™å®Œæˆ {len(new_data_rows)} ç­†æ–°è³‡æ–™")
                
                # ğŸ”´ğŸ”´ğŸ”´ ç›´æ¥åŠ åˆ°æœ€å¾Œé¢ ğŸ”´ğŸ”´ğŸ”´
                logger.info(f"ğŸ“¥ å°‡æ–°è³‡æ–™æ·»åŠ åˆ°æœ€å¾Œ...")
                df_buyer = pd.concat([
                    df_buyer,
                    pd.DataFrame(new_data_rows)
                ], ignore_index=True)
                logger.info(f"âœ… æ·»åŠ å®Œæˆ,ç¾æœ‰ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’¾ å„²å­˜å›æª”æ¡ˆ
                logger.info(f"ğŸ’¾ é–‹å§‹å„²å­˜åˆ° {BUYER_FILE}...")
                df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
                logger.info(f"âœ… å„²å­˜å®Œæˆ")
        
        except Timeout:
            logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
            return jsonify({
                "status": "error",
                "message": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"
            }), 503

        # ğŸ“ æº–å‚™å›å‚³è¨Šæ¯
        success_msg = f"æˆåŠŸæ›´æ–° PO {po_no} Item {item}: åˆªé™¤ {deleted_count} ç­†,æ–°å¢ {len(new_data_rows)} ç­† (å·²åŠ åˆ°æœ€å¾Œ)"
        
        if abs(sod_diff) > 0.01:
            success_msg += f" (âš ï¸ SODå·®ç•°: {sod_diff:+.2f})"
        
        logger.info("=" * 80)
        logger.info(f"âœ… {success_msg}")
        logger.info("=" * 80)

        # ğŸ—‘ï¸ åˆªé™¤æš«å­˜çš„ CSV æª”æ¡ˆ
        cleanup_temp_csv_files(po_no)
        
        return jsonify({
            "status": "success",
            "message": success_msg,
            "deleted_count": int(deleted_count),
            "inserted_count": len(new_data_rows),
            "sod_info": {
                "expected": float(expected_total),
                "actual": float(actual_total),
                "difference": float(sod_diff),
                "is_match": abs(sod_diff) < 0.01
            },
            "preserved_fields": {
                "unit_price": float(unit_price),
                "specification": first_old_row.get("è¦æ ¼", ""),
                "wbs": first_old_row.get("WBS", ""),
                "epr_no": first_old_row.get("ePR No.", "")
            }
        })
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ confirm_quantity_update ç™¼ç”Ÿåš´é‡éŒ¯èª¤")
        logger.error(f"âŒ éŒ¯èª¤è¨Šæ¯: {str(e)}")
        logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
        logger.error("âŒ å®Œæ•´å †ç–Šè¿½è¹¤:")
        logger.error(traceback.format_exc())
        logger.error("=" * 80)
        
        return jsonify({
            "status": "error",
            "message": f"æ›´æ–°å¤±æ•—: {str(e)}"
        }), 500


# èµ°åˆä½µ
@app.route("/api/confirm_merge", methods=["POST"])
def confirm_merge():
    """è™•ç†åˆ†æ‰¹è®Šå›åˆä½µçš„ç¢ºèªæ›´æ–°"""
    
    logger.info("=" * 80)
    logger.info("ğŸ”„ confirm_merge å‡½æ•¸è¢«èª¿ç”¨")
    logger.info("=" * 80)
    
    try:
        data = request.get_json()
        logger.info(f"ğŸ“¥ æ”¶åˆ°çš„å®Œæ•´è³‡æ–™: {json.dumps(data, ensure_ascii=False, indent=2)}")
        
        po_no = data.get("po_no")
        item = data.get("item")
        xls_data = data.get("xls_data")  # æ–°çš„ XLS è³‡æ–™ (é€šå¸¸åªæœ‰ 1 ç­†)
        
        logger.info(f"ğŸ“¦ è§£æå¾Œçš„åƒæ•¸:")
        logger.info(f"   PO No: {po_no}")
        logger.info(f"   Item: {item}")
        logger.info(f"   æ–°ç­†æ•¸: {len(xls_data) if xls_data else 0}")
        
        if not po_no or not item or not xls_data:
            error_msg = f"ç¼ºå°‘å¿…è¦åƒæ•¸: po_no={po_no}, item={item}, xls_data={len(xls_data) if xls_data else 'None'}"
            logger.error(f"âŒ {error_msg}")
            return jsonify({
                "status": "error",
                "message": error_msg
            }), 400
        
        def smart_number_format(value):
            """æ™ºèƒ½æ ¼å¼åŒ–æ•¸å­—"""
            try:
                num = float(value)
                if num.is_integer():
                    return str(int(num))
                else:
                    return str(num)
            except (ValueError, TypeError):
                return str(value)
        
        try:
            with buyer_file_lock:
                # ğŸ“Š è¼‰å…¥ Buyer_detail.csv
                logger.info(f"ğŸ“‚ é–‹å§‹è¼‰å…¥ {BUYER_FILE}")
                
                if not os.path.exists(BUYER_FILE):
                    logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {BUYER_FILE}")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {BUYER_FILE}"
                    }), 500
                
                df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str).fillna("")
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ Buyer_detail.csv,å…± {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ—‘ï¸ åˆªé™¤èˆŠçš„åˆ†æ‰¹è³‡æ–™
                logger.info(f"ğŸ” é–‹å§‹å°‹æ‰¾è¦åˆªé™¤çš„è³‡æ–™ (PO={po_no}, Item={item}, å‚™è¨»åŒ…å«'åˆ†æ‰¹', ç‹€æ…‹=V)")
                
                old_mask = (
                    (df_buyer["PO No."].str.strip() == po_no) &
                    (df_buyer["Item"].str.strip() == item) &
                    (df_buyer["å‚™è¨»"].str.contains("åˆ†æ‰¹", na=False)) &
                    (df_buyer["é–‹å–®ç‹€æ…‹"] == "V")
                )
                
                deleted_count = old_mask.sum()
                logger.info(f"ğŸ“Š æ‰¾åˆ° {deleted_count} ç­†ç¬¦åˆæ¢ä»¶çš„è³‡æ–™")
                
                if deleted_count == 0:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™")
                    return jsonify({
                        "status": "error",
                        "message": f"æ‰¾ä¸åˆ°è¦åˆªé™¤çš„èˆŠè³‡æ–™ (PO {po_no} - Item {item})"
                    }), 404
                
                # ğŸ¯ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬
                logger.info(f"ğŸ“‹ å–å¾—ç¬¬ä¸€ç­†èˆŠè³‡æ–™ä½œç‚ºç¯„æœ¬")
                first_old_row = df_buyer[old_mask].iloc[0]
                
                # ğŸ“ è¨˜éŒ„è¢«åˆªé™¤çš„è³‡æ–™
                deleted_data = []
                for idx, row in df_buyer[old_mask].iterrows():
                    deleted_data.append({
                        "å“é …": row.get("å“é …", ""),
                        "äº¤æœŸ": row.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", ""),
                        "SOD": row.get("SOD Qty å» å•†æ‰¿è«¾æ•¸é‡", ""),
                        "å‚™è¨»": row.get("å‚™è¨»", "")
                    })
                
                logger.info(f"ğŸ—‘ï¸ å°‡åˆªé™¤çš„è³‡æ–™:")
                for i, d in enumerate(deleted_data, 1):
                    logger.info(f"   ç¬¬ {i} ç­†: {d}")
                
                # åˆªé™¤èˆŠè³‡æ–™
                logger.info(f"ğŸ—‘ï¸ åŸ·è¡Œåˆªé™¤æ“ä½œ...")
                df_buyer = df_buyer[~old_mask]
                logger.info(f"âœ… åˆªé™¤å®Œæˆ,å‰©é¤˜ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’° è¨ˆç®—å–®åƒ¹
                try:
                    unit_price_str = str(first_old_row.get("å–®åƒ¹", "0")).replace(",", "").strip()
                    unit_price = float(unit_price_str) if unit_price_str else 0.0
                    logger.info(f"ğŸ’° åŸå§‹å–®åƒ¹: {unit_price}")
                except Exception as e:
                    logger.error(f"âŒ è§£æå–®åƒ¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    unit_price = 0.0
                
                # â• æº–å‚™æ–°è³‡æ–™ (åˆä½µå¾Œçš„è³‡æ–™)
                logger.info(f"â• é–‹å§‹æº–å‚™æ–°è³‡æ–™...")
                new_data_rows = []
                
                for idx, row_data in enumerate(xls_data):
                    logger.info(f"   è™•ç†ç¬¬ {idx+1}/{len(xls_data)} ç­†æ–°è³‡æ–™...")
                    
                    try:
                        sod_qty = float(row_data.get("sod_qty", 0))
                    except:
                        sod_qty = 0.0
                    
                    sod_qty_formatted = smart_number_format(sod_qty)
                    total_price = unit_price * sod_qty
                    total_price_formatted = smart_number_format(total_price)
                    unit_price_formatted = smart_number_format(unit_price)
                    
                    # æ¸…ç†å‚™è¨» (ç§»é™¤ã€Œåˆ†æ‰¹ã€å­—æ¨£)
                    old_note = first_old_row.get("å‚™è¨»", "")
                    new_note = old_note.replace("åˆ†æ‰¹1/2", "").replace("åˆ†æ‰¹2/2", "").strip()
                    
                    new_row = {
                        "Id": first_old_row.get("Id", ""),
                        "é–‹å–®ç‹€æ…‹": "V",
                        "äº¤è²¨é©—è­‰": first_old_row.get("äº¤è²¨é©—è­‰", ""),
                        "User": first_old_row.get("User", ""),
                        "ePR No.": first_old_row.get("ePR No.", ""),
                        "PO No.": po_no,
                        "Item": item,
                        "å“é …": row_data.get("description", first_old_row.get("å“é …", "")),
                        "è¦æ ¼": first_old_row.get("è¦æ ¼", ""),
                        "æ•¸é‡": sod_qty_formatted,
                        "ç¸½æ•¸": sod_qty_formatted,
                        "å–®åƒ¹": unit_price_formatted,
                        "ç¸½åƒ¹": total_price_formatted,
                        "å‚™è¨»": new_note,  # å·²ç§»é™¤ã€Œåˆ†æ‰¹ã€å­—æ¨£
                        "å­—æ•¸": first_old_row.get("å­—æ•¸", ""),
                        "isEditing": "False",
                        "backup": "{}",
                        "_alertedItemLimit": "",
                        "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": row_data.get("delivery", ""),
                        "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": sod_qty_formatted,
                        "é©—æ”¶æ•¸é‡": "",
                        "æ‹’æ”¶æ•¸é‡": "",
                        "ç™¼ç¥¨æœˆä»½": "",
                        "WBS": first_old_row.get("WBS", ""),
                        "éœ€æ±‚æ—¥": first_old_row.get("éœ€æ±‚æ—¥", ""),
                        "RTé‡‘é¡": "",
                        "RTç¸½é‡‘é¡": "",
                        "é©—æ”¶ç‹€æ…‹": "X"
                    }
                    new_data_rows.append(new_row)
                    logger.info(f"      å“é …: {new_row['å“é …']}, SOD: {new_row['SOD Qty å» å•†æ‰¿è«¾æ•¸é‡']}, ç¸½åƒ¹: {new_row['ç¸½åƒ¹']}")
                
                logger.info(f"âœ… æº–å‚™å®Œæˆ {len(new_data_rows)} ç­†æ–°è³‡æ–™")
                
                # ğŸ”´ğŸ”´ğŸ”´ ç›´æ¥åŠ åˆ°æœ€å¾Œé¢ ğŸ”´ğŸ”´ğŸ”´
                logger.info(f"ğŸ”¥ å°‡æ–°è³‡æ–™æ·»åŠ åˆ°æœ€å¾Œ...")
                df_buyer = pd.concat([
                    df_buyer,
                    pd.DataFrame(new_data_rows)
                ], ignore_index=True)
                logger.info(f"âœ… æ·»åŠ å®Œæˆ,ç¾æœ‰ {len(df_buyer)} ç­†è³‡æ–™")
                
                # ğŸ’¾ å„²å­˜å›æª”æ¡ˆ
                logger.info(f"ğŸ’¾ é–‹å§‹å„²å­˜åˆ° {BUYER_FILE}...")
                df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
                logger.info(f"âœ… å„²å­˜å®Œæˆ")
        
        except Timeout:
            logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
            return jsonify({
                "status": "error",
                "message": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"
            }), 503

        # ğŸ“ æº–å‚™å›å‚³è¨Šæ¯
        success_msg = f"æˆåŠŸåˆä½µ PO {po_no} Item {item}: åˆªé™¤ {deleted_count} ç­†åˆ†æ‰¹è³‡æ–™,æ–°å¢ {len(new_data_rows)} ç­†åˆä½µè³‡æ–™"
        
        logger.info("=" * 80)
        logger.info(f"âœ… {success_msg}")
        logger.info("=" * 80)

        # ğŸ—‘ï¸ åˆªé™¤æš«å­˜çš„ CSV æª”æ¡ˆ
        cleanup_temp_csv_files(po_no)
        
        return jsonify({
            "status": "success",
            "message": success_msg,
            "deleted_count": int(deleted_count),
            "inserted_count": len(new_data_rows),
            "merge_info": {
                "po_no": po_no,
                "item": item,
                "old_count": int(deleted_count),
                "new_count": len(new_data_rows)
            }
        })
        
    except Exception as e:
        logger.error("=" * 80)
        logger.error(f"âŒ confirm_merge ç™¼ç”Ÿåš´é‡éŒ¯èª¤")
        logger.error(f"âŒ éŒ¯èª¤è¨Šæ¯: {str(e)}")
        logger.error(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
        logger.error("âŒ å®Œæ•´å †ç–Šè¿½è¹¤:")
        logger.error(traceback.format_exc())
        logger.error("=" * 80)
        
        return jsonify({
            "status": "error",
            "message": f"åˆä½µå¤±æ•—: {str(e)}"
        }), 500

    
@app.route("/api/save_override_all", methods=["POST"])
def save_override_all():
    """
    Version 31 - å“åå„ªå…ˆæ¯”å°é‚è¼¯ï¼ˆå·²æ”¹é€²è™•ç†é‡è¤‡ Itemï¼‰
    æ”¹é€²ï¼šå„ªå…ˆä»¥å“åç›¸ä¼¼åº¦ç‚ºä¸»è¦æ¯”å°ä¾æ“šï¼ŒItem ä½œç‚ºæ¬¡è¦åƒè€ƒ
    å„ªå…ˆé †åºï¼šåŒ PO å…§çš„å“åé«˜åº¦ç›¸ä¼¼ > Item ç›¸åŒ > å“åä¸­åº¦ç›¸ä¼¼ > æ–°å¢
    """
    # å‚™ä»½
    # backup_files()
    data = request.get_json()
    rows = data.get("rows", [])
    confirm_override = data.get("confirm_override", False)  # æ˜¯å¦å·²ç¢ºèªè¦†è“‹

    if not rows:
        return jsonify({"status": "error", "msg": "âŒ æ²’æœ‰æ”¶åˆ°ä»»ä½•è³‡æ–™"}), 400

    def clean_text(x):
        """æ¸…ç†æ–‡å­—ï¼šç§»é™¤æ›è¡Œå’Œç©ºç™½"""
        return str(x).replace("\n", "").replace("\r", "").strip()
    
    def calculate_similarity(text1, text2):
        """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-100)"""
        text1_clean = clean_text(text1).lower()
        text2_clean = clean_text(text2).lower()
        return SequenceMatcher(None, text1_clean, text2_clean).ratio() * 100
    
    # è™•ç† pandas int64 è½‰æ›å•é¡Œ
    def convert_to_json_serializable(obj):
        """å°‡ pandas çš„ç‰¹æ®Šé¡å‹è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„é¡å‹"""
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_json_serializable(item) for item in obj]
        elif hasattr(obj, 'item'):  # numpy/pandas æ•¸å€¼é¡å‹
            return obj.item()
        elif pd.isna(obj):  # NaN å€¼
            return None
        else:
            return obj

    # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–ä¿è­·è®€å–æ“ä½œ
    try:
        with buyer_file_lock:
            # è®€å–ä¸¦è™•ç†è³‡æ–™
            df_buyer = pd.read_csv(BUYER_FILE, encoding="utf-8-sig", dtype=str, on_bad_lines="skip").fillna("")
            df_buyer["PO No."] = df_buyer["PO No."].str.strip()
            df_buyer["Item"] = (
                df_buyer["Item"]
                .str.replace(r"\.0$", "", regex=True)
                .str.strip()
                .apply(lambda x: x.zfill(4) if x.isdigit() else x)
            )
            
            # ğŸ”´ é‡è¦ï¼šå»ºç«‹ä¸€å€‹åªåŒ…å«ç‹€æ…‹ç‚º V çš„è³‡æ–™ç´¢å¼•
            df_active = df_buyer[df_buyer["é–‹å–®ç‹€æ…‹"] == "V"].copy()
            logger.info(f"ç¸½è³‡æ–™ç­†æ•¸: {len(df_buyer)}, æœ‰æ•ˆè³‡æ–™(ç‹€æ…‹=V): {len(df_active)}")

    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ è®€å– Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"è®€å–æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500


    # âœ… ğŸ†• **åœ¨é€™è£¡æ·»åŠ åˆå§‹åŒ– all_group_results**
    all_group_results = []  # ç”¨ä¾†æ”¶é›†æ‰€æœ‰ PO çš„æ¯”å°çµæœ

    updated_count = 0
    inserted_count = 0
    failed = []
    need_confirm_items = []  # éœ€è¦ç¢ºèªçš„é …ç›®
    auto_updated_items = []  # ğŸ†• è‡ªå‹•æ›´æ–°çš„é …ç›®
    matching_output = []  # æ¯”å°çµæœè¼¸å‡º

    new_item = ''
    epr_no = 0
    po_no_new = ''
    
    # è¼¸å‡ºé–‹å§‹è¨Šæ¯
    logger.info("\n" + "="*80)
    logger.info("é–‹å§‹è™•ç†è³‡æ–™æ¯”å° (Version 31 - å“åå„ªå…ˆ)")
    logger.info("="*80)

    for row_num, row in enumerate(rows, 1):
        id_ = row.get("id", "").strip()
        po_no_new = row.get("po_no", "").strip()
        item_new = row.get("item", "").strip()
        
        # ç¢ºä¿ item æ ¼å¼ä¸€è‡´ï¼ˆ4ä½æ•¸ï¼‰
        if item_new.isdigit():
            item_new = item_new.zfill(4)
            
        new_delivery = row.get("delivery_date", "").strip()
        new_qty = row.get("sod_qty", "").strip()
        new_desc = row.get("po_description", "").strip()
        new_desc_clean = clean_text(new_desc)

        target_idx = None
        match_reason = ""
        
        # è¼¸å‡ºç•¶å‰è™•ç†é …ç›®
        logger.info(f"\n[ç¬¬ {row_num} ç­†]")
        logger.info(f"  æ–°è³‡æ–™ => PO: {po_no_new}, Item: {item_new}")
        logger.info(f"  å“å: {new_desc[:50]}{'...' if len(new_desc) > 50 else ''}")
        
        # ğŸ” Version 31 æ ¸å¿ƒæ”¹è®Šï¼šå…ˆæ‰¾å“åç›¸ä¼¼åº¦ï¼Œå†è€ƒæ…® Item
        # æ­¥é©Ÿ1ï¼šå…ˆåœ¨åŒ PO å…§æ‰¾è³‡æ–™ï¼ˆåªæ‰¾ç‹€æ…‹ç‚º V çš„ï¼‰
        po_group = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        
        if not po_group.empty:
            logger.info(f"     åœ¨ PO {po_no_new} æ‰¾åˆ° {len(po_group)} ç­†è³‡æ–™")
            
            # ğŸ”¥ Version 31ï¼šè¨ˆç®—æ‰€æœ‰é …ç›®çš„å“åç›¸ä¼¼åº¦
            similarity_scores = []
            for idx, row_data in po_group.iterrows():
                existing_desc = row_data["å“é …"]
                existing_item = row_data["Item"]
                similarity = calculate_similarity(new_desc, existing_desc)
                similarity_scores.append({
                    'index': idx,
                    'item': existing_item,
                    'desc': existing_desc,
                    'similarity': similarity,
                    'item_match': (existing_item == item_new),  # è¨˜éŒ„ Item æ˜¯å¦ç›¸åŒ
                    'delivery_date': row_data.get("Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ", "")  # åŠ å…¥äº¤æœŸè³‡è¨Š
                })
            
            # ğŸ”´ğŸ”´ğŸ”´ é€™è£¡æ˜¯ä¿®æ”¹çš„é‡é» ğŸ”´ğŸ”´ğŸ”´
            # Version 31 æ”¹é€²ï¼šè™•ç†ç›¸åŒ Item çš„å¤šç­†è³‡æ–™
            # å¦‚æœæœ‰å¤šç­†å®Œå…¨ç›¸åŒçš„ Itemï¼Œå„ªå…ˆè€ƒæ…®å“åç›¸ä¼¼åº¦ï¼Œå†è€ƒæ…®äº¤æœŸ
            item_matches = [s for s in similarity_scores if s['item_match']]
            if len(item_matches) > 1:
                logger.info(f"     ç™¼ç¾ {len(item_matches)} ç­†ç›¸åŒçš„ Item {item_new}")
                
                # ğŸ†• è¨ˆç®—æ¯ç­†çš„å“åç›¸ä¼¼åº¦
                for match in item_matches:
                    match['name_similarity'] = calculate_similarity(new_desc, match['desc'])
                    
                    # è™•ç†äº¤æœŸ
                    try:
                        date_str = match['delivery_date'].strip()
                        if date_str:
                            date_str = date_str.split(' ')[0]
                            date_str = date_str.replace('/', '-')
                            match['parsed_date'] = date_str
                        else:
                            match['parsed_date'] = '1900-01-01'
                    except:
                        match['parsed_date'] = '1900-01-01'
                    
                    logger.info(f"       - Index {match['index']}: å“åç›¸ä¼¼åº¦ {match['name_similarity']:.1f}%, äº¤æœŸ {match['delivery_date']}")
                
                # ğŸ†• æ”¹é€²çš„é¸æ“‡é‚è¼¯
                # 1. å…ˆæ‰¾å“åå®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„ï¼ˆâ‰¥95%ï¼‰
                exact_matches = [m for m in item_matches if m['name_similarity'] >= 95]
                
                if exact_matches:
                    # å¦‚æœæœ‰å“åå¹¾ä¹ç›¸åŒçš„ï¼Œå¾ä¸­é¸æ“‡äº¤æœŸæœ€æ–°çš„
                    exact_matches.sort(key=lambda x: x.get('parsed_date', '1900-01-01'), reverse=True)
                    newest_match = exact_matches[0]
                    logger.info(f"     => âœ… é¸æ“‡å“åç›¸åŒä¸”äº¤æœŸæœ€æ–°çš„è³‡æ–™")
                    logger.info(f"        Index {newest_match['index']}")
                    logger.info(f"        å“åç›¸ä¼¼åº¦: {newest_match['name_similarity']:.1f}%")
                    logger.info(f"        äº¤æœŸ: {newest_match['delivery_date']}")
                else:
                    # å¦‚æœæ²’æœ‰å“åç›¸åŒçš„ï¼Œé¸æ“‡å“åæœ€ç›¸ä¼¼çš„ï¼ˆä½†è¦è­¦å‘Šï¼‰
                    item_matches.sort(key=lambda x: (x['name_similarity'], x.get('parsed_date', '1900-01-01')), reverse=True)
                    newest_match = item_matches[0]
                    
                    if newest_match['name_similarity'] < 60:
                        logger.info(f"     => âš ï¸âš ï¸ è­¦å‘Šï¼šç›¸åŒ Item ä½†å“åå·®ç•°å¾ˆå¤§ï¼")
                        logger.info(f"        Index {newest_match['index']}")
                        logger.info(f"        å“åç›¸ä¼¼åº¦åƒ…: {newest_match['name_similarity']:.1f}%")
                        logger.info(f"        åŸå“å: {newest_match['desc'][:50]}...")
                        logger.info(f"        æ–°å“å: {new_desc[:50]}...")
                        logger.info(f"        å»ºè­°æ‰‹å‹•æª¢æŸ¥ï¼")
                    else:
                        logger.info(f"     => âš ï¸ é¸æ“‡å“åæœ€ç›¸ä¼¼çš„è³‡æ–™")
                        logger.info(f"        Index {newest_match['index']}")
                        logger.info(f"        å“åç›¸ä¼¼åº¦: {newest_match['name_similarity']:.1f}%")
                
                # å°‡é¸ä¸­çš„è³‡æ–™ç§»åˆ° similarity_scores çš„æœ€å‰é¢
                similarity_scores = [s for s in similarity_scores if not s['item_match']]
                similarity_scores.insert(0, newest_match)
            else:
                # æ’åºï¼šå…ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œç›¸ä¼¼åº¦ç›¸åŒæ™‚ Item ç›¸åŒçš„å„ªå…ˆ
                similarity_scores.sort(key=lambda x: (x['similarity'], x['item_match']), reverse=True)
            # ğŸ”´ğŸ”´ğŸ”´ ä¿®æ”¹çµæŸ ğŸ”´ğŸ”´ğŸ”´
            
            # è¼¸å‡ºç›¸ä¼¼åº¦æ’åï¼ˆé™¤éŒ¯ç”¨ï¼‰
            logger.info(f"     å“åç›¸ä¼¼åº¦æ’åï¼š")
            for i, score in enumerate(similarity_scores[:3], 1):  # é¡¯ç¤ºå‰3å
                item_marker = " [Itemç›¸åŒ]" if score['item_match'] else ""
                logger.info(f"       {i}. Item {score['item']}: {score['similarity']:.1f}%{item_marker} - {score['desc'][:30]}...")
            
            # å–å¾—æœ€é«˜ç›¸ä¼¼åº¦çš„é …ç›®
            best_match = similarity_scores[0]
            best_similarity = best_match['similarity']
            best_idx = best_match['index']
            best_item = best_match['item']
            best_desc = best_match['desc']
            
            # ğŸ¯ æ ¹æ“šç›¸ä¼¼åº¦å’Œ Item æ˜¯å¦ç›¸åŒä¾†æ±ºå®šè™•ç†æ–¹å¼
            if best_similarity >= 95:  # å“åå¹¾ä¹å®Œå…¨ç›¸åŒ
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = "å“åå®Œå…¨ç›¸åŒ+Itemç›¸åŒ"
                    logger.info(f"  âœ… å“åå®Œå…¨ç›¸åŒä¸” Item ç›¸åŒï¼ˆç›¸ä¼¼åº¦ {best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                    
                    # ğŸ†• åŠ å…¥è‡ªå‹•æ›´æ–°æ¸…å–®
                    auto_updated_items.append({
                        "row": row_num,
                        "po_no": po_no_new,
                        "new_item": item_new,
                        "old_item": best_item,
                        "new_desc": new_desc,
                        "old_desc": best_desc,
                        "similarity": round(best_similarity, 1),
                        "new_delivery": new_delivery,
                        "new_qty": new_qty,
                        "target_index": int(best_idx),
                        "reason": "å“åèˆ‡Itemå®Œå…¨ç›¸åŒ",
                        "action_type": "auto_updated"
                    })
                else:
                    match_reason = f"å“åå®Œå…¨ç›¸åŒ(Item:{best_item}â†’{item_new})"
                    logger.info(f"  âš ï¸ å“åå®Œå…¨ç›¸åŒä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–° Item")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åç›¸åŒä½†Itemä¸åŒ",
                            "action_type": "update_item_change"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸åŒä½†Itemä¸åŒ"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–° Item")
                        
            elif best_similarity >= 80:  # å“åé«˜åº¦ç›¸ä¼¼
                target_idx = best_idx
                if best_item == item_new:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼+Itemç›¸åŒ({best_similarity:.0f}%)"
                    logger.info(f"  âœ… å“åé«˜åº¦ç›¸ä¼¼ä¸” Item ç›¸åŒï¼ˆ{best_similarity:.1f}%ï¼‰ => ç›´æ¥æ›´æ–°")
                    
                    # ğŸ†• åŠ å…¥è‡ªå‹•æ›´æ–°æ¸…å–®
                    auto_updated_items.append({
                        "row": row_num,
                        "po_no": po_no_new,
                        "new_item": item_new,
                        "old_item": best_item,
                        "new_desc": new_desc,
                        "old_desc": best_desc,
                        "similarity": round(best_similarity, 1),
                        "new_delivery": new_delivery,
                        "new_qty": new_qty,
                        "target_index": int(best_idx),
                        "reason": f"å“åé«˜åº¦ç›¸ä¼¼({best_similarity:.0f}%)ä¸”Itemç›¸åŒ",
                        "action_type": "auto_updated"
                    })
                else:
                    match_reason = f"å“åé«˜åº¦ç›¸ä¼¼(Item:{best_item}â†’{item_new})"
                    logger.info(f"  âš ï¸ å“åé«˜åº¦ç›¸ä¼¼ä½† Item ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     å“åç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    logger.info(f"     åŸå“å: {best_desc[:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åé«˜åº¦ç›¸ä¼¼ä½†Itemä¸åŒ",
                            "action_type": "update_high_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åç›¸ä¼¼{best_similarity:.0f}%"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            elif best_similarity >= 60:  # å“åä¸­åº¦ç›¸ä¼¼
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item ç›¸åŒçš„é …ç›®
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match and item_match['similarity'] >= 40:
                    # å¦‚æœæœ‰ Item ç›¸åŒä¸”ç›¸ä¼¼åº¦ä¸æ˜¯å¤ªä½ï¼Œå„ªå…ˆé¸æ“‡ Item ç›¸åŒçš„
                    target_idx = item_match['index']
                    match_reason = f"Itemç›¸åŒ+å“åç›¸ä¼¼({item_match['similarity']:.0f}%)"
                    logger.info(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒçš„é …ç›®ï¼Œå“åç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%")
                    logger.info(f"     åŸå“å: {item_match['desc'][:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ›´æ–°")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå·®ç•°è¼ƒå¤§",
                            "action_type": "update_medium_similarity"
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åå·®ç•°{item_match['similarity']:.0f}%"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                else:
                    # å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒ
                    target_idx = best_idx
                    match_reason = f"å“åä¸­åº¦ç›¸ä¼¼({best_similarity:.0f}%)"
                    logger.info(f"  âš ï¸ å“åä¸­åº¦ç›¸ä¼¼ï¼ŒItem ä¸åŒï¼ˆ{best_item} â†’ {item_new}ï¼‰")
                    logger.info(f"     ç›¸ä¼¼åº¦: {best_similarity:.1f}%")
                    logger.info(f"     åŸå“å: {best_desc[:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": best_item,
                            "old_desc": best_desc,
                            "similarity": round(best_similarity, 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(best_idx),
                            "reason": "å“åä¸­åº¦ç›¸ä¼¼",
                            "action_type": "update_medium_similarity",
                            "warning": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": f"{best_item}â†’{item_new}",
                            "match": match_reason,
                            "action": "å¾…ç¢ºèª",
                            "note": f"å“åä¸­åº¦ç›¸ä¼¼"
                        })
                        continue
                    else:
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ›´æ–°")
                        
            else:  # ç›¸ä¼¼åº¦ < 60%
                # æª¢æŸ¥æ˜¯å¦æœ‰ Item å®Œå…¨ç›¸åŒçš„
                item_match = next((s for s in similarity_scores if s['item_match']), None)
                
                if item_match:
                    # Item ç›¸åŒä½†å“åç›¸ä¼¼åº¦ä½
                    logger.info(f"  âš ï¸ æ‰¾åˆ° Item ç›¸åŒä½†å“åå·®ç•°å¾ˆå¤§ï¼ˆç›¸ä¼¼åº¦ {item_match['similarity']:.1f}%ï¼‰")
                    logger.info(f"     åŸå“å: {item_match['desc'][:40]}...")
                    logger.info(f"     æ–°å“å: {new_desc[:40]}...")
                    
                    if not confirm_override:
                        logger.info(f"     âš ï¸âš ï¸ å“åå·®ç•°å¾ˆå¤§ï¼éœ€è¦ç¢ºèª")
                        
                        need_confirm_items.append({
                            "row": row_num,
                            "po_no": po_no_new,
                            "new_item": item_new,
                            "new_desc": new_desc,
                            "old_item": item_new,
                            "old_desc": item_match['desc'],
                            "similarity": round(item_match['similarity'], 1),
                            "new_delivery": new_delivery,
                            "new_qty": new_qty,
                            "target_index": int(item_match['index']),
                            "reason": "Itemç›¸åŒä½†å“åå®Œå…¨ä¸åŒ",
                            "action_type": "update_low_similarity",
                            "warning": True,
                            "critical": True
                        })
                        
                        matching_output.append({
                            "row": row_num,
                            "po": po_no_new,
                            "item": item_new,
                            "match": f"Itemç›¸åŒ(ç›¸ä¼¼åº¦{item_match['similarity']:.0f}%)",
                            "action": "å¾…ç¢ºèª",
                            "note": f"âŒå“åå·®ç•°æ¥µå¤§"
                        })
                        continue
                    else:
                        target_idx = item_match['index']
                        match_reason = f"Itemç›¸åŒ(å“åå·®ç•°å¤§)"
                        logger.info(f"     => å·²ç¢ºèªï¼Œå°‡å¼·åˆ¶æ›´æ–°")
                else:
                    # æ²’æœ‰ä»»ä½•åŒ¹é…ï¼Œå»ºè­°æ–°å¢
                    target_idx = None
        
        # æ­¥é©Ÿ2ï¼šå¦‚æœåœ¨åŒ PO å…§æ‰¾ä¸åˆ°åŒ¹é…ï¼Œè©¢å•æ˜¯å¦æ–°å¢
        if target_idx is None and not po_group.empty:
            logger.info(f"  âš ï¸  åœ¨ PO {po_no_new} å…§æ‰¾ä¸åˆ°ç›¸ä¼¼çš„å“åæˆ–ç›¸åŒçš„ Item")
            logger.info(f"     æ–°Item: {item_new}, æ–°å“å: {new_desc[:40]}...")
            
            if not confirm_override:
                logger.info(f"     éœ€è¦ç¢ºèªæ˜¯å¦è¦æ–°å¢ç‚ºæ–°é …ç›®")
                
                # åˆ—å‡ºç¾æœ‰çš„é …ç›®ä¾›åƒè€ƒ
                existing_items = []
                for score in similarity_scores[:5]:  # é¡¯ç¤ºå‰5å€‹ç›¸ä¼¼åº¦æœ€é«˜çš„
                    existing_items.append({
                        "item": score['item'],
                        "desc": score['desc'][:50],
                        "similarity": round(score['similarity'], 1)
                    })
                
                need_confirm_items.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "new_item": item_new,
                    "new_desc": new_desc,
                    "existing_items": existing_items,
                    "new_delivery": new_delivery,
                    "new_qty": new_qty,
                    "reason": "ç„¡ç›¸ä¼¼é …ç›®",
                    "action_type": "add_new"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡åŒ¹é…",
                    "action": "å¾…ç¢ºèª",
                    "note": "å»ºè­°æ–°å¢"
                })
                continue
            else:
                logger.info(f"     => å·²ç¢ºèªï¼Œå°‡æ–°å¢ç‚ºæ–°é …ç›®")
        
        # æ­¥é©Ÿ3ï¼šå…¶ä»–æ¯”å°æ–¹å¼ï¼ˆIDã€æ¨¡ç³Šæ¯”å°ç­‰ï¼‰- åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None and id_:
            candidates = df_active[df_active["Id"] == id_].copy()
            
            if len(candidates) > 1:
                candidates["å“é …_clean"] = candidates["å“é …"].apply(clean_text)
                exact_match = candidates[candidates["å“é …_clean"] == new_desc_clean]
                if len(exact_match) == 1:
                    target_idx = exact_match.index[0]
                    match_reason = "ID+å“é …åŒ¹é…"
                    logger.info(f"  âœ… ID+å“é …åŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")
            elif len(candidates) == 1:
                target_idx = candidates.index[0]
                match_reason = "IDåŒ¹é…"
                logger.info(f"  âœ… IDåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # æ­¥é©Ÿ4ï¼šPO + å“é …æ¨¡ç³Šæ¯”å° - åªæ‰¾ç‹€æ…‹ç‚º V çš„
        if target_idx is None:
            po_match = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
            po_match = po_match[po_match["å“é …"].apply(lambda x: fuzzy_in(x, new_desc_clean))]

            if not po_match.empty:
                target_idx = po_match.index[0]
                match_reason = "PO+å“é …æ¨¡ç³ŠåŒ¹é…"
                logger.info(f"  âœ… PO+å“é …æ¨¡ç³ŠåŒ¹é… => æ›´æ–°è³‡æ–™ (ç‹€æ…‹=V)")

        # ğŸ” å¦‚æœæ‰¾åˆ°åŒ¹é…é …ç›®ï¼ŒåŸ·è¡Œæ›´æ–°
        if target_idx is not None:
            # è¨˜éŒ„åŸå§‹å€¼ï¼ˆç”¨æ–¼è¼¸å‡ºï¼‰
            old_values = {
                "po": df_buyer.at[target_idx, "PO No."],
                "item": df_buyer.at[target_idx, "Item"],
                "delivery": df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"],
                "qty": df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"],
                "desc": df_buyer.at[target_idx, "å“é …"]
            }
            
            # åŸ·è¡Œæ›´æ–°
            df_buyer.at[target_idx, "PO No."] = po_no_new
            df_buyer.at[target_idx, "Item"] = item_new
            df_buyer.at[target_idx, "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ"] = new_delivery
            df_buyer.at[target_idx, "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡"] = new_qty
            if new_desc:
                df_buyer.at[target_idx, "å“é …"] = new_desc
            updated_count += 1
            
            # è¼¸å‡ºè®Šæ›´è©³æƒ…
            if old_values["po"] != po_no_new:
                logger.info(f"     POè®Šæ›´: {old_values['po']} â†’ {po_no_new}")
            if old_values["item"] != item_new:
                logger.info(f"     Itemè®Šæ›´: {old_values['item']} â†’ {item_new}")
            if old_values["delivery"] != new_delivery:
                logger.info(f"     äº¤æœŸè®Šæ›´: {old_values['delivery']} â†’ {new_delivery}")
            if old_values["qty"] != new_qty:
                logger.info(f"     æ•¸é‡è®Šæ›´: {old_values['qty']} â†’ {new_qty}")
            
            matching_output.append({
                "row": row_num,
                "po": po_no_new,
                "item": item_new if old_values["item"] == item_new else f"{old_values['item']}â†’{item_new}",
                "match": match_reason,
                "action": "å·²æ›´æ–°",
                "note": "å“åå·²è®Šæ›´" if old_values["desc"] != new_desc else ""
            })
            continue

        # ğŸ†• å¦‚æœéƒ½æ‰¾ä¸åˆ° â†’ æ–°å¢è³‡æ–™ï¼ˆä½†è¦æª¢æŸ¥ PO æ˜¯å¦å­˜åœ¨æ–¼ç‹€æ…‹ V çš„è³‡æ–™ä¸­ï¼‰
        po_matches = df_active[df_active["PO No."].apply(lambda x: is_po_in_record(x, po_no_new))]
        if po_matches.empty:
            # å†æª¢æŸ¥æ˜¯å¦æœ‰ç‹€æ…‹ç‚º X çš„ç›¸åŒ PO
            po_cancelled = df_buyer[
                (df_buyer["é–‹å–®ç‹€æ…‹"] == "X") & 
                (df_buyer["PO No."].apply(lambda x: is_po_in_record(x, po_no_new)))
            ]
            
            if not po_cancelled.empty:
                logger.info(f"  âš ï¸  æ‰¾åˆ° PO {po_no_new} ä½†ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰ï¼Œç„¡æ³•æ›´æ–°")
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": f"PO {po_no_new} ç‹€æ…‹ç‚º Xï¼ˆå·²å–æ¶ˆï¼‰"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "POå·²å–æ¶ˆ",
                    "action": "å¤±æ•—",
                    "note": "ç‹€æ…‹ç‚ºX"
                })
                continue
            else:
                # ğŸ”´ Version 31ï¼šPO å®Œå…¨ä¸å­˜åœ¨çš„æƒ…æ³
                logger.info(f"  âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®ï¼šPO {po_no_new}")
                
                failed.append({
                    "row": row_num,
                    "po_no": po_no_new,
                    "item": item_new,
                    "reason": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                
                matching_output.append({
                    "row": row_num,
                    "po": po_no_new,
                    "item": item_new,
                    "match": "ç„¡PO",
                    "action": "å¤±æ•—",
                    "note": "360è¡¨å–®ç„¡æ­¤é …ç›®"
                })
                continue
        
        # ğŸ†“ æ¨æ¸¬ Idï¼ˆå–é¦–ç­†ï¼‰
        possible_ids = po_matches["Id"].dropna().unique().tolist()
        id_ = possible_ids[0] if possible_ids else row.get("id") or row.get("Id", "")
        id_ = str(id_).strip()

        # ğŸ‘¤ å–åŒçµ„ç¬¬ä¸€ç­†çš„è³‡è¨Š
        user = po_matches["User"].iloc[0] if not po_matches.empty else ""
        epr_no = po_matches["ePR No."].iloc[0] if not po_matches.empty else ""
        wbs_no = po_matches["WBS"].iloc[0] if not po_matches.empty else ""
        need_day_no = po_matches["éœ€æ±‚æ—¥"].iloc[0] if not po_matches.empty else ""

        logger.info(f"  ğŸ†• æ‰¾ä¸åˆ°åŒ¹é…é …ç›® => æ–°å¢è³‡æ–™")
        logger.info(f"     æ–°å¢åˆ° ePR No.: {epr_no}")

        # â• æ–°å¢æ–°çš„ä¸€ç­†è³‡æ–™
        new_row = {
            "Id": id_,
            "é–‹å–®ç‹€æ…‹": "V",
            "äº¤è²¨é©—è­‰": "",
            "User": user,
            "ePR No.": epr_no,
            "PO No.": po_no_new,
            "Item": item_new,
            "å“é …": new_desc,
            "è¦æ ¼": "",
            "æ•¸é‡": new_qty,
            "ç¸½æ•¸": new_qty,
            "å–®åƒ¹": "",
            "ç¸½åƒ¹": "",
            "å‚™è¨»": "",
            "å­—æ•¸": "",
            "isEditing": "False",
            "backup": "{}",
            "_alertedItemLimit": "",
            "Delivery Date å» å•†æ‰¿è«¾äº¤æœŸ": new_delivery,
            "SOD Qty å» å•†æ‰¿è«¾æ•¸é‡": new_qty,
            "é©—æ”¶æ•¸é‡": "",
            "æ‹’æ”¶æ•¸é‡": "",
            "ç™¼ç¥¨æœˆä»½": "",
            "WBS": wbs_no,
            "éœ€æ±‚æ—¥": need_day_no,
            "RTé‡‘é¡": '',
            "RTç¸½é‡‘é¡": '',
            "é©—æ”¶ç‹€æ…‹": "X" 
        }

        # ğŸ“Œ æ‰¾é€™å€‹ id çš„æœ€å¾Œä¸€ç­†ä½ç½®
        same_id_idx = df_buyer[df_buyer["Id"] == id_].index
        insert_pos = same_id_idx[-1] + 1 if len(same_id_idx) > 0 else len(df_buyer)

        # âœ¨ æ’å…¥åˆ°åŸ df_buyer ä¸­æŒ‡å®šä½ç½®
        df_buyer = pd.concat([
            df_buyer.iloc[:insert_pos],
            pd.DataFrame([new_row]),
            df_buyer.iloc[insert_pos:]
        ], ignore_index=True)
        
        new_item = 'æ–°å¢ç‰©ä»¶'
        inserted_count += 1
        
        matching_output.append({
            "row": row_num,
            "po": po_no_new,
            "item": item_new,
            "match": "ç„¡åŒ¹é…",
            "action": "å·²æ–°å¢",
            "note": f"ePR:{epr_no}"
        })

    # è¼¸å‡ºæ¯”å°çµæœæ‘˜è¦
    logger.info("\n" + "="*80)
    logger.info("æ¯”å°çµæœæ‘˜è¦ (Version 31 - å“åå„ªå…ˆ)")
    logger.info("="*80)
    logger.info(f"ç¸½è™•ç†ç­†æ•¸: {len(rows)}")
    logger.info(f"æ›´æ–°ç­†æ•¸: {updated_count}")
    logger.info(f"æ–°å¢ç­†æ•¸: {inserted_count}")
    logger.info(f"å¤±æ•—ç­†æ•¸: {len(failed)}")
    logger.info(f"å¾…ç¢ºèªç­†æ•¸: {len(need_confirm_items)}")
    logger.info(f"è‡ªå‹•æ›´æ–°ç­†æ•¸: {len(auto_updated_items)}")  # 
    
    # è¼¸å‡ºè©³ç´°æ¯”å°è¡¨æ ¼
    if matching_output:
        logger.info("\nè©³ç´°æ¯”å°çµæœ:")
        logger.info("-"*80)
        logger.info(f"{'ç­†æ•¸':<5} {'PO No.':<15} {'Item':<10} {'æ¯”å°æ–¹å¼':<20} {'è™•ç†':<8} {'å‚™è¨»'}")
        logger.info("-"*80)
        for item in matching_output:
            logger.info(f"{item['row']:<5} {item['po']:<15} {item['item']:<10} {item['match']:<20} {item['action']:<8} {item['note']}")
    
    logger.info("="*80 + "\n")

    # å¦‚æœæœ‰éœ€è¦ç¢ºèªçš„é …ç›®ï¼Œå›å‚³çµ¦å‰ç«¯
    if need_confirm_items and not confirm_override:
        # æª¢æŸ¥æ˜¯å¦æœ‰é—œéµç¢ºèªé …ï¼ˆå“åå®Œå…¨ä¸åŒï¼‰
        critical_items = [item for item in need_confirm_items if item.get("critical", False)]
        warning_items = [item for item in need_confirm_items if item.get("warning", False)]
        
        # æ ¹æ“šä¸åŒçš„ç¢ºèªåŸå› å’Œå‹•ä½œé¡å‹ï¼Œç”¢ç”Ÿä¸åŒçš„è¨Šæ¯
        action_types = set(item.get("action_type", "") for item in need_confirm_items)
        
        if critical_items:
            msg = f"âš ï¸ ç™¼ç¾ {len(critical_items)} å€‹å“åå®Œå…¨ä¸åŒçš„é …ç›®éœ€è¦ç‰¹åˆ¥ç¢ºèª"
        elif "update_low_similarity" in action_types:
            msg = f"âŒ ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸ä¼¼åº¦æ¥µä½çš„é …ç›®éœ€è¦ç¢ºèª"
        elif "update_item_change" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹å“åç›¸åŒä½†Itemä¸åŒçš„é …ç›®éœ€è¦ç¢ºèª"
        elif "add_new" in action_types:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹é …ç›®å¯èƒ½éœ€è¦æ–°å¢"
        else:
            msg = f"ç™¼ç¾ {len(need_confirm_items)} å€‹éœ€è¦ç¢ºèªçš„é …ç›®"
        
        return jsonify(convert_to_json_serializable({
            "status": "confirm_needed",
            "msg": msg,
            "items": need_confirm_items,
            "auto_updated": auto_updated_items,  # ğŸ†• æ–°å¢è‡ªå‹•æ›´æ–°çš„é …ç›®
            "updated": updated_count,
            "inserted": inserted_count,
            "matching_output": matching_output,
            "has_critical": len(critical_items) > 0,
            "has_warning": len(warning_items) > 0
        }))

    # ğŸ”’ ä½¿ç”¨æª”æ¡ˆé–ä¿è­·å„²å­˜æ“ä½œ
    try:
        with buyer_file_lock:
            # å„²å­˜å›æª”æ¡ˆ
            df_buyer.to_csv(BUYER_FILE, index=False, encoding="utf-8-sig")
    except Timeout:
        logger.error("âŒ ç„¡æ³•å–å¾—æª”æ¡ˆé–é€²è¡Œå„²å­˜,è«‹ç¨å¾Œå†è©¦")
        return jsonify({"status": "error", "msg": "ç³»çµ±å¿™ç¢Œä¸­,ç„¡æ³•å„²å­˜,è«‹ç¨å¾Œå†è©¦"}), 503
    except Exception as e:
        logger.error(f"âŒ å„²å­˜ Buyer_detail.csv æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return jsonify({"status": "error", "msg": f"å„²å­˜æª”æ¡ˆå¤±æ•—: {str(e)}"}), 500
    
    # åˆªé™¤æš«å­˜æª”æ¡ˆ
    if po_no_new:
        temp_file = f"uploads/{po_no_new}.csv"
        if os.path.exists(temp_file):
            os.remove(temp_file)
    
    # ğŸ”´ æª¢æŸ¥æ˜¯å¦æ‰€æœ‰é …ç›®éƒ½å¤±æ•—ä¸”åŸå› éƒ½æ˜¯ã€Œ360è¡¨å–®ç„¡æ­¤é …ç›®ã€
    if failed and len(failed) == len(rows):
        all_not_found = all(f.get("reason") == "360è¡¨å–®ç„¡æ­¤é …ç›®" for f in failed)
        if all_not_found:
            return jsonify(convert_to_json_serializable({
                "status": "not_found",
                "msg": "âŒ 360è¡¨å–®ç„¡æ­¤é …ç›®",
                "failed": failed,
                "matching_output": matching_output
            }))
    
    # å›å‚³çµæœ
    if new_item == 'æ–°å¢ç‰©ä»¶':
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"æœ‰æ–°å¢çš„ç‰©ä»¶(item)éœ€è¦ç¶­è­·ï¼ŒePR No. å–®è™Ÿç‚º {epr_no}ã€‚æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output,
            "auto_updated": auto_updated_items  # ğŸ†• ä¹Ÿåœ¨æˆåŠŸæ™‚å›å‚³
        }))
    else:
        return jsonify(convert_to_json_serializable({
            "status": "ok",
            "msg": f"âœ… æ›´æ–° {updated_count} ç­†ï¼Œæ–°å¢ {inserted_count} ç­†",
            "failed": failed,
            "matching_output": matching_output,
            "auto_updated": auto_updated_items  # ğŸ†• ä¹Ÿåœ¨æˆåŠŸæ™‚å›å‚³
        }))
    



if __name__ == "__main__":
    app.run(debug=True)

